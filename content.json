{"meta":{"title":"AIfish","subtitle":"think how to think","description":"NLP AI","author":"aifish","url":"http://yoursite.com","root":"/"},"pages":[{"title":"About Me","date":"2019-10-08T09:22:51.366Z","updated":"2019-10-08T09:22:51.366Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"王晓宇研究方向：自然语言处理 微信二维码： github: https://github.com/ofooo 博客: https://ofooo.github.io 邮箱: ofyu@163.com"},{"title":"Categories","date":"2019-10-08T09:22:51.366Z","updated":"2019-10-08T09:22:51.366Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-10-08T09:22:51.366Z","updated":"2019-10-08T09:22:51.366Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"笔记-如何有效阅读一本书","slug":"读书笔记/笔记-如何有效阅读一本书","date":"2019-11-06T01:09:43.000Z","updated":"2019-11-06T12:06:45.629Z","comments":true,"path":"wiki/读书笔记/笔记-如何有效阅读一本书/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/笔记-如何有效阅读一本书/","excerpt":"","text":". 书的简介书名: &lt;&lt;如何有效阅读一本书&gt;&gt; 书籍作者: 日本, 奥野宣之 播音员: 陈章鱼 来源: 知乎-讲书 阅读理由很多读书法要求很多准备工作和工具, 而本书介绍的方法只围绕一样东西: 笔记本. 所以称之为”一元化笔记读书法” 书中内容:核心观点: 选书才是读书的第一步 核心阅读标记法 葱鲔（wěi）火锅式”笔记读书法 学以致用的关键: “重读” 一. 选书才是读书的第一步很多人选书是近乎随机的, 被人推荐了或者无意中看了几页就开始看. 缺点: 目的性不强. 被动. 我们应该: 弄清书的基本信息(作者/出版社和讲了什么), 还要理清为什么读这本书, 也就是阅读理由. 选择的要素: 1. 兴趣. 2.问题. 3.枢纽书(好书推荐的书也是好书) 二. 核心阅读标记法第一步: 通读用浏览的方式看看每页大概说了什么就可以了, 不用管细节. 通读过程中, 你觉得哪页有价值, 有启发, 符合自己的实际需要, 就给那一页做个折角. 第一步相当于给书籍做粗筛, 把无法引起兴趣的内容全部剔除掉, 节省阅读时间. 第二步: 重读放弃那些没有折页的内容, 把折角的部分重新读一遍. 如果仍然觉得很好, 你再追加一个折页, 就是折两次. 因为很多内容初读时候觉得很不错, 再读就会觉得启发有限. 这样的内容我们应该果断舍弃. 第三步: 标记把折2次的部分精读一遍. 如果仍然觉得内容很有价值, 就在关键内容处做标记. 要用不同的标记做出区分, 例如用直线标记”客观的, 事实性的内容”, 用波浪线标记”主观观点”, 用星号标记”觉得重要/以后加以运用”的素材, 用圆圈把”专有名词/关键词/关键句等吸引眼球的内容”标记出来. 运用核心阅读标记法就把把书的精华内容成功定位了. 可以把大脑想象成GPS, 大脑的任务就是在庞大的信息中, 找到你需要的部分, 其他部分可以视而不见. 这样不仅能帮你把书读薄, 还有助于勾勒出一副属于你的精华内容地图. 以后写笔记时把地图直接拿来用, 就能简单/方便/可靠/高效的定位到关键素材. 三. 葱鲔（wěi）火锅式”笔记读书法葱鲔火锅式 = 佐料 + 鱼肉 + 大葱 葱鲔火锅式笔记读书法 = 基本信息 + 摘抄 + 评论 基本信息: 三要素: (6位数)笔记日期, 书名, 作者. 在基本信息之下可以摘抄重要的原文: 例如文章中心句, 小标题, 触动你的客观事实或数据, 能够带来启发或联想的内容. 重点是要摘抄让自己心动的内容, 而不是那些客观来讲很重要的段落. (摘抄时最好记录页码, 表示这是摘抄的, 也方便以后翻阅) 紧跟摘抄内容的是对它的感想和评论. (这是最重要的部分, 因为在阅读中获得思考是我们的重要目的) 评论最好有一个特定的标记与摘抄来区分. 范文: [080715] &lt;&lt;决定人类未来的50件事&gt;&gt; 杰西卡.威廉/草思社 ○ 自杀者中有三分之二是因为抑郁.(P180) ※ 作者说, 世界上的自杀者比在战争中死去的还多.人类在战争年代会死亡, 在和平年代也会死亡, 真是不容易. 这表达了作者对死亡问题的看法(这段是笔记作者的感想) 四. 重读重读可以让理解更深刻, 笔记内容更精炼. 重读是学以致用的关键, 反复记忆的内容才可能在需要使用的时候被大脑及时提取出来, 恰当的应用. 如何形成重读的习惯? 在固定场景中重读笔记, 最好是无人打扰的环境 可以利用碎片时间, 比如睡前/饭后/洗完澡等等 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"英文信件结尾除了Best_Regards, 还有56种表达方式","slug":"读书笔记/英文信件结尾56种表达方式","date":"2019-10-30T07:55:15.000Z","updated":"2019-10-31T13:38:57.417Z","comments":true,"path":"wiki/读书笔记/英文信件结尾56种表达方式/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/英文信件结尾56种表达方式/","excerpt":"","text":"Best regards是英文信件结尾通常使用的敬语，虽然它没有什么特殊的含义，但在英文信件结尾部分却是必不可少的，就像我们平常说的“谢谢了”、“您辛苦了”等来结束邮件一样。 福布斯职场专家苏珊·亚当斯和礼仪顾问莱特为我们总结了以下邮件结尾（email sign-offs），并附上了用法解释。 BestThis is the most ubiquitous; it’s totallysafe. I recommend it highly and so do the experts.最普遍；最安全。我和专家们都极力推荐这个。 My BestA little stilted. Etiquette consultant Lettlikes it.有点生硬。礼仪顾问莱特喜欢这个。 My best to youLett also likes this one. I think it’sold-fashioned.莱特也喜欢这个。我认为它过时了。 All BestHarmless.可以用。 All the bestThis works too.这个也可以。 Best WishesSeems too much like a greeting card butit’s not bad.看起来特别像贺卡，但是还不赖。 BestsI know people who like this but I find itfussy. Why do you need the extra “s”?我知道有人喜欢，但是我觉得它很繁琐。为什么你需要多余的“s”？ Best RegardsMore formal than the ubiquitous”Best”. I use this when I want a note of formality.比最普遍的“Best”更正式。当我想要正式的写结尾时我会用这个。 RegardsFine, anodyne, helpfully brief. I use this.不错，稳重，简短。我用这个。 RgdsI used to use this but stopped, becauseit’s trying too hard to be abbreviated. Why not type three more letters? OK ifyou’re sending it from your phone.我过去常用这个但是现在没用了，缩写太过了点。为什么不多写几个字母？如果是用手机发邮件的话，这个OK 。 Warm RegardsI like this for a personal email to someoneyou don’t know very well, or a business email that is meant as a thank-you.我喜欢在给不是很了解的人发私人邮件时用这个，或在商业邮件里表达谢意。 Warmest RegardsAs good as Warm Regards, with a touch ofadded heat.跟Warm Regards一样好，且更增添了一丝温暖。 WarmestI use this often for personal emails,especially if I’m close to someone but not in regular touch.私人邮件中我常常用这个，尤其是当我跟某人比较熟但又不常联系时。 WarmlyThis is a nice riff on the “warm”theme that can safely be used among colleagues.这是一个关于 “warm” 主题很好的结尾，在同事之间使用很安全。 Take careIn the right instances, especially forpersonal emails, this works.在适合的情况下，尤其是私人邮件，这个很有用。 ThanksLett says this is a no-no. “This isnot a closing. It’s a thank-you,” she insists. I disagree. ForbesLeadership editor Fred Allen uses it regularly and I think it’s an appropriate,warm thing to say. I use it too.莱特说这个禁用。她认为“这不是结尾而是感谢。”我不同意。福布斯领导力编辑弗莱德·艾伦经常用这个，我认为它是一个合适的、暖心的词。我也用。 Thanks so muchI also like this and use it, especiallywhen someone—a colleague, a source, someone with whom I have a businessrelationship—has put time and effort into a task or email.我也喜欢而且会用这个，尤其是当某人——同事、人脉资源、跟我有业务关系的人——花时间和精力完成任务和邮件时，我会用这个。 Thanks!This rubs me the wrong way because I usedto have a boss who ended every email this way. She was usually asking me toperform a task and it made her sign-off seem more like a stern order, with aforced note of appreciation, than a genuine expression of gratitude. But in theright context, it can be fine.这个很让我恼火，因为我曾经有个老板每封邮件都用这个结尾。他经常让我完成一个任务，带着勉强感谢的符号，这让他的结尾看起来像是一个严厉的命令，而不是真诚地致谢。但是在合适的语境中，它也适用。 Thank youMore formal than “Thanks.” I usethis sometimes.比“Thanks”更正式。我有时用这个。 Thank you!This doesn’t have the same grating qualityas “Thanks!” The added “you” softens it.这个不会像“Thanks”那么让人不悦！“You” 缓和了语气。 Many thanksI use this a lot, when I genuinelyappreciate the effort the recipient has undertaken.当我由衷感谢收件人所付出的努力时，我常常用这个。 Thanks for your consideration.A tad stilted with a note of servility,this can work in the business context, though it’s almost asking for arejection. Steer clear of this when writing a note related to seekingemployment.有点生硬带点屈从，尽管它几乎是寻求排斥，但是在商业邮件中它很有用。当你写就业相关的邮件时，避免使用它。 ThxI predict this will gain in popularity asour emails become more like texts.随着我们的邮件变得更像文本，我猜想这个会越来越受欢迎。 Hope this helpsI like this in an email where you aretrying to help the recipient.在试图帮助收件人的邮件中我喜欢这个。 Looking forwardI use this too. I think it’s gracious andwarm, and shows you are eager to meet with the recipient.我也用这个。我认为它很亲切、暖心，并且显得你很渴望见到收件人。 RushingThis works when you really are rushing. Itexpresses humility and regard for the recipient.当你赶时间时这个很有用。它表达了你的谦卑和对收件人的尊重。 In hasteAlso good when you don’t have time toproofread.当你没有时间校对时用这个也蛮好。 Be wellSome people find this grating. Not appropriatefor a business email.有些人认为不太好听，在商业邮件中不合适。 PeaceRetro, this sign-off wears its politics onits sleeve. It doesn’t bother me but others might recoil.很复古，这个结尾具有政治色彩。有人可能不太适应，但我OK。 Yours TrulyI don’t like this. It makes me feel likeI’m ten years old and getting a note from a pen pal inSweden.我不喜欢这个。它让我有种我十岁从瑞典笔友那里收到一封信的感觉。 YoursSame problem as above.和上面那个问题一样。 Very Truly YoursLett likes this for business emails but Ifind it stilted and it has the pen pal problem.莱特喜欢在商业邮件中用这个，但我认为不自然，还是像笔友信件。 SincerelyLett also likes this but to me, it signalsthat the writer is stuck in the past. Maybe OK for some formal businesscorrespondence, like from the lawyer handling your dead mother’s estate.莱特也喜欢这个，但是对于我来说，这表明作者停留在过去。也许对于某些正式的商业通信是OK的，例如来自处理你去世母亲遗产的律师信函。 Sincerely YoursSame problem as “Sincerely,” buthokier.跟“Sincerely”问题一样，但是有些做作。 Cheers!I wonder how prevalent this is in theUK.I’ve only seen it from Americans who are trying for a British affectation. Iknow it shouldn’t grate on me but it does. I also don’t like people telling meto cheer up.我好奇这个在英国怎么这么流行。我只在试图模仿英国人的美国人的邮件中看到过这个。我知道这个不该激怒我，但是它确实会。我也不喜欢别人告诉我cheer up。 Ciao（意大利语的“你好；再见”）Pretentious for an English-speaker, thoughI can see using it in a personal, playful email.母语是英语(精品课)的人用这个词显得很装，虽然在一些玩笑化的个人邮件中使用它我能理解。 Your nameTerse but just fine in many circumstances.Probably not a good idea for an initial email.虽然简洁，但是在很多情况下都适用。可能用在最开始的邮件中用不是很好。 InitialGood if you know the recipient and evenfine in a business context if it’s someone with whom you correspond frequently.如果你认识收件人，这样用很好；如果你经常和他通信，那么这个在商业邮件中也很适用。 LoveThis seems too informal, like over-sharingin the business context.这个看起来太不正式，在商业邮件中就像过度分享。 XOXOI’ve heard of this being used in businessemails but I don’t think it’s a good idea.我听说过在商业邮件中使用这个，但是我认为这不是个好主意。 Lots of loveI would only use this in a personal email.The “lots of” makes it even more inappropriately effusive than thesimple, clean “Love.”我只会在私人邮件中使用这个。“Lots of”比简洁的“Love”更显得过度热情。 HugsIt’s hard to imagine this in a businessemail but it’s great when you’re writing to your granny.很难想象在商务邮件中用这个，但是如果你是在跟你的奶奶写信，这个就非常好。 Smiley faceEmoticons are increasingly accepted, thoughsome people find them grating. I wouldn’t sign off this way unless I werewriting to my kid.尽管有些人为之恼火，但是表情符号越来越被广为接受。但除非我跟我的孩子写信，否则我不会用这个结尾。 ;-)I’ve gotten emails from colleagues withthese symbols and I find they brighten my day.我收到过同事发的邮件中使用这个符号，我觉得它们很阳光。 [:-)I’m a sucker for variations on the smileyface made with punctuation marks, though I suspect most people don’t like them.尽管我怀疑大多数人都不喜欢，但是我看到用标点符号构成的笑脸还挺高兴的。 Highfive from down lowA colleague shared this awful sign-offwhich is regularly used by a publicist who handles tech clients. An attempt tosound cool, which fails.一个同事分享了这个糟糕的结尾，这个结尾被处理技术客户的公关频繁使用。这个尝试听起来很酷，但是未能凑效。 Take it easy broThough it might turn some people off, Iwould be fine receiving an email with this sign-off, knowing the sender livesin an informal milieu.尽管它可能使一些人反感，但是我收到用它结尾的邮件时感觉挺好，因为我知道这个发件人在非正式的环境下生活。 See you aroundLett would cringe but this seems fine tome.莱特可能不喜欢，但是对我来说似乎挺好。 Have a wonderful bountiful lustful dayIt’s weird and off-putting.这种表达很奇怪而且会令人不快。 Sent from my iPhoneThis may be the most ubiquitous sign-off.It used to bother me but I realize that it explains brevity and typos.这可能是你常常看到的结尾。以前这个签名总是困扰我，但后来我明白了它只是为了解释信件简洁和出现错别字的原因。 Typoscourtesy of my iPhoneSlightly clever but it’s gotten old. Betterto use the automated message.稍显聪明，但很老旧了。用自动消息会更好。 Sent from a prehistoric stone tabletI laughed the first time I read it but thenthe joke wore thin.当我第一次读到时，我笑了，但是过后这个笑话就不好笑了。 Pardon my monkey thumbsSame problem here.跟上面的问题一样。 Please consider the environment before printing this e-mail.A preachy relic of the past. Who doesn’tknow that printing uses paper?证明发件人过去爱说教。谁不知道用纸打印对环境的影响？ vCardsI think these are a great idea. At leastthey work well on my Dell desktop when I want to load a contact into Outlook.我认为这个是个好主意。当我想要加载一个联系方式到Outlook上时，至少这个在我的戴尔桌面上很好用。 This email is off the record unless otherwise indicated.I’m wondering what kind of paranoid peopleput this in their signatures.我很好奇多么偏执的人才会在他的签名中用这个。 Lengthy disclaimersWe’ve all seen these and ignored them,though I understand that many companies require them. Forbes’ in-house legalcounsel, Kai Falkenberg, says she knows of no cases that have relied on legaldisclaimers, though she says they might serve as persuasive evidence in a tradesecrets case where a party was attempting to keep information confidential.我们都看到过这些，但是忽略了他们，尽管我知道很多公司需要它们。福布斯的咨询律师卡伊·法尔肯贝里说她还没听说过法律免责声明派上用场的时候，尽管它们可能在一方试图保持信息机密的商业机密案中被当作是具有说服力的证据。 附英文信件结尾四规则 1.Don’t include quotes.不要包含引言。 Avoid oversized corporate logos.Sometimes we have no choice about this, because our companies insist we includethese things, but if they are too big, they draw the eye away from the message.避免过大的公司标识。有时候我们别无选择，因为公司坚持让我们使用这些东西，但是如果标识太大了，它会分散注意力。 Include your title and contact info, butkeep it short. In most business emails, you’re doing the person a favor bysharing your vital information. But make it minimal. E.g., “Susan Adams,Senior Editor, Forbes 212-206-5571.”包含你的职位和联系信息，但是保持简洁。如：“Susan Adams，福布斯杂志高级编辑，212-206-5571。” Do include some kind of sign-off.务必包含结尾。 参考资料 http://bbs.fob5.com/thread-192289-1-1.html","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"笔记-《做事的常识》----原著:[日本]小苍广,   播音员: 胖子邓","slug":"读书笔记/笔记-《做事的常识》","date":"2019-10-28T23:51:58.000Z","updated":"2019-10-29T05:11:50.836Z","comments":true,"path":"wiki/读书笔记/笔记-《做事的常识》/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/笔记-《做事的常识》/","excerpt":"","text":"书的简介书籍作者: 日本, 小苍广 播音员: 胖子邓 来源: 知乎-讲书 为什么要看“黄金时间做重要但不紧急的事”这个观点特别打动我, 因为”重要但不紧急的事”真的就是”不会去做的事”. 其他几个观点感觉上也比较容易实施, 还需要后续的实践验证. 书中内容:一. 时间并不均匀如何制定计划, 有效管理时间 每个人的一天都有24小时, 而且每个小时都同样长, 但是每个小时的意义是不一样的. 一般来说, 早上精力充沛, 下午更适合简单的工作, 晚上更适合需要联想活整合回忆的工作 了解自己的精力周期 需要注意个体差异, 找到自己的时间分布情况, 找到你的黄金时间, 普通时间, 碎片时间 黄金时间要做”重要但不紧急的事” 注意事项1: 黄金时间不做”紧急且重要的事” 1.1 因为紧急的事本身会迫使你去完成, 而重要却不紧急的事通常我们会拖延并忘记去做 1.2 因为紧急所以精神容易集中, 使用普通时间也不影响做事效果, 所以可以不用黄金时间 注意事项2: 别在黄金时间做杂事(收集资料/回复邮件等等) 2.1 不要做收集资料/回复邮件/这类杂事 碎片时间适合做”整合回忆/计划规划”类的事推而广之, 人生的每一年也不一样, 80岁后的决策就不如20岁时的重要二. 习惯养成需要刻意训练第一个习惯: 一次专注于一件事 如果发现感兴趣的旁支, 加入书签, 之后可用碎片时间查看 第二个习惯: 制定B计划 对于比较难做的事, 或者需要长期努力才能完成的事, 有必要在事先备好补救计划 B计划可以避免我们无法按照计划行事时, 产生的焦虑和挫败感 B计划设置了缓冲地带, 避免”必须完成”的强迫感造成的焦虑情绪 第三个习惯: 培养仪式感 例如工作时把手机 调成静音或关机, 把计划本摆出来 在工作桌前比躺着床上效率更高 例如在完成了一定进度时候, 想象成功的情景来激励 自己 第四个习惯: 运用—&gt;日志—&gt;复盘(循环) 刻意使用这些习惯 记录自己的行为 每天晚上睡前花半分钟时间, 复盘自己有什么地方做的好与不好 第五个习惯: 追溯原因 如果喜欢某个小说, 试试把”这本小说好看”变成”我觉得这不小说不错的地方有N点, 分别是…….” 试试把”这个人的演技不错”变成”究竟是什么原因让我觉得他出色的展现了这种形象” 三. 建立有效激励自己完成进度节点时, 使用随机奖励奖励自己 恒常的奖励给人的奖励感会逐渐降低 这是游戏行业经常做的事, 例如可以淘宝购买随机贴纸, 没完成节点可以一次抽签 让朋友监督自己的计划 对简单的事或要完成的事,不要发朋友圈, 避免赞美使自己提前满足, 降低去实现的动力 对困难的计划可以发朋友圈, 让朋友监督","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"语料收集","slug":"人工智能/资料/语料收集","date":"2019-10-24T07:34:27.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/人工智能/资料/语料收集/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/语料收集/","excerpt":"","text":". CoNLL-2012http://conll.cemantix.org/2012/introduction.html 景略集智收集的多个中文语料https://www.zhihu.com/question/22956189/answer/411522145 中文缩写词库https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/test_set.txt 中文自然语言处理领域发展贡献语料https://github.com/brightmart/nlp_chinese_corpus 有人收罗了40个中文NLP词库，放到了GitHub上https://github.com/fighting41love/funNLP 腾讯AI Lab开源800万中文词的NLP数据集https://ai.tencent.com/ailab/nlp/embedding.html 豆瓣读书评论(网络语言比较多, 比较新潮)https://github.com/JaniceZhao/Douban-Dushu-Dataset.git Chiphell回帖(网络语言比较多, 比较新潮)https://github.com/JaniceZhao/Chinese-Forum-Corpus.git 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"阅读理解模型相关","slug":"人工智能/资料/阅读理解模型相关","date":"2019-10-24T06:34:27.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/人工智能/资料/阅读理解模型相关/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/阅读理解模型相关/","excerpt":"","text":". 2018 机器阅读理解技术竞赛冠军分享问答系统新思路https://zhuanlan.zhihu.com/p/40898301 基于深度神经网络的自动问答系统概述https://zhuanlan.zhihu.com/p/41217854 ACL 2018｜华盛顿大学：简单有效的多段落阅读理解http://www.sohu.com/a/294501748_100118081 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"论文翻译 Enhanced Sequential Inference Model（ESIM）","slug":"读书笔记/论文翻译/论文翻译-ESIM","date":"2019-10-24T03:21:20.000Z","updated":"2019-10-29T05:13:24.905Z","comments":true,"path":"wiki/读书笔记/论文翻译/论文翻译-ESIM/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/论文翻译/论文翻译-ESIM/","excerpt":"","text":"","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"},{"name":"论文翻译","slug":"读书笔记/论文翻译","permalink":"http://yoursite.com/categories/读书笔记/论文翻译/"}],"tags":[]},{"title":"语义蕴含模型相关","slug":"人工智能/资料/语义蕴含模型相关","date":"2019-10-24T00:38:21.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/人工智能/资料/语义蕴含模型相关/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/语义蕴含模型相关/","excerpt":"","text":". 短文本匹配的利器-ESIMhttps://zhuanlan.zhihu.com/p/47580077 论文阅读笔记：文本蕴含之ESIMhttps://zhuanlan.zhihu.com/p/73408108 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"笔记: 建立专属的时间管理系统----胖子邓","slug":"读书笔记/笔记-建立专属的时间管理系统-胖子邓","date":"2019-10-23T23:51:58.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/读书笔记/笔记-建立专属的时间管理系统-胖子邓/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/笔记-建立专属的时间管理系统-胖子邓/","excerpt":"","text":". 制作计划, 要包含娱乐时间 因为娱乐时间是不可缺少的, 如果不做入计划计划就会被打乱 计划中的短娱乐时间, 不能进行那种”可能停不下来”的娱乐活动, 例如刷B站, 刷视频网站, 刷知乎, 刷微博, 防止自己被娱乐APP侵害 输出能力(输出观点带上自己分析出原因): “这个很有趣”—–&gt;有什么相关的知识, 能够收集哪些信息 直觉型思考转变为使用逻辑思考 “这部小说很不错”—-&gt; “这个小说很不错, 因为哪里写的xxxxxxxxxxx” “他的演技很好”—&gt;”他的演技很好, 因为xxxxxxxxxxxx” 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"model.eval() 和 with torch.no_grad() 对比","slug":"人工智能/pytorch/model-eval-vs-torch-no-grad","date":"2019-10-23T08:23:19.000Z","updated":"2019-10-23T08:52:53.810Z","comments":true,"path":"wiki/人工智能/pytorch/model-eval-vs-torch-no-grad/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/model-eval-vs-torch-no-grad/","excerpt":"","text":"两者目标不一样 model.eval()eval()将通知所有层当前的使用模式(train/eval), 因为一些特殊的层会有两种工作方式. batchnorm 在train时使用batch的数据归一化, 而在eval时使用统计数据进行归一化 dropout 在train时正常, 在eval时不会生效 torch.no_grad()12with torch.no_grad(): y = model(x) no_grad()会关闭自动梯度引擎, 这会降低内存或显存的使用数量, 并且加快计算速度 参考资料 https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[]},{"title":"论文翻译 I Know What You Want: Semantic Learning for Text Comprehension","slug":"读书笔记/论文翻译/论文翻译-semantic-learning-for-text-comprehension","date":"2019-10-23T03:21:20.000Z","updated":"2019-10-29T05:13:24.905Z","comments":true,"path":"wiki/读书笔记/论文翻译/论文翻译-semantic-learning-for-text-comprehension/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/论文翻译/论文翻译-semantic-learning-for-text-comprehension/","excerpt":"","text":"I Know What You Want: Semantic Learning for Text Comprehension ABSTRACT 摘要 Who did what to whom is a major focus in natural language understanding, which is right the aim of semantic role labeling (SRL). Although SRL is naturally essential to text comprehension tasks, it is surprisingly ignored in previous work. This paper thus makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal arguments and their corresponding semantic roles. In terms of deep learning models, our embeddings are enhanced by se\u0002mantic role labels for more fine-grained semantics. We show that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks. Extensive experiments on benchmark machine reading comprehension and inference datasets verify that the proposed semantic learning helps our system reach new state-of-the-art. 谁对谁做了什么是自然语言理解的主要重点，这正是语义角色标记（SRL）的目标。 尽管SRL自然是文本理解任务所必需的，但是在以前的工作中却意外地忽略了它。 因此，本文首次尝试通过指定语言参数及其相应的语义角色来使SRL增强文本的理解和推断能力。 在深度学习模型方面，我们的嵌入通过语义角色标签得到了增强，以实现更细粒度的语义。 我们证明了可以将显着标签方便地添加到现有模型中，并在挑战性的文本理解任务中显着改善深度学习模型。 在基准机器阅读理解和推理数据集上进行的大量实验证明，所提出的语义学习有助于我们的系统达到最新的水平。 IntroductionText comprehension (TC) is challenging for it requires computers to read and understand natural language texts to answer questions or make inference, which is indispensable for advanced context-oriented dialogue and interactive systems. This paper focuses on two core text comprehension tasks, machine reading comprehension (MRC) and textual entailment (TE). 文本理解（TC）具有挑战性，因为它要求计算机阅读和理解自然语言文本以回答问题或进行推理，这对于高级的面向上下文的对话和交互式系统是必不可少的。 本文着重于两个核心的文本理解任务，机器阅读理解（MRC）和文本蕴涵（TE）。 One of the intrinsic challenges for TC is the semantic learning. Though deep learning has been applied to a variety of natural language processing (NLP) tasks with remarkable performance (Zhang et al. 2018; Zhu et al. 2018; Zhang and Zhao 2018), recent studies have found deep learning models might not really understand the natural language texts (Mudrakarta et al. 2018) and vulnerably suffer from adversarial attacks (Jia and Liang 2017). Typically, an MRC model pays great attention to non-significant words and ignores important terms and actions. For example, most of the highly attributed words such as there, be, how are usually less important in questions. To help model better understand natural language, we are motivated to discover an effective way to distill the semantics inside the input sentence explicitly, such as semantic role labeling, instead of completely relying on uncontrollable model parameter learning or manual pruning techniques. TC的内在挑战之一是语义学习。 尽管深度学习已应用于表现出色的各种自然语言处理（NLP）任务（Zhang et al.2018; Zhu et al.2018; Zhang and Zhao 2018），但最近的研究发现深度学习模型可能无法真正理解 自然语言文本（Mudrakarta等人，2018）并且极易遭受对抗性攻击（Jia和Liang，2017）。 通常，MRC模型非常注意非有效词，而忽略重要的术语和动作。 例如，大多数高属性词（例如，在那儿）在问题中通常不那么重要。 为了帮助模型更好地理解自然语言，我们希望找到一种有效的方法来明确提取输入句子中的语义，例如语义角色标签，而不是完全依赖不可控制的模型参数学习或手动修剪技术。 Semantic role labeling (SRL) is a shallow semantic parsing task aiming to discover who did what to whom, when and why, which naturally matches the task target of text comprehension. For MRC, questions are usually formed with who, what, how, when and why, whose predicate-argument relationship that is supposed to be from SRL is of the same importance as well. Besides, SRL has been proved to be beneficial to a wide range of NLP tasks, including discourse relation sense classification (Mihaylov and Frank 2016), machine translation (Shi et al. 2016) and question answering (Yih et al. 2016). All the previous successful work indicates that SRL may be hopefully integrated into reading comprehension and inference tasks. 语义角色标记（SRL）是一项浅层的语义分析任务，旨在发现谁对谁，何时何地做了什么，这自然符合文本理解的任务目标。 对于MRC，通常由谁，什么，如何，何时以及为什么以及应该来自SRL的谓词-自变量关系同样重要地形成问题。 此外，已证明SRL对许多NLP任务均有益，包括话语关系意义分类（Mihaylov和Frank 2016），机器翻译（Shi等人2016）和问题解答（Yih等人2016）。 所有先前的成功工作表明，SRL有望集成到阅读理解和推理任务中。 Some work studied question answering (QA) driven SRL, like QA-SRL parsing (He, Lewis, and Zettlemoyer 2015; Fitzgerald, He, and Zettlemoyer 2018). They focus on detecting argument spans for a predicate and generating questions to annotate the semantic relationship. However, our task is quite different. In QA-SRL, the focus is commonly simple and short factoid questions that are less related to the context, let alone makes inference. Actually, text comprehension and inference are quite challenging tasks in NLP, requiring to dig the deep semantics between the document and comprehensive question which are usually raised or rewritten by humans, instead of shallow argument alignment around the same predicate in QA-SRL. In this work, to alleviate such an obvious shortcoming about semantics, we make the first attempt to explore integrative models for finergrained text comprehension and inference. In this work, we propose an SRL-based enhancement framework for TC tasks, which boosts the strong baselines effectively. We implement an easy and feasible scheme to integrate SRL signals in downstream neural models in end-to-end manner. An example about how SRL helps MRC is illustrated in Figure 1. A series of detailed case studies are employed to analyze the robustness of the semantic role labeler. To our best knowledge, our work is the first attempt to apply SRL for text comprehension tasks, which have been ignored in previous works for a long time. 一些工作研究了由问答（QA）驱动的SRL，例如QA-SRL解析（He，Lewis，and Zettlemoyer 2015; Fitzgerald，He，and Zettlemoyer 2018）。他们专注于检测谓词的论元跨度并生成问题以注释语义关系。但是，我们的任务完全不同。在QA-SRL中，重点通常是简单的，简短的事实性问题，与上下文关系较少，更不用说进行推理了。实际上，文本理解和推理在NLP中是一项极富挑战性的任务，需要挖掘文档和通常由人提出或重写的综合性问题之间的深层语义，而不是围绕QA-SRL中相同谓词的浅层参数对齐。在这项工作中，为了缓解语义上的明显缺陷，我们首次尝试探索用于更细粒度的文本理解和推断的集成模型。在这项工作中，我们为TC任务提出了一个基于SRL的增强框架，该框架有效地提高了坚固的基准。我们实现了一种简单可行的方案，以端到端的方式将SRL信号集成到下游神经模型中。图1给出了有关SRL如何帮助MRC的示例。采用了一系列详细的案例研究来分析语义角色标记器的鲁棒性。据我们所知，我们的工作是将SRL应用于文本理解任务的首次尝试，而在以前的工作中长期以来一直忽略该任务。 The rest of this paper is organized as follows. The next section reviews the related work. Section 3 will demonstrate our semantic learning framework and implementation. Task details and experimental results are reported in Section 4, followed by case studies and analysis in Section 5 and conclusion in Section 6. 本文的其余部分安排如下。 下一节将回顾相关工作。 第三部分将演示我们的语义学习框架和实现。 任务详细信息和实验结果在第4节中进行了报告，随后在第5节中进行了案例研究和分析，在第6节中进行了总结。 Figure 1: Semantic role labeling guides text comprehension. 图1：语义角色标签指导文本理解。 Related WorkText ComprehensionSemantic Role LabelingSemantic Role Labeling for Text ComprehensionFor either of text comprehension tasks, we consider an end-to-end model as well as the semantic learning model. The former may be regarded as downstream model of the latter. Thus, our SRL augmented model will be an integration of two end-to-end models through simple embedding concatenation as shown in Figure 2. 对于任何一种文本理解任务，我们都考虑了端到端模型以及语义学习模型。 前者可被视为后者的下游模型。因此，我们的SRL增强模型将通过简单的嵌入串联将两个端到端模型集成在一起，如图2所示。 We apply semantic role labeler to annotate the semantic tags (i.e. predicate, argument) for each token in the input sequence, and then the input sequence along with the corresponding SRL labels is fed to downstream models. We regard the SRL signals as SRL embeddings and employ a lookup table to map each label to vectors, similar to the implementation of word embedding. For each word x, a joint embedding e^j(w) is obtained by the concatenation of word embedding e^w(x) and SRL embedding e^s(x), where ⊕ is the concatenation operator. The downstream model is task-specific. In this work, we focus on the textual entailment and machine reading comprehension, which will be discussed latter. 我们应用语义角色标记器为输入序列中的每个标记注释语义标记（即谓词，自变量），然后将输入序列与相应的SRL标签一起馈入下游模型。 我们将SRL信号视为SRL嵌入，并使用查找表将每个标签映射到向量，类似于单词嵌入的实现。 对于每个单词x，通过单词嵌入e ^ w（x）和SRL嵌入e ^ s（x）的级联获得联合嵌入e ^ j（w），其中⊕是级联运算符。 下游模型是特定于任务的。 在这项工作中，我们专注于文本含义和机器阅读理解，这将在后面讨论。 Figure 2: Overview of the semantic learning framework. Semantic Role LabelerOur concerned SRL includes two subtasks: predicate identification and argument labeling. While the CoNLL-2005 shared task assumes gold predicates as input, this information is not available in many applications, which requires us to identify the predicates for a input sentence at the very beginning. Thus, our SRL module has to be end-to-end, predicting all predicates and corresponding arguments in one shot. 我们关注的SRL包括两个子任务：谓词识别和参数标注。 虽然CoNLL-2005共享任务假定使用黄金谓词作为输入，但是在许多应用程序中此信息不可用，这要求我们从一开始就确定输入句子的谓词。 因此，我们的SRL模块必须是端到端的，一次就能预测所有谓词和相应的参数。 We use spaCy① to tokenize the input sentence with part-of-speech (POS) tags and the verbs are marked as the binary predicate indicator for whether the word is the verb for the sentence. 我们使用spaCy①通过词性（POS）标记对输入句子进行标记化，并将动词标记为该词是否为该句子的动词的二元谓词指示符。 ①https://spacy.io/ Following (He et al. 2017), we model SRL as a span tagging problem using BIO encoding ② and use an 8-layer deep BiLSTM with forward and backward directions interleaved. Different from the baseline model, we replace the GloVe embeddings with ELMo representations③ due to the recent success of ELMo in NLP tasks (Peters et al. 2018). 随后（He et al.2017），我们将SRL建模为使用BIO编码② 的跨度标记问题，并使用8层深度BiLSTM，其中前后方向交错。 与基线模型不同，由于ELMo在NLP任务中的最新成功，我们用ELMo③表示法替换了GloVe嵌入（Peters et al.2018）。 ②To represent a token at the beginning, interior, or outside of any span, respectively. ③3The ELMo representation is obtained from https://allennlp.org/elmo. We use the original one for this work whose output size is 512. In brief, the implementation of our SRL is a series of stacked interleaved LSTMs with highway connections (Srivastava, Greff, and Schmidhuber 2015). The inputs are embedded sequences of words concatenated with a binary indicator containing whether a word is the verbal predicate. Additionally, during inference, Viterbi decoding is applied to accommodate valid BIO sequences. The details are as follows. 简而言之，我们SRL的实现是一系列具有highway连接的堆叠式交错LSTM（Srivastava，Greff和Schmidhuber 2015）。 输入是单词的嵌入序列，该序列与二进制指示符连接在一起，该二进制指示符包含单词是否为谓语。 另外，在推论期间，维特比解码被应用以适应有效的BIO序列。 详情如下所示。 Figure 3: Semantic role labeler. Word RepresentationThe word representation of our SRL model is the concatenation of two vectors: an ELMo embedding e^(l) and predicate indicator embedding (PIE) e^(p) . ELMo is trained from the internal states of a deep bidirectional language model (BiLM), which is pre-trained on a large text corpus with approximately 30 million sentences (Chelba et al. 2014). Besides, following (He et al. 2018b) who shows the predicate-specific feature is helpful in promoting the role labeling, we employ a predicate indicator embedding e^(p) to mark whether a word is a predicate when predicting and labeling the argum ents. The final word representation is given by e = e^(l) ⊕ e^(p) , where ⊕ is the concatenation operator. 我们的SRL模型的词表示形式是两个向量的串联：ELMo嵌入e ^(l) 和谓词指示符嵌入（PIE）e ^（p）。 ELMo是从深度双向语言模型（BiLM）的内部状态进行训练的，该模型在具有大约3000万个句子的大型文本语料库上进行了预训练（Chelba等，2014）。 此外，以下（He et al.2018b）展示了谓词特定的功能有助于促进角色标注，我们在预测和标注argum时采用了谓词指示符嵌入e ^（p）来标记单词是否为谓词。 ents。 最终的单词表示形式由e = e ^（l）⊕e ^（p）给出，其中⊕是连接运算符。 EncoderAs commonly used to model the sequential input, BiLSTM is adopted for our sentence encoder. By incorporating a stack of distinct LSTMs, BiLSTM processes an input sequence in both forward and backward directions. In this way, the BiLSTM encoder provides the ability to incorporate the contextual information for each word. Given a sequence of word representation S = {e1, e2, · · · , eN } as input, the i-th hidden state hi is encoded as follows: 作为对顺序输入进行建模的常用方法，我们的句子编码器采用了BiLSTM。 通过合并不同LSTM的堆栈，BiLSTM在向前和向后两个方向上处理输入序列。 这样，BiLSTM编码器就可以为每个单词合并上下文信息。 给定一系列单词表示形式S = {e1，e2，···，eN}作为输入，第i个隐藏状态hi编码如下： where LSTM^F denotes the forward LSTM transformation and LSTMB denotes the backward LSTM transformation. hfiand hbiare the hidden state vectors of the forward LSTM and backward LSTM respectively. This LSTM uses highway connections between layers and variational recurrent dropout. The encoded representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags. The predicted SRL Labels are defined in PropBank (Palmer, Gildea, and Kingsbury 2005) augmented with B-I-O tag set to represent argument spans. 该LSTM使用图层之间的highway连接和可变递归dropout。 然后，使用最终的密集层投影编码的表示，然后进行softmax激活以在所有可能的标签上形成分布。 预测的SRL标签在PropBank中定义（Palmer，Gildea和Kingsbury 2005），并增加了B-I-O标签集以表示参数范围。 Model ImplementationThe training objective is to maximize the logarithm of the likelihood of the tag sequence, and we expect the correct output sequence matches with, where C is candidate label set. 训练目标是最大化标签序列可能性的对数，并且我们期望与之匹配的正确输出序列，其中C是候选标签集。 Our semantic role labeler is trained on English OntoNotes v5.0 benchmark dataset (Pradhan et al. 2013) for the CoNLL-2012 shared task, achieving an F1 of 84.6%4 on the test set. At test time, we perform Viterbi decoding to enforce valid spans using BIO constraints5 . For the following evaluation, the default dimension of SRL embeddings is 5 and the case study concerning the dimension is shown in the subsection dimension of SRL Embedding. 我们的语义角色标签在CoNLL-2012共享任务的英语OntoNotes v5.0基准数据集（Pradhan等人，2013）上进行了培训，在测试集上的F1达到84.6％4。 在测试时，我们执行Viterbi解码，以使用BIO约束5强制有效跨度。 为了进行以下评估，SRL嵌入的默认尺寸为5，有关该尺寸的案例研究显示在SRL嵌入的小节尺寸中。 The model is run forward for every verb in the sentence. In some cases there is more than one predicate in a sentence, resulting in various semantic role sets whose number is equal to the number of predicates. For convenient downstream model input, we need to ensure the word and the corresponding label are matched one-by-one, that is, only one set for a sentence. To this end, we select the corresponding BIO sets with the most non-O labels as the semantic role labels. For sentences with no predicate, we directly assign O labels to each word in those sentences. 该模型针对句子中的每个动词都向前运行。 在某些情况下，一个句子中有多个谓词，导致各种语义角色集的数量等于谓词的数量。 为了方便下游模型输入，我们需要确保单词和对应的标签是一对一匹配的，也就是说，一个句子只有一组。 为此，我们选择具有最多非O标签的相应BIO集作为语义角色标签。 对于没有谓词的句子，我们直接为这些句子中的每个单词分配O标签。 Text Comprehension ModelTextual EntailmentOur basic TE model is the reproduced Enhanced Sequential Inference Model (ESIM) (Chen et al. 2017) which is a widely used baseline model for textual entailment. ESIM employs a BiLSTM to encode the premise and hypothesis, followed by an attention layer, a local inference layer, an inference composition layer. Slightly different from (Chen et al. 2017), we do not include extra syntactic parsing features and directly replace the pre-trained Glove word embedding with ELMo which are completely character based. Our SRL embedding is concatenated with ELMo embeddings and the joint embeddings are then fed to the BiLSTM encoders. 我们的基本TE模型是复制的增强顺序推理模型（ESIM）（Chen等，2017），它是用于文本蕴涵的广泛使用的基线模型。 ESIM使用BiLSTM编码前提和假设，然后是关注层，本地推理层，推理组成层。 与（Chen et al.2017）稍有不同，我们不包含额外的语法解析功能，而是直接使用完全基于字符的ELMo替换预先训练的Glove单词嵌入。 我们的SRL嵌入与ELMo嵌入串联在一起，然后将联合嵌入馈送到BiLSTM编码器。 Machine Reading ComprehensionOur baseline MRC model is an enhanced version of Bidirectional Attention Flow (Seo et al. 2017) following (Clark and Gardner 2018). The token embedding is the concatenation of pre-trained Glove word vectors, a character-level embedding from a convolutional neural network with max-pooling and pretrained ELMo embeddings from language models (Peters et al. 2018). Our SRL enhanced model takes input of concatenating the token embedding with SRL embeddings. The embeddings of document and question are passed through a shared bi-directional GRU (BiGRU), followed by a bidirectional attention from (Seo et al. 2017) to obtain the context vectors. The contextual document and question representations are then passed to a residual self-attention layer. Then, the model predicts the start and end token of the answer. To this end, a BiGRU is applied, with a linear layer to compute answer start scores for each word. The hidden states are concatenated with the input and fed into a second bidirectional GRU and linear layer to predict answer end scores. Then, we apply a softmax operation to produce start and end probabilities, and we optimize the negative loglikelihood of selecting correct start and end tokens. 我们的基线MRC模型是（Clark and Gardner 2018）之后的双向注意力流的增强版本（Seo et al.2017）。令牌嵌入是预训练的Glove词向量的串联，这是来自卷积神经网络的字符级嵌入，具有最大池和来自语言模型的预训练ELMo嵌入（Peters等人2018）。我们的SRL增强模型采用了将令牌嵌入与SRL嵌入串联的输入。文档和问题的嵌入通过共享的双向GRU（BiGRU）传递，然后来自（Seo et al.2017）的双向关注以获得上下文向量。然后将上下文文档和问题表示形式传递到剩余的自我注意层。然后，模型预测答案的开始和结束标记。为此，应用BiGRU，它具有一个线性层来计算每个单词的答案开始分数。隐藏状态与输入串联在一起，并馈入第二个双向GRU和线性层以预测答案得分。然后，我们应用softmax运算来产生开始和结束概率，并优化选择正确的开始和结束标记的负对数可能性。 EvaluationIn this section, we evaluate the performance of SRL embeddings on two kinds of text comprehension tasks, textual entailment and reading comprehension. Both of the concerned tasks are quite challenging, and could be even more difficult considering that the latest performance improvement has been already very marginal. However, we present a new solution instead of heuristically stacking network design techniques. Namely, we show that SRL embeddings could be potential to give further advances due to its meaningful linguistic augments, which has not been studied yet for the concerned tasks. 在本节中，我们评估SRL嵌入在两种文本理解任务上的性能，即文本蕴涵和阅读理解。 考虑到最新的性能改进已经非常微不足道，这两个相关的任务都是非常具有挑战性的，并且甚至可能更加困难。 但是，我们提出了一种新的解决方案，而不是试探性地堆叠网络设计技术。 即，我们表明，由于SRL有意义的语言扩充，SRL嵌入有潜力进一步发展，但尚未针对相关任务进行研究。 Table 3 shows the hyper-parameters of our models. In our experiments, we basically follow the same hyper-parameters for each model as the original settings from their corresponding literatures (He et al. 2018b; Peters et al. 2018; Chen et al. 2017; Clark and Gardner 2018) except those specified (e.g. SRL embedding dimension). For both of the tasks, we also report the results by using pre-trained BERT (Devlin et al. 2018) as word representation in our baseline models 6 . The hyperparameters were selected using the Dev set, and the reported Dev and Test scores are averaged over 5 random seeds using those hyper-parameters. 表3显示了我们模型的超参数。 在我们的实验中，除了指定的参数外，我们基本上遵循每个模型与其相应文献中原始设置相同的超参数（He等人2018b; Peters等人2018; Chen等人2017; Clark和Gardner 2018） （例如，SRL嵌入尺寸）。 对于这两项任务，我们还通过使用预训练的BERT（Devlin等人，2018）作为基线模型中的单词表示来报告结果6。 使用Dev集选择超参数，并使用这些超参数在5个随机种子上平均报告的Dev和Test分数。 Textual Entailment 文字蕴涵Textual entailment is the task of determining whether a hypothesis is entailment, contradiction and neutral, given a premise. The Stanford Natural Language Inference (SNLI) corpus (Bowman et al. 2015) provides approximately 570k hypothesis/premise pairs. We evaluate the model performance in terms of accuracy. 文本蕴涵是在给定前提的情况下确定假设是否是蕴涵，矛盾和中立的任务。 斯坦福大学自然语言推理（SNLI）语料库（Bowman等，2015）提供了大约570k假设/前提对。 我们根据准确性评估模型性能。 Results in Table 4 show that SRL embedding can boost the ESIM+ELMo model by +0.7% improvement 7 . With the semantic cues, the simple sequential encoding model yields substantial gains over the previous models, and our single BERTLARGE model also achieves a new state-of-the-art, even outperforms all the ensemble models in the leaderboard. This would be owing to more accurate and fine-grained information from effective explicit semantic cues. 表4中的结果表明，SRL嵌入可以将ESIM + ELMo模型提高+ 0.7％7。 有了语义提示，简单的顺序编码模型比以前的模型有了实质性的收益，我们的单个BERT-LARGE模型也实现了最新的技术水平，甚至超过了排行榜中的所有集成模型。 这将归因于来自有效显式语义线索的更准确和更细粒度的信息。 To evaluate the contributions of key factors in our method, a series of ablation studies are performed on the SNLI dev and test set. The results are in Table 5. We observe both SRL and ELMo embeddings contribute to the overall performance. Note that ELMo is obtained by deep bidirectional language with 4,096 hidden units on a large-scale corpus, which requires a huge amount of training time with 93.6 million parameters. The output dimension of ELMo is 512. Compared with the massive computation and high dimension, the SRL embedding is much more convenient for training and much easier for model integration, giving the same level of performance gains. 为了评估我们方法中关键因素的贡献，对SNLI开发人员和测试集进行了一系列消融研究。 结果在表5中。我们观察到SRL和ELMo嵌入都有助于整体性能。 请注意，ELMo是通过深度双向语言获得的，具有大规模语料库中的4,096个隐藏单元，这需要大量训练时间和9360万个参数。 ELMo的输出维度为512。与海量计算和高维度相比，SRL嵌入更易于训练，更易于模型集成，从而获得了相同水平的性能提升。 Machine Reading ComprehensionTo investigate the effectiveness of the SRL embedding in conjunction with more complex models, we conduct experiments on machine reading comprehension tasks. The reading comprehension task can be described as a triple &lt; D, Q, A &gt;, where D is a document (context), Q is a query over the contents of D, in which a span is the right answer A. 为了研究与更复杂的模型结合使用SRL的有效性，我们对机器阅读理解任务进行了实验。 阅读理解任务可以描述为三元组&lt;D，Q，A&gt;，其中D是文档（上下文），Q是对D内容的查询，其中跨度是正确答案A。 As a widely used benchmark dataset for machine reading comprehension, the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al. 2016) contains 100k+ crowd sourced question-answer pairs where the answer is a span in a given Wikipedia paragraph. Two metrics are selected to evaluate the model performance: Exact Match (EM) and a softer metric F1 score, which measures the weighted average of the precision and recall rate at a character level. 作为用于机器阅读理解的广泛使用的基准数据集，斯坦福问答数据集（SQuAD）（Rajpurkar等人，2016）包含100k +众包问答题对，其中答案是给定Wikipedia段落中的跨度。 选择了两个度量来评估模型性能：精确匹配（EM）和较软的度量F1分数，该度量在字符级别上测量精度和召回率的加权平均值。 Table 6 shows the results⑧ . The SRL embeddings give an absolute +0.81% and relative +5.48% performance gains, showing it is also quite effective for more complex document and question encoding. 表6示出了结果。 SRL嵌入提供了绝对的+ 0.81％和相对的+ 5.48％的性能提升，表明它对于更复杂的文档和问题编码也非常有效。 Case Studies 实例探究From the above experiments, we see our semantic learning framework works effectively and the semantic role labeler boosts model performance, verifying our hypothesis that semantic roles are critical for text understanding. Though the semantic role labeler is trained on a standard benchmark dataset, Ontonotes, whose source ranges from news, conversational telephone speech, weblogs, etc., it turns out to be generally useful for text comprehension from probably quite different domains in both textual entailment and machine reading comprehension. To further evaluate the proposed method, we conduct several case studies as follows ⑨. 通过以上实验，我们看到我们的语义学习框架有效工作，语义角色标记器提高了模型性能，证明了我们的假设：语义角色对于理解文本至关重要。 尽管语义角色标记器是在标准基准数据集Ontonotes上进行训练的，其来源包括新闻，对话电话语音，Weblog等，但事实证明，它对于从文本蕴含和 机器阅读理解。 为了进一步评估提出的方法，我们进行了以下几个案例研究studies。 Dimension of SRL EmbeddingThe dimension of embedding is a critical hyper-parameter in deep learning models that may influence the performance. Too high dimension would cause severe over-fitting issues while too low dimension would also cause under-fitting results. To investigate the influence of the dimension of SRL embeddings, we change the dimension in the intervals [1, 2, 5, 10, 20, 50, 100]. Figure 4-5 show the results. We see that 5-dimension SRL embedding gives the best performance on both SNLI and SQuAD datasets. 在深度学习模型中，嵌入的尺寸是一个关键的超参数，它可能会影响性能。 尺寸太大会导致严重的过拟合问题，而尺寸太小也会导致拟合结果不足。 为了研究SRL嵌入尺寸的影响，我们以[1、2、5、10、20、50、100]间隔更改尺寸。 结果如图4-5所示。 我们看到5维SRL嵌入在SNLI和SQuAD数据集上均提供了最佳性能。 Compare with POS/NER TagsPart-of-speech (POS) and named entity (NE) tags have been used in various NLP tasks. To make comparsion between them, we conduct experiments on SNLI with modifications on label embeddings using tags of SRL, POS and NE, respectively. Results in Table 7 show that SRL gives the best result, showing semantic roles contribute to the performance, which also indicates that semantic information matches the purpose of NLI task best. 词性（POS）和命名实体（NE）标签已用于各种NLP任务中。 为了进行比较，我们在SNLI上进行了实验，分别使用SRL，POS和NE标签对标签嵌入进行了修改。 表7中的结果表明，SRL给出了最好的结果，表明语义角色对性能有贡献，这也表明语义信息最符合NLI任务的目的。 Model TrainingWe are interested in how the SRL embeddings influence the model training procedure. We observe that our model converge much more quickly than baseline models without SRL information. Our model achieves the best result after nearly 10 epochs of training while for the baseline model, the iteration is about 20-epoch. Besides, for each epoch, the accuracy of our model is basically higher than the baseline. This shows SRL signals could accelerate model training with accurate hints. Owing to the semantic role indication, our models are able to gain the best performance with less trainingtime. 我们对SRL嵌入如何影响模型训练过程感兴趣。 我们观察到，与没有SRL信息的基线模型相比，我们的模型收敛速度快得多。 经过将近10个训练周期后，我们的模型获得了最佳结果，而对于基线模型，迭代约为20个周期。 此外，对于每个时期，我们模型的准确性基本上都高于基线。 这表明SRL信号可以通过准确提示来加速模型训练。 由于语义角色指示，我们的模型能够通过较少的培训获得最佳性能时间。 ConclusionThis paper presents a novel semantic learning framework for fine-grained text comprehension and inference. We show that our proposed method is simple yet powerful, which achieves a significant improvement over strong baseline models. This work discloses the effectiveness of SRL in text comprehension and inference and proposes an easy and feasible scheme to integrate SRL information in neural models. A series of detailed case studies are employed to analyze the robustness of the semantic role labeler. Though most recent works focus on heuristically stacking complex mechanismsfor performance improvement, we hope to shed some lights on fusing accurate semantic signals for deeper comprehension and inference. 本文提出了一种新颖的语义学习框架，用于细粒度的文本理解和推理。 我们表明，我们提出的方法既简单又强大，与强大的基线模型相比，有了显着的改进。 这项工作揭示了SRL在文本理解和推理中的有效性，并提出了一种简单可行的方案来将SRL信息集成到神经模型中。 一系列详细的案例研究用于分析语义角色标记器的鲁棒性。 尽管最近的工作集中在启发式地堆叠复杂的机制为了提高性能，我们希望为融合准确的语义信号提供一些启发，以进行更深入的理解和推断。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"},{"name":"论文翻译","slug":"读书笔记/论文翻译","permalink":"http://yoursite.com/categories/读书笔记/论文翻译/"}],"tags":[]},{"title":"论文翻译 ALBERT：A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","slug":"读书笔记/论文翻译/论文翻译-ALBERT","date":"2019-10-13T03:21:20.000Z","updated":"2019-10-29T05:13:24.905Z","comments":true,"path":"wiki/读书笔记/论文翻译/论文翻译-ALBERT/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/论文翻译/论文翻译-ALBERT/","excerpt":"","text":"A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS 小型BERT (自监督学习语言表征)原文链接: https://openreview.net/pdf?id=H1eA7AEtvS 译注: 本人英语水平有限, 如有优化建议, 请随时提交issue. ABSTRACT 摘要 Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. 在预训练自然语言表示时增加模型大小通常会导致下游任务的性能得到改善。 但是，由于GPU / TPU内存的限制，更长的训练时间以及意外的模型降级，在某些时候，进一步的模型增加变得更加困难。 为了解决这些问题，我们提出了两种参数减少技术，以降低内存消耗并提高BERT的训练速度（Devlin等人，2019）。 全面的经验证据表明，与原始BERT相比，我们提出的方法所导致的模型可扩展性更好。 我们还使用了一个自我监督的损失，该损失专注于模拟句子间的连贯性，并表明它始终可以帮助多句子输入的下游任务。 因此，我们的最佳模型在GLUE，RACE和SQuAD基准测试中建立了最新的结果，而与BERT-large相比，参数却更少。 1 INTRODUCTION 介绍Full network pre-training (Radford et al., 2018; Devlin et al., 2019) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. 全面的预训练网络（Radford 2018; Devlin 2019）在语言表征学习方面取得了一系列突破。 许多很难的NLP任务，包括那些训练数据有限的任务，都从这些预训练模型中受益匪浅。 这些突破最令人信服的迹象之一是，针对中国的中学和高中英语考试的RACE考试（Lai 2017）设计的阅读理解任务上性能的提升：论文最初描述了 任务并制定建模挑战报告，然后将最新机器的准确性提高到44.1％； 最新公布的结果报告其模型性能为83.2％（Liu等，2019）; 我们在这里所做的工作将其提高到了89.4％，惊人的45.3％的提高，这主要归功于预训练模型高超的语言表征能力。 Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? 这些改进表明，大型网络对于实现最佳性能至关重要（Devlin 2019; Radford 2019）。 预训练大型模型并将其提炼成较小的模型已经成为一种常见的做法（Sun 2019; Turc 2019）以用于实际应用。 考虑到模型大小的重要性，我们问：拥有更好的NLP模型和拥有更大的模型一样容易吗？ An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model. We also observe that simply growing the hidden size of a model such as BERT-large (Devlin et al., 2019) can lead to worse performance. Table 1 and Fig. 1 show a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get worse results with this BERT-xlarge model. 回答此问题的障碍是可用硬件的内存限制。 鉴于当前最先进的模型通常具有数亿甚至数十亿个参数，当我们尝试扩展模型时，很容易遇到这些限制。 由于通信开销与模型中参数的数量成正比，因此在分布式训练中，训练速度也可能受到很大的阻碍。 我们还观察到，仅仅增加诸如BERT-large（Devlin 2019）之类的模型的hinden_size可能会导致性能下降。 表1和图1给出了一个典型示例，在该示例中，我们仅将BERT-large的hinden_size增加到2倍，而使用BERT-xlarge模型则得到了较差的结果。 Model Hidden Size Parameters RACE (Accuracy) BERT-large (Devlin et al., 2019) 1024 334M 72.0% BERT-large(ours) 1024 334M 73.9% BERT-xlarge(ours) 2028 1270M 54.3% Table 1: Increasing hidden size of BERT-large leads to worse performance on RACE. 表1：增加BERT-large的hinden_size会导致RACE上的性能变差。 Figure 1: Training loss (left) and dev masked LM accuracy (right) of BERT-large and BERT-xlarge (2x larger than BERT-large in terms of hidden size). The larger model has lower masked LM accuracy while showing no obvious sign of over-fitting. 图1：BERT-large和BERT-xlarge（hinden_size比BERT-large大2倍）的训练损失（左）和dev遮罩语言模型精度（右）。 较大的模型具有较低的遮罩语言模型的精度，同时没有明显的过拟合迹象。 Existing solutions to the aforementioned problems include model parallelization (Shoeybi et al.,2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017). These solutions address the memory limitation problem, but not the communication overhead and model degradation problem. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture. 解决上述问题的现有解决方案包括模型并行化（Shoeybi 2019）和聪明的内存管理（Chen 2016; Gomez 2017）。 这些解决方案解决了内存限制问题，但没有解决通信开销和模型性能恶化问题。 在本文中，我们通过设计一种小型BERT(Lite BERT)（ALBERT）架构来解决所有上述问题，该架构的参数比传统BERT架构少得多。 ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. ALBERT结合了两种参数缩减技术，这些技术可消除缩放预训练模型时的主要障碍。 第一个是分解式词向量参数化。 通过将大的词向量矩阵分解为两个小的矩阵，我们将隐藏层的大小与词向量的大小分开。 这种分隔使得在不显著增加词汇表嵌入参数大小的情况下更容易增加隐藏的大小。 第二种技术是跨层参数共享。 此技术可防止参数随着网络的深度而增长。 两种技术都可以显着减少BERT的参数数量，而不会严重影响性能，从而提高了参数效率。 与BERT-large相似的ALBERT配置参数减少了18倍，并且训练速度快了1.7倍。 参数减少技术还可以充当正则化的一种形式，从而稳定训练并有助于泛化。 To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT. 为了进一步提高ALBERT的性能，我们还引入了一种自监督的句子顺序预测损失（SOP）。 SOP主要关注句子间的连贯性，旨在解决原始BERT中提出的下一个句子预测（NSP）丢失的无效性（Yang 2019; Liu 2019）。 As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2. 这些设计的结果是，我们能够扩展到更大的ALBERT配置，该配置的参数仍然比BERT-large的参数少，但性能却明显好于BERT。 我们在著名的GLUE，SQuAD和RACE基准上建立了最新的自然语言理解任务的最佳性能。 具体来说，我们将RACE准确度提高到89.4％，将GLUE基准提高到89.4，将SQuAD 2.0的F1得分提高到92.2。 2 RELATED WORK 相关工作2.1 SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE 提升自然语言的表征学习Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted (Mikolov et al., 2013; Le &amp; Mikolov, 2014; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019). One of the most significant changes in the last two years is the shift from pre-training word embeddings, whether standard (Mikolov et al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018), to full-network pre-training followed by task-specific fine-tuning (Radford et al., 2018; Devlin et al., 2019). In this line of work, it is often shown that larger model size improves performance. For example, Devlin et al. (2019) show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance. However, they stop at a hidden size of 1024. We show that, under the same setting, increasing the hidden size to 2048 leads to model degradation and hence worse performance. Therefore, scaling up representation learning for natural language is not as easy as simply increasing model size. 自然语言表征学习对许多NLP任务有用，并已被广泛采用（Mikolov 2013; Le＆Mikolov，2014; Peters 2018; Devlin 2019 ; Radford 2018; 2019）。过去两年中最重大的变化之一是从预训练词向量转变为标准的（Mikolov 2013; Pennington 2014）或上下文的（McCann 2017; Peters 2018）进行全神经网络预训练，然后进行特定于任务的微调（Radford 2018; Devlin 2019）。在这方面的工作中，经常显示出更大的模型尺寸可以提高性能。例如，Devlin 2019）显示，在三个选定的自然语言理解任务中，使用更大的维度大小，更多隐藏层和更多attention heads 总是可以提高性能。但是，这在1024的维度上失效了。我们表明，在相同设置下，将hidden_size增加到2048会导致模型变弱，从而导致性能下降。因此，提升自然语言的表征学习并不像简单地增加模型大小那样容易。 In addition, it is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen et al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be sublinear at the cost of an extra forward pass. Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed. 此外，由于计算限制，尤其是在GPU / TPU内存限制方面，很难用大型模型进行实验。 鉴于当前最先进的模型通常具有数亿甚至数十亿个参数，我们经常会达到内存限制。 为了解决这个问题，Chen(2016）提出了一种称为梯度检查点的方法，以减少额外的前向传递为代价的亚线性内存需求。 Gomez(2017）提出了一种从下一层重建每个层的激活值的方法，这样它们就不需要存储中间层的激活值。 两种方法都以速度为代价减少了内存消耗。 相反，我们的参数减少技术可减少内存消耗并提高训练速度。 2.2 CROSS-LAYER PARAMETER SHARING 跨层参数共享The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoder-decoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer. 跨层共享参数的想法以前曾使用Transformer架构进行探讨（Vaswani 2017），但之前的这项工作集中在针对标准编码器-解码器任务的训练上，而不是预训练/微调设置上。 与我们的观察结果不同，Dehghani等人（2018）显示具有跨层参数共享的网络（Universal Transformer，UT）在语言模型和主谓词一致方面比标准Transformer具有更好的性能。 最近，Bai等（2019）提出了针对Transformer的深度均衡模型（DQE），并证明DQE可以达到一个平衡点，对于该平衡点，某层的输入嵌入和输出嵌入保持相同。 我们的观察表明，我们的嵌入是振荡的而不是收敛的。 郝等（2019）将参数共享transformer与标准transformer相结合，这进一步增加了标transformer的参数数量。 2.3 SENTENCE ORDERING OBJECTIVES 句子排序目标任务 ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday &amp; Hasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstreamtasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two. ALBERT根据预测两个连续文本段的顺序使用的损失函数。几位研究人员已经尝试过与话语连贯性相似的预训练目标。话语中的连贯性和衔接性已得到广泛研究，并且已发现许多现象将相邻的文本片段连接起来（Hobbs 1979； Halliday＆Hasan 1976； Grosz 1995）。在实践中发现有效的大多数任务目标都非常简单。通过使用句子的编码来预测相邻句子中的单词，可以了解Skip-thought（Kiros 2015）和FastSent（Hill 2016）的句子嵌入。句子嵌入学习的其他目标包括预测未来的句子而不是仅预测相邻（Gan 2017）和预测显式话语标记（Jernite 2017; Nie 2019）。我们的损失与Jernite等人的句子排序目标最相似。 （2017），其中学习句子嵌入以确定两个连续句子的顺序。但是，与上述大多数工作不同，我们的损失是按文本段而不是句子来定义的。 BERT（Devlin 2019）使用损失的依据是预测一对文本片段的第二个片段是否已与另一个文档中的一个片段交换。我们在实验中与这种损失进行了比较，发现句子排序是一项更具挑战性的预训练任务，并且对于某些下游任务更有用。与我们的工作同时，Wang等（2019）也尝试预测文本的两个连续段的顺序，但他们将其与原始的下一句预测结合在三向分类任务中，而不是根据经验对两者进行比较。 3 THE ELEMENTS OF ALBERT 模型ALBERT的元素In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019). 在本节中，我们介绍了ALBERT的设计思路，并提供了与原始BERT架构的相应配置的量化比较（Devlin 2019）。 3.1 MODEL ARCHITECTURE CHOICES 模型结构选择The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks &amp; Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64. ALBERT架构的主干与BERT相似，因为它使用具有GELU非线性的transformer编码器（Vaswani 2017）（Hendrycks＆Gimpel，2016）。 我们遵循BERT标记约定，将词向量大小表示为E，将编码器层数表示为L，将隐藏大小表示为H。 （2019），我们将前馈/filter的大小设置为4H，attention heads的数量设置为H / 64。 There are three main contributions that ALBERT makes over the design choices of BERT. ALBERT对BERT的设计选择做出了三点主要贡献。 Factorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H, i.e., E ≡ H. This decision appears suboptimal for both modeling and practical reasons, as follows. 分解词向量。 在BERT中，以及随后的建模改进（例如XLNet（Yang 2019）和RoBERTa（Liu 2019））中，WordPiece词向量维度E与隐藏层大小H绑定，即E≡H 出于建模和实际原因，此决策似乎不是最优，如下所示。 From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H &gt;&gt; E. 从建模角度看，WordPiece词向量旨在学习上下文无关的表征，而(注意力)隐藏层嵌入旨在学习上下文相关的表征。 正如上下文长度的实验所表明的那样（Liu 2019），类似BERT的表征的力量来自上下文的使用，以提供学习此类依赖于上下文表征的信号。 这样，将WordPiece词向量维度E与隐藏层大小H脱开，可以使我们更有效地利用建模所需的总模型参数，这表明H &gt;&gt; E。 From a practical perspective, natural language processing usually require the vocabulary size V to be large. 1 If E ≡ H, then increasing H increases the size of the embedding matrix, which has size V × E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training. 从实践的角度来看，自然语言处理通常要求词汇量V很大。 ① 如果E≡H，则增加H会增大嵌入矩阵的大小，该矩阵的大小为V×E。这很容易形成具有数十亿个参数的模型，其中大多数参数仅在训练期间稀疏更新。 Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V × H) to O(V × E + E × H). This parameter reduction is significant when H &gt;&gt; E. 因此，对于ALBERT，我们使用嵌入参数的分解，将它们分解为两个较小的矩阵。 与其直接将onehot向量投影到大小为H的隐藏空间中，不如将它们投影到大小为E的低维词向量空间中，然后将其投影到隐藏空间中。 通过这种分解，我们将嵌入参数从O（V×H）减少到O（V×E + E×H）。 当H &gt;&gt; E时，此参数减小非常明显。 Cross-layer parameter sharing. For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efficiency. There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers. We compare this design decision against other strategies in our experiments in Sec. 4.5. 跨层参数共享。 对于ALBERT，我们提出了跨层参数共享作为提高参数效率的另一种方法。 有多种共享参数的方法，例如，仅跨层共享前馈网络（FFN）参数，或仅共享attention参数。 ALBERT的默认决定是跨层共享所有参数。 在本节的实验中，我们将此设计决策与其他策略进行了比较。 4.5。 Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our observations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al. (2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging. Dehghani等人也探索了类似的策略（2018）（Universal Transformer，UT）和Bai等人（2019）（Deep Equirbrium Models，DQE）for Transformer network。 与我们的观察结果不同，Dehghani等人（2018）显示UT的性能优于vanilla Transformer。 Bai等（2019）表明，他们的DQE达到了一个平衡点，对于该平衡点，特定层的输入和输出嵌入保持不变。 我们对L2距离和余弦相似度的测量表明，我们的嵌入是振荡的而不是收敛的。 Figure 2: The L2 distances and cosine similarity (in terms of degree) of the input and output embedding of each layer for BERT-large and ALBERT-large. 图2：对于BERT-large和ALBERT-large，每层输入和输出嵌入的L2距离和余弦相似度（以度为单位）。 Figure 2 shows the L2 distances and cosine similarity of the input and output embeddings for each layer, using BERT-large and ALBERT-large configurations (see Table 2). We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE. 图2显示了使用BERT-large和ALBERT-large配置的每一层输入和输出嵌入的L2距离和余弦相似度（参见表2）。 我们观察到，与BERT相比，ALBERT从一层到另一层的过渡要平滑得多。 这些结果表明，权重共享对稳定网络参数有影响。 尽管与BERT相比，这两个指标都有所下降，但是即使经过24层，它们也不会收敛为0。 这表明ALBERT参数的解决方案空间与DQE发现的空间有很大不同。 Model type Parameters Layers Hidden Embedding Parameter-sharing BERT base 108M 12 768 768 FALSE BERT large 334M 24 1024 1024 FALSE BERT xlarge 1270M 24 2048 2048 FALSE ALBERT base 12M 12 768 128 TRUE ALBERT large 18M 24 1024 128 TRUE ALBERT xlarge 59M 24 2048 128 TRUE ALBERT xxlarge 233M 12 4096 128 TRUE Inter-sentence coherence loss. In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning aboutthe relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP’s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. 句子间连贯性损失。 除了遮罩语言模型（MLM）损失（Devlin 2019）之外，BERT还使用了另一种损失，称为下一句预测（NSP）。 NSP是一种二分类损失，用于预测原始文本中是否有两个片段连续出现，如下所示：通过从训练语料库中获取连续片段来创建正样本；负样本是通过将来自不同文档的句段配对而创建的； 正样本和负样本均以相同的概率采样。 NSP目标旨在提高需要推理的下游任务性能, （例如自然语言推断）的句子对之间的关系。 然而，随后的研究（Yang 2019; Liu 2019）发现NSP的影响不可靠，因此决定消除它，这一决定得到了多项任务下游任务性能的改善的支持。 We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a single task ② . However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. 我们推测，与遮罩语言模型（MLM）相比，NSP失效的主要原因是其缺乏任务难度。 按照规定，NSP可以在单个任务中融合主题预测和连贯性预测②。 但是，与连贯性预测相比，主题预测更容易学习，并且与使用MLM损失学习的内容重叠更多。 We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at random baseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks. 我们坚持说句间建模是语言理解的一个重要方面，但是我们提出了一个主要基于连贯性的损失。也就是说，对于ALBERT，我们使用了句子顺序预测（SOP）目标损失，它避免了主题预测，而侧重于建模句子间的连贯性。 SOP损失使用与BERT（同一文档中的两个连续段）相同的技术作为正样本，而负样本使用相同的两个连续段，但顺序互换。这迫使模型学习关于话语级连贯性的细粒度区别。正如我们在第二节中所示。 4.6，事实证明NSP根本无法解决SOP任务（即，它最终学习了更容易的主题预测信号，并在SOP任务上以随机基线水平执行），而SOP可以将NSP任务解决到一个合理程度，大概是基于分析未对准的相干线索。结果，ALBERT模型持续提高了多语句编码任务的下游任务性能。 3.2 MODEL SETUP 模型设置We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. 我们在表2中介绍了具有可比较的超参数设置的BERT和ALBERT模型之间的差异。由于上述设计选择，与相应的BERT模型相比，ALBERT模型的参数大小要小得多。 For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. If we set BERT to have an extra-large size with H = 2048, we end up with a model that has 1.27 billion parameters and under-performs (Fig. 1). In contrast, an ALBERT-xlarge configuration with H = 2048 has only 59M parameters, while an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERT-large’s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. 例如，与BERT-large（18M与334M）相比，ALBERT-large的参数少了约18倍。 如果我们将BERT设置为具有H = 2048的超大尺寸，我们最终会得到一个模型，该模型具有12.7亿个参数并且表现不佳（图1）。 相比之下，H = 2048的ALBERT-xlarge配置只有59M参数，而H = 4096的ALBERT-xxlarge配置具有233M参数，即BERT-large参数的70％左右。 请注意，对于ALBERT-xxlarge，我们主要在12层网络上报告结果，因为24层网络（具有相同的配置）可获得相似的结果，但计算量更大。 This improvement in parameter efficiency is the most important advantage of ALBERT’s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail. 参数效率的提高是ALBERT设计选择的最重要优势。 在量化这一优势之前，我们需要更详细地介绍我们的实验设置。 4 EXPERIMENTAL RESULTS 实验结果4.1 EXPERIMENTAL SETUP 实验设置To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1, x1,2 · · · and x2 = x1,1, x1,2 · · · are two segments. ③ We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo &amp; Richardson, 2018) as in XLNet (Yang et al., 2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by 为了使比较尽可能有意义，我们在使用BOOKCORPUS（Zhu 2015）和英文维基百科（Devlin 2019）进行预训练基线模型时遵循BERT（Devlin 2019）的设置。 这两个语料库包含大约16GB的未压缩文本。 我们将输入的格式设置为“[[CLS] x1 [SEP] x2 [SEP]”，其中x1 = x1,1，x1,2···和x2 = x1,1，x1,2···是两个段。③ 我们始终将最大输入长度限制为512，并随机生成小于512的输入序列，概率为10％。 像BERT一样，我们使用的词汇量为30,000，使用XLNet中的SentencePiece（Kudo＆Richardson，2018）进行标记化（Yang 2019）。 我们使用n-gram掩码（Joshi 2019）为MLM目标生成被遮罩的输入，每个n-gram遮罩的长度是随机选择的。 长度为n的概率为 We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as “White House correspondents”). 我们将n-gram（即n）的最大长度设置为3（即，MLM目标最多可以包含3个完整的单词，例如“白宫|通讯|员”）。 All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 1024,depending on model size. 所有模型更新均使用4096的批量大小和学习率为0.00176的LAMB优化器（You 2019）。 除非另有说明，否则我们将训练所有模型125,000步。 在Cloud TPU V3上进行训练。 用于训练的TPU数量从64到1024不等，取决于型号。 The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 除非另有说明，否则本节中描述的实验设置将用于我们自己的所有BERT版本和ALBERT模型。 4.2 EVALUATION BENCHMARKS 评估基准4.2.1 INTRINSIC EVALUATION 内部评估To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 为了监控训练进度，我们基于SQuAD和RACE的开发集创建了一个开发集，并使用与本节4.1相同的步骤。 我们报告了MLM和句子分类任务的准确性。 注意，我们仅使用此集合来检查模型如何收敛； 它的使用方式不会影响任何下游评估的性能，例如通过模型选择。 4.2.2 DOWNSTREAM EVALUATION 下游任务评估Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.1. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. 继杨等（2019）和Liu等（2019），我们在三个流行的基准上评估我们的模型：通用语言理解评估（GLUE）基准（Wang 2018），两个版本的斯坦福问答数据集（SQuAD; Rajpurkar 2016; 2018 ），以及来自考试的成绩理解（RACE）数据集（Lai 2017）。 为了完整起见，我们在附录A.1中提供了这些基准的描述。 与（Liu 2019）中一样，我们对开发集执行提早停止，除了基于任务排行榜的最终比较（我们还报告测试集结果）之外，我们会报告所有比较。 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT BERT和ALBERT之间的总体比较We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT’s design choices, as shown in Table 3: with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.7%), SQuAD v2.0 (+4.2%), MNLI (+2.2%), SST-2 (+3.0%), and RACE (+8.5%). 现在，我们准备量化本节3 中描述的设计选择的影响。 具体是围绕参数效率的。 参数效率的提高展示了ALBERT设计选择的最重要优势，如表3所示：仅凭BERT-large的参数的70％左右，ALBERT-xxlarge就比BERT-large取得了显着改进，这通过开发差异来衡量 设置几个代表性下游任务的分数：SQuAD v1.1（+1.7％），SQuAD v2.0（+4.2％），MNLI（+2.2％），SST-2（+ 3.0％）和RACE（+ 8.5％） ）。 We also observe that BERT-xlarge gets significantly worse results than BERT-base on all metrics. This indicates that a model like BERT-xlarge is more difficult to train than those that have smaller parameter sizes. Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. The slowest one is the BERT-xlarge model, which we use as a baseline. As the models get larger, the differences between BERT and ALBERT models become bigger, e.g., ALBERT-xlarge can be trained 2.4x faster than BERT-xlarge. 我们还观察到，在所有指标上，BERT-xlarge的结果均比BERT-base差得多。 这表明像BERT-xlarge这样的模型比具有较小参数大小的模型更难训练。 另一个有趣的发现是在相同训练配置（相同数量的TPU）下，训练时间的数据吞吐速度。 由于较少的通信和较少的计算，相比其对应的BERT模型，ALBERT模型具有更高的数据吞吐量。 速度最慢的是BERT-xlarge模型，我们将其用作基准。 随着模型变大，BERT和ALBERT模型之间的差异变得更大，例如ALBERT-xlarge的训练速度比BERT-xlarge快2.4倍。 Table 3: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps. Here and everywhere else, the Avg column is computed by averaging the scores of the downstream tasks to its left (the two numbers of F1 and EM for each SQuAD are first averaged). 表3：经过BOOKCORPUS和Wikipedia预训练模型的开发集结果(训练了125k步)。 在这里和其他任何地方，“平均”列都是通过平均下游任务在其左侧的得分来计算的（每个SQuAD的F1和EM的两个数字首先取平均值）。 Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT. 接下来，我们执行消融实验，以量化ALBERT每个设计选择的个体贡献。 4.4 FACTORIZED EMBEDDING PARAMETERIZATION 分解词向量Table 4 shows the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting (see Table 2), using the same set of representative downstream tasks. Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling. 表4显示了使用基于ALBERT在相同的游任务集更改词向量维度 E的效果。 在非共享条件下（BERT样式），较大的词向量维度可提供更好的性能，但幅度不大。 在全共享情况下（ALBERT样式），大小为128的词向量维度似乎是最好的。 根据这些结果，我们在以后的所有设置中都使用词向量维度E = 128，这是进行进一步缩放的必要步骤。 Table 4: The effect of vocabulary embedding size on the performance of ALBERT-base. 表4：词向量维度大小对基于ALBERT的性能的影响。 4.5 CROSS-LAYER PARAMETER SHARING 跨层参数共享Table 5 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 2) with two embedding sizes (E = 768 and E = 128). We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones). 表5展示了使用具有两种嵌入大小（E = 768和E = 128）的基于ALBERT的配置（表2）进行的各种跨层参数共享策略的实验。 我们比较了全共享策略（ALBERT风格），非共享策略（BERT风格）和仅共享attention参数（但不共享FNN）或仅共享FFN参数的中间策略（ 但没有attention）。 The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (-1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg). 在两种情况下，全共享策略都会损害性能，但是与E = 768（平均-2.5）相比，E = 128（平均-1.5）的严重性要小一些。 此外，大多数性能下降似乎来自共享FFN层参数，而共享注意力参数导致当E = 128（平均为+0.1时）不下降，而当E = 768（-0.7时）时略有下降。 平均）。 Table 5: The effect of cross-layer parameter-sharing strategies, ALBERT-base configuration. 表5：跨层参数共享策略的影响，基于ALBERT的配置。 4.6 SENTENCE ORDER PREDICTION (SOP) 句子顺序预测We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERT-base configuration. Results are shown in Table 6, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. 我们比较了语句间损失的 head-to-head 三个实验条件：无（XLNet和RoBERTa风格），NSP（BERT风格）和SOP（ALBERT风格）, 使用ALBERT的配置。 结果显示在表6中，包括固有任务（MLM，NSP和SOP任务的准确性）和下游任务的结果。 Table 6: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks. 表6：句子预测损失（NSP与SOP）对内部任务和下游任务的影响。 The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the “None” condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 固有任务的结果表明，NSP损失对SOP任务没有任何判别能力（准确性为52.0％，类似于无条件的随机猜测性能）。 这使我们可以得出结论，NSP最终仅对主题转移建模。 相反，SOP损失确实可以较好地解决NSP任务（准确度为78.9％），而SOP本任务准确度为86.5% 。 更重要的是，SOP丢失似乎可以持续提高多句编码任务的下游任务性能（SQuAD1.1大约为+ 1％，SQuAD2.0为+ 2％，RACE为+ 1.7％），从而提高了平均得分 大约+ 1％。 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME? 训练时间相同的情况如何？The speed-up results in Table 3 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 7, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). 表3中的加速结果表明，与ALBERT-xxlarge相比，BERT-large的数据吞吐量大约高3.17倍。 由于更长的训练通常会带来更好的性能，因此我们进行比较，而不是控制数据吞吐量（训练步骤数），而是控制实际训练时间（即让模型训练相同的小时数） 。 在表7中，我们比较了BERT大模型在400k训练步骤后（训练34小时）的性能，大致相当于训练有125k训练步骤（训练32小时）的ALBERT-xxlarge模型所需的时间。 Table 7: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge configurations. 表7：控制训练时间的效果，BERT-large和ALBERT-xxlarge配置。 After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 经过大致相同的时间训练后，ALBERT-xxlarge明显优于BERT-large：平均提高+ 1.5％，RACE的差异高达+ 5.2％。 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS 其他训练数据和DROPOUT效果The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019). In this section, we report measurements on the impact of the additional data used by both XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). 至此，完成的实验仅使用Wikipedia和BOOKCORPUS数据集，如（Devlin 2019）。 在本节中，我们报告了XLNet（Yang 2019）和RoBERTa（Liu 2019）所使用的附加数据的影响的测量结果。 Fig. 3a plots the dev set MLM accuracy under two conditions, without and with additional data, with the latter condition giving a significant boost. We also observe performance improvements on the downstream tasks in Table 8, except for the SQuAD benchmarks (which are Wikipedia-based, and therefore are negatively affected by out-of-domain training material). 图3a绘制了在没有附加数据和有附加数据的情况下，两种情况下开发集的MLM精度，其中后一种情况有明显的提高。 除了SQuAD基准（基于Wikipedia，因此受到域外培训材料的负面影响）之外，我们还观察到了表8中下游任务的性能改进。 Figure 3: The effects of adding data and removing dropout during training. 图3：在训练期间添加数据和移除dropout的影响。 Table 8: The effect of additional training data using the ALBERT-base configuration. 表8：使用基于ALBERT的配置的其他训练数据的效果。 4.9 CURRENT STATE-OF-THE-ART ON NLU TASKS NLU任务的当前最佳性能The results we report in this section make use of the training data used by Devlin et al. (2019), as well as the additional data used by Liu et al. (2019) and Yang et al. (2019). We report state-of-the-art results under two settings for fine-tuning: single-model and ensembles. In both settings, we only do single-task fine-tuning4 . Following Liu et al. (2019), on the development set we report the median result over five runs. 我们在本节中报告的结果利用了Devlin等人使用的训练数据。 （2019），以及Liu等人使用的其他数据（2019）和Yang等（2019）。 我们在两种设置下报告了当前最佳性能(SOTA)，以进行微调：单模型和模型融合。 在这两种设置中，我们仅执行单任务微调4。 继刘等（2019），在开发集上，我们报告了五次运行的中位数结果。 The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 2) using combined MLM and SOP losses, and no dropout. The checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 10) and RACE (Table 11) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 11), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the “unanswerable” decision. 单模型ALBERT配置结合了所讨论的最佳性能设置：ALBERT-xxlarge配置（表2）结合了MLM和SOP损耗，并且没有dropout。 根据开发集性能选择有助于最终整体模型的检查点； 根据任务的不同，为此选择考虑的检查点数量在6到17之间。 对于GLUE（表10）和RACE（表11）基准，我们对集合模型的模型预测取平均，其中使用12层和24层体系结构从不同的训练步骤对候选者进行微调。 对于SQuAD（表11），我们对那些具有多个概率的跨度的预测得分取平均值。 我们还将“无法回答(unanswerable)”的决定的得分平均。 Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models specifically designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model. 单模型结果和整体结果均表明，ALBERT显著改善了所有三个基准的最佳性能，GLUE得分为89.4，SQuAD 2.0测试F1得分为92.2，RACE测试准确性为89.4。 后者似乎是一个特别强大的改进，绝对值比BERT（Devlin 2019）跃升+ 17.4％，比XLNet（Yang 2019）跃升7.6％，比RoBERTa（Liu）跃升6.2％（2019），比DCMI +高5.3％（Zhang 2019），这是专门为阅读理解任务设计的多种模型的集合。 我们的单一模型可达到86.5％的准确度，仍比最新的集成模型高2.4％。 Table 10: State-of-the-art results on the GLUE benchmark. For single-task single-model results, we report ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble uses models trained with 1M, 1.5M, and other numbers of steps. 表10：GLUE基准测试的最新结果。 对于单任务单模型结果，我们报告ALBERT的步长为1M（与RoBERTa相比），步长为150万。 ALBERT集成使用经过1M，1.5M和其他数量步数训练的模型。 Table 11: State-of-the-art results on the SQuAD and RACE benchmarks. 表11：SQuAD和RACE的当前最佳性能 5 DISCUSSION 讨论While ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018). An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations. 尽管ALBERT-xxlarge的参数比BERT-large的参数少，并且获得了明显更好的结果，但由于其较大的结构，计算量更大。 因此，重要的下一步是通过稀疏注意力（Child 2019）和block attention（Shen 2018）之类的方法来加快ALBERT的训练和推理速度。 可以提供更多表征能力的正交研究包括hard example mining（Mikolov 2013）和更有效的语言模型训练（Yang 2019）。 此外，尽管我们有令人信服的证据表明句子顺序预测是一种更一致有用的学习任务，可以带来更好的语言表示，但我们假设当前的自我监督训练损失可能还没有捕获到更多维度，这可能会产生更多的表示形式 结果表示的能力。 REFERENCES 参考文献 (略)","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"},{"name":"论文翻译","slug":"读书笔记/论文翻译","permalink":"http://yoursite.com/categories/读书笔记/论文翻译/"}],"tags":[]},{"title":"jpype和hanlp 安装","slug":"python/jpype和hanlp","date":"2019-10-08T12:17:31.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/python/jpype和hanlp/","link":"","permalink":"http://yoursite.com/wiki/python/jpype和hanlp/","excerpt":"","text":"安装jpype 安装jpype模块 install jpype1```1234567892. 安装jvm```bashjava -version # bash会提示如何安装openjdksudo apt-get install openjdk-11-jre-headless # ubuntu18sudo apt-get install -y openjdk-9-jre-headless # ubuntu16 测试是否成功 1234567from jpype import *startJVM(getDefaultJVMPath(), \"-ea\")java.lang.System.out.println(\"Hello World\")shutdownJVM() 使用hanlp 安装包 1pip install pyhanlp 目录格式 1234567static||---data||---hanlp.properties||---hanlp-1.7.4.jar 配置文件修改data文件夹所在的文件夹123456789104. 设置环境变量```pythonimport os# jar所在文件夹os.environ[&apos;HANLP_STATIC_ROOT&apos;] = &apos;/home/fish3/code/fish_code/hanlp_data/hanlp-1.7.4-release&apos;# jar文件所在路径os.environ[&apos;HANLP_JAR_PATH&apos;] = &apos;/home/fish3/code/fish_code/hanlp_data/hanlp-1.7.4-release/hanlp-1.7.4.jar&apos; import pyhanlp 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576for term in pyhanlp.HanLP.segment('下雨天地面积水'): print('&#123;&#125;\\t&#123;&#125;'.format(term.word, term.nature)) # 获取单词与词性def test(): sentence = pyhanlp.HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\") for word in sentence.iterator(): # 通过dir()可以查看sentence的方法 print(\"%s --(%s)--&gt; %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA)) print() # 也可以直接拿到数组，任意顺序或逆序遍历 word_array = sentence.getWordArray() for word in word_array: print(\"%s --(%s)--&gt; %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA)) print() # 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根 CoNLLWord = pyhanlp.JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\") head = word_array[12] while head.HEAD: head = head.HEAD if (head == CoNLLWord.ROOT): print(head.LEMMA) else: print(\"%s --(%s)--&gt; \" % (head.LEMMA, head.DEPREL)) \"\"\" 输出： 下雨天 n 地面 n 积水 n 徐先生 --(主谓关系)--&gt; 帮助 还 --(状中结构)--&gt; 帮助 具体 --(状中结构)--&gt; 帮助 帮助 --(核心关系)--&gt; ##核心## 他 --(兼语)--&gt; 帮助 确定 --(动宾关系)--&gt; 帮助 了 --(右附加关系)--&gt; 确定 把 --(状中结构)--&gt; 作为 画 --(介宾关系)--&gt; 把 雄鹰 --(动宾关系)--&gt; 画 、 --(标点符号)--&gt; 松鼠 松鼠 --(并列关系)--&gt; 雄鹰 和 --(左附加关系)--&gt; 麻雀 麻雀 --(并列关系)--&gt; 雄鹰 作为 --(动宾关系)--&gt; 确定 主攻 --(定中关系)--&gt; 目标 目标 --(动宾关系)--&gt; 作为 。 --(标点符号)--&gt; 帮助 徐先生 --(主谓关系)--&gt; 帮助 还 --(状中结构)--&gt; 帮助 具体 --(状中结构)--&gt; 帮助 帮助 --(核心关系)--&gt; ##核心## 他 --(兼语)--&gt; 帮助 确定 --(动宾关系)--&gt; 帮助 了 --(右附加关系)--&gt; 确定 把 --(状中结构)--&gt; 作为 画 --(介宾关系)--&gt; 把 雄鹰 --(动宾关系)--&gt; 画 、 --(标点符号)--&gt; 松鼠 松鼠 --(并列关系)--&gt; 雄鹰 和 --(左附加关系)--&gt; 麻雀 麻雀 --(并列关系)--&gt; 雄鹰 作为 --(动宾关系)--&gt; 确定 主攻 --(定中关系)--&gt; 目标 目标 --(动宾关系)--&gt; 作为 。 --(标点符号)--&gt; 帮助 麻雀 --(并列关系)--&gt; 雄鹰 --(动宾关系)--&gt; 画 --(介宾关系)--&gt; 把 --(状中结构)--&gt; 作为 --(动宾关系)--&gt; 确定 --(动宾关系)--&gt; 帮助 --(核心关系)--&gt; ##核心## \"\"\" 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"《hanlp神经网络依存句法分析器》笔记","slug":"人工智能/cs224n笔记/《hanlp神经网络依存句法分析器》笔记","date":"2019-10-08T11:42:54.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/cs224n笔记/《hanlp神经网络依存句法分析器》笔记/","link":"","permalink":"http://yoursite.com/wiki/人工智能/cs224n笔记/《hanlp神经网络依存句法分析器》笔记/","excerpt":"","text":". 原文链接基于神经网络的高性能依存句法分析器 最大熵依存句法分析器的实现 https://github.com/hankcs/HanLP/wiki/FAQ 摘录主流的统计句法分析一般分为两大流派——生成式和判决式。 《生成式》就是生成一系列句法树，从里面挑选出概率最大的那一棵作为输出。其优点是效果好，但开销大。由于是全局最优，所以可以取得较高的准确率，还可以很方便地处理非投射的句法树。不过也由于搜索的全局性和特征函数的复杂度，模型常常会过拟合，在训练集和测试集上的准确率差别很大。 《判决式》一般是基于动作（或称转移）和一个分类器实现的，仿照人类从左到右的阅读顺序，判决式句法分析器不断地读入单词，根据该单词和已构建的句法子树等信息建立分类模型，分类模型输出当前状态下的最佳动作，然后判决式分析器根据最佳动作“拼装”句法树。 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"cs224n笔记","slug":"人工智能/cs224n笔记","permalink":"http://yoursite.com/categories/人工智能/cs224n笔记/"}],"tags":[]},{"title":"BERT与NER","slug":"人工智能/资料/bert与NER","date":"2019-10-08T04:21:20.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/资料/bert与NER/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/bert与NER/","excerpt":"","text":"相关博客Bert时代的创新：Bert应用模式比较及其它—张俊林 NLP的bert和后bert时代的挑战—倾国倾城sun 后Bert时代NLP相关进展—johnchenyhl 最新序列模型介绍（一）—johnchenyhl 最新中文NER模型介绍（二）—johnchenyhl [2018.10]命名实体识别前沿总结—陈海斌 NER- 命名实体识别（Chinese NER 、Cross-domain NER）—PeterLee CRF和LSTM 模型在序列标注上的优劣？—zenRRan 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"torch中的几种乘法。torch.mm, torch.mul, torch.matmul","slug":"人工智能/pytorch/torch乘法","date":"2019-10-06T12:29:59.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/pytorch/torch乘法/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/torch乘法/","excerpt":"","text":". 一、点乘点乘都是broadcast的，可以用torch.mul(a, b)实现，也可以直接用*实现。 1234567891011121314&gt;&gt;&gt; a = torch.ones(3,4)&gt;&gt;&gt; atensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])&gt;&gt;&gt; b = torch.Tensor([1,2,3]).reshape((3,1))&gt;&gt;&gt; btensor([[1.], [2.], [3.]])&gt;&gt;&gt; torch.mul(a, b)tensor([[1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]]) 当a, b维度不一致时，会自动填充到相同维度相点乘。 二、矩阵乘矩阵相乘有torch.mm和torch.matmul两个函数。其中前一个是针对二维矩阵，后一个是高维。当torch.mm用于大于二维时将报错。 123456789101112131415161718192021&gt;&gt;&gt; a = torch.ones(3,4)&gt;&gt;&gt; b = torch.ones(4,2)&gt;&gt;&gt; torch.mm(a, b)tensor([[4., 4.], [4., 4.], [4., 4.]])&gt;&gt;&gt; a = torch.ones(3,4)&gt;&gt;&gt; b = torch.ones(5,4,2)&gt;&gt;&gt; torch.matmul(a, b).shapetorch.Size([5, 3, 2])&gt;&gt;&gt; a = torch.ones(5,4,2)&gt;&gt;&gt; b = torch.ones(5,2,3)&gt;&gt;&gt; torch.matmul(a, b).shapetorch.Size([5, 4, 3])&gt;&gt;&gt; a = torch.ones(5,4,2)&gt;&gt;&gt; b = torch.ones(5,2,3)&gt;&gt;&gt; torch.matmul(b, a).shape报错。 参考资料 https://blog.csdn.net/weixin_42105432/article/details/100691592","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[]},{"title":"我们为什么喜欢巧克力","slug":"读书笔记/我们为什么喜欢巧克力","date":"2019-10-06T04:58:58.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/读书笔记/我们为什么喜欢巧克力/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/我们为什么喜欢巧克力/","excerpt":"","text":". 丹尼尔·丹尼特的认知科学笔记讲座视频: https://www.bilibili.com/video/av68266594 白光是由三原色光组成的吗?有没有人觉得是赤橙黄绿青蓝紫7种颜色光组成的? 我们先看看另一个问题. 我们为什么喜欢吃巧克力因为我们喜欢巧克力. 为什么男人喜欢美女. 为什么我们都觉得婴儿很可爱. 如果回答因为巧克力很甜(很好吃), 这是倒果为因的思维方式. 之所以我们觉得巧克力很甜, 是因为糖的热量很高, 而进化让我们身体能够分辨哪些实物的热量更高, 让我们的生存机会更大. 因为巧克力本身没有一种内涵的属性叫做”甜”, 甜是我们的舌尖接触到巧克力通过一系列化学和电的神经反应, 大脑获得的一种意识体验. 类似的, 白光也并不是由三原色的光组成的, 白光其实包含了各种波长的光. 但是我们的眼睛只有3种感光细胞. 不同波长的光可以对3种感光细胞产生不同的激活值, 我们的大脑收到这些激活值以后, 就能分辨出这种光的颜色. 我们的液晶显示器, 其实是一种针对人脑的黑客设备, 显示器只提供三原色发光, 却能让我们的大脑感觉到其中内涵的整个世界. 某种意义上, 显示器hack了我们的大脑, 让我们大脑产生了错误, 当然这些”错误”其实正是我们的期望. 赤橙黄绿青蓝紫还有一个小问题, 为什么大致上会分出7种颜色. 这和我们大脑分辨力与负荷有关. 如果分的过多, 也就会过于模糊, 不利于人与人之间互相交流. 如果分的过少, 又会过于简单, 没法表达一个复杂世界的信息. 模因和基因类似, 但是是指一种可以复制传播的思想 引入类比: 一只蚂蚁往草尖上爬, 不停的爬, 是为什么? 因为蚂蚁被一种寄生虫寄生了, 这种寄生虫需要进入牛羊的身体, 所以蚂蚁的大脑被黑了, 爬上草尖就更容易被牛羊吃掉. 与此类似, 我们人类也被什么东西黑进了大脑. 哪些东西能让我们甘愿放弃生命? 自由 宗教 国家荣誉 等等 问一个问题: 大家觉得这种寄生虫是不是非常聪明? 寄生虫控制蚂蚁往草尖上爬, 这个行为真的是匪夷所思的聪明. 但是我们去观察寄生虫本身, 他的神经细胞非常稀少, 它本身的智商可能和西红柿/西瓜的差不多. 这个矛盾现象是为什么呢? 生物学家莱斯利·欧格(Leslie Orgel)曾写道: “演化比你更聪明。”(演化和进化都可以, 指的是同一个东西.) 这是因为这种聪明的行为存在于整个进化环境当中, 而不是在于寄生虫的大脑个体 不过我不太认同这句话, 我觉得如果说大自然有一个存在的目的的话, 那么就是为了进化出”由于进化本身的智慧” 例如我们人类的大脑, 他的智慧不在需要上万年的进化才能体现出智能行为, 大脑只需要学习几十天就能理解贝叶斯定理. 这是比大自然的进化本身更高效的智慧, 虽然这种智慧正是在大自然的进化这种智慧之下的产物. 这也正是是我选择做人工智能工作的动因. 因为我希望能创造, 或者至少是可以研究研究, 这种大自然花费几十亿年所得到的东西, 就是人类级别的智慧. 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"PyTorch","slug":"人工智能/pytorch/debug-CUDA-device-side-assert","date":"2019-10-05T04:14:22.000Z","updated":"2019-10-23T08:27:14.224Z","comments":true,"path":"wiki/人工智能/pytorch/debug-CUDA-device-side-assert/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/debug-CUDA-device-side-assert/","excerpt":"","text":"原文: Debugging CUDA device-side assert in PyTorchPyTorch的立即执行模型的美丽之处在于您实际上可以调试程序。 但是，有时CUDA执行的异步特性使其变得很难。 这是调试程序的一个小技巧。 当您使用CUDA操作运行PyTorch程序时，该程序通常不等到计算完成，而是继续向GPU抛出指令，直到它实际需要结果为止（例如，使用.item（）或.cpu（）进行评估或打印）。 尽管这是PyTorch程序出色性能的关键，但有一个弊端：当cuda操作失败时，您的程序可能正在执行其他操作(因为异步)。 通常的症状是，在触发错误的指令之后的某个地方，或多或少地出现随机性错误。 通常看起来像这样： 12345678---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-4-3d8a992c81ab&gt; in &lt;module&gt;() 1 loss = torch.nn.functional.cross_entropy(activations, labels) 2 average = loss/4----&gt; 3 print(average.item())RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/tv/pytorch/pytorch/aten/src/THC/generic/THCStorage.cpp:36 好吧，这很难理解，我敢肯定，报错位置是合法的代码。 因此，设备端断言意味着系统只是发现某个地方出了点问题。 这是导致此输出的错误程序： 12345678import torchdevice = torch.device('cuda:0')activations = torch.randn(4,3, device=device) # usually we get our activations in a more refined way...labels = torch.arange(4, device=device)loss = torch.nn.functional.cross_entropy(activations, labels)average = loss/4print(average.item()) 调试中的一种选择是将事物移至CPU。 但通常，我们使用库或复杂的东西就没法这么做。 所以现在怎么办？ 如果我们能获得良好的追溯，我们就能找到并解决问题。 以下是获得良好回溯的方式：您可以在环境变量CUDA_LAUNCH_BLOCKING设置为1的情况下启动程序。这也可以在python代码中解决：在程序的顶部，在导入任何内容（尤其是PyTorch）之前，先插入 12import osos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" 通过此添加，我们可以获得更好的错误回溯： 12345678910111213141516171819202122---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-4-3d8a992c81ab&gt; in &lt;module&gt;()----&gt; 1 loss = torch.nn.functional.cross_entropy(activations, labels) 2 average = loss/4 3 print(average.item())/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce) 1472 &gt;&gt;&gt; loss.backward() 1473 \"\"\"-&gt; 1474 return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce) 1475 1476 /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce) 1362 .format(input.size(0), target.size(0))) 1363 if dim == 2:-&gt; 1364 return torch._C._nn.nll_loss(input, target, weight, size_average, ignore_index, reduce) 1365 elif dim == 4: 1366 return torch._C._nn.nll_loss2d(input, target, weight, size_average, ignore_index, reduce)RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/tv/pytorch/pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:116 显然，损失函数的输入不合法。 实际上，我们的激活形状批处理为x 3，因此我们只允许三个类别（0、1、2），但标签的值为3！ 这对特殊情况也适用, 如果我们能恢复计算中的非GPU位而不需要完全重启就好了.(The best part is that this also works for nontrivial examples. Now if only we could recover the non-GPU bits of our calculation instead of needing a complete restart… 这句话翻译的不太好, 有建议可以联系我邮箱ofyu@163.com) 12345678910RuntimeError: copy_if failed to synchronize: device-side assert triggeredRuntimeError: CUDA error: device-side assert triggeredTHCudaCheck FAIL file=/pytorch/aten/src/THC/THCCachingHostAllocator.cpp line=296 error=59 : device-side assert triggered*** RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCCachingHostAllocator.cpp:296# 本次出错, 是因为词向量的id超出了词向量维度, 还有可能id=-1,或者label超出维度等等","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[]},{"title":"通过正念减少压力,提升专注力","slug":"读书笔记/学习方法/使用正念减少压力","date":"2019-10-03T02:19:18.000Z","updated":"2019-10-29T05:12:07.851Z","comments":true,"path":"wiki/读书笔记/学习方法/使用正念减少压力/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/学习方法/使用正念减少压力/","excerpt":"","text":"参考:知乎live: &lt;&lt;如何通过科学”正念”管理压力,提升专注力&gt;&gt; 作者: 刘轩 学习笔记为什么我们总是做不好大脑中的悬念太多 做不完 做不好 放大缺点 做事分心 现代科技的信息过载 如何准备 宽松舒适的衣服 能够专注/独处/不被打扰的房间(不背靠门窗, 或关闭挡住门窗) 环境温度合适 噪音合适(可以播放白噪音) 用能够坐直,不需要靠背和扶手的凳子 要做的不要做的要的: 保持放松 给自己时间, 不要急于求成, 缓慢成长才是正常的 对自己温柔一点 不要: 太使劲想练成功 急迫仓促 对自己的专注力感到懊恼 体验1: 练习用呼吸当锚点456呼吸法: 4秒吸气 5秒 停 6秒 呼气 口诀: 吸 2 3 4 停 2 3 4 5 呼 2 3 4 5 6 这时不必须闭眼 可以闭上眼睛, 或者看着固定的位置(比如地面) 把注意力集中在鼻子/嘴部/腹部 尽量用腹部(丹田)来呼吸:先尽情的呼吸, 感受呼气时腹部扩展,吸气时腹部收缩 这个呼吸就是我的锚点,随意可以控制的动作(30秒来找回自己的锚点, 找回自己的稳定锚点) 肩膀放松, 背部挺直 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"},{"name":"学习方法","slug":"读书笔记/学习方法","permalink":"http://yoursite.com/categories/读书笔记/学习方法/"}],"tags":[]},{"title":"cs224n-02词向量","slug":"人工智能/cs224n笔记/02词向量","date":"2019-09-22T02:22:32.000Z","updated":"2019-10-08T11:43:15.171Z","comments":true,"path":"wiki/人工智能/cs224n笔记/02词向量/","link":"","permalink":"http://yoursite.com/wiki/人工智能/cs224n笔记/02词向量/","excerpt":"","text":". 一些思想机器学习的优化一般喜欢做最小化, 而不是最大化, 只需要一个负号即可进行这种转变 wordnet 语义词典12345# 使用nltk打开wordnetfrom nltk.corpus import wordnet panda = wordnet.synset('panda.n.01')hyper = lambda s: s.hypernyms()list(panda.closure(hyper)) 分布相似性上下文相似的单词, 语义有相似性 word2vec主要思想: 预测每个单词和他们的上下文单词 2种算法: Skip-grams / Continuous Bag of Words(CBOW) 2种训练方法: hierarchical softmax / Negative sampling Skip-grams(SG)每个单词有2个向量表示, 一个用于输入, 一个用于计算输出上下文的概率, 这样在计算上更简单 softmax为什么求指数, 这样能把任何浮点数转化成正数 除以指数的和, 是为了归一化, 时所有值的和为1, 这样时物理意义上变成了”概率” 之所以叫”softmax”, 因为如果你取指数时,就接近于一个最大值函数, 这样大的数值会进一步放大, 结果他们占绝对主导, 就像一个 max 函数, 但仍然是一个软性操作(soft) 词汇表distributional 英文 中文 distributional 分布式 distributed representations 分布式表示(用密集型向量表示词汇的含义) co-occur 共现 distributional simliarity 分布相似性(上下文相同的2个单词的相似性) hyper parameters 超参数 first principal component 第一主成分 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"cs224n笔记","slug":"人工智能/cs224n笔记","permalink":"http://yoursite.com/categories/人工智能/cs224n笔记/"}],"tags":[]},{"title":"datetime","slug":"python/datetime","date":"2019-08-31T07:25:52.000Z","updated":"2019-10-08T09:22:51.338Z","comments":true,"path":"wiki/python/datetime/","link":"","permalink":"http://yoursite.com/wiki/python/datetime/","excerpt":"","text":". time 模块12345678 time.time()---&gt; time_stamp(秒级时间戳)time.localtime(time_stamp=None)time_stamp ---&gt; time.struct_timetime.strftime('%Y-%m-%d %H:%M:%S', struct_time)struct_time ---&gt; str_time 文本转时间戳123456a1 = \"2019-5-10 23:40:00\"# 先转换为时间数组timeArray = time.strptime(a1, \"%Y-%m-%d %H:%M:%S\") # 转换为时间戳timeStamp = int(time.mktime(timeArray)) datetime 模块参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"写代码技巧","slug":"编程思维/写代码技巧","date":"2019-08-27T13:38:55.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程思维/写代码技巧/","link":"","permalink":"http://yoursite.com/wiki/编程思维/写代码技巧/","excerpt":"","text":". 多留历史数据, 避免无法逆转的操作例如当使用新数据库时, 把老的数据库move到其他位置, 而不彻底删除 参考资料","categories":[{"name":"编程思维","slug":"编程思维","permalink":"http://yoursite.com/categories/编程思维/"}],"tags":[]},{"title":"oracle的SQL基础","slug":"人工智能/搜索引擎/sql-oracle","date":"2019-08-22T10:27:03.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/sql-oracle/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/sql-oracle/","excerpt":"","text":". 查看全局信息查看所有表123select table_name from user_tables; -- 查看当前用户拥有的表select table_name from all_tables; -- 查看所有用户的表select table_name from dba_tables; -- 查看所有用户的表包括系统表 查看所有表字段12345select * from user_tab_columns where TABLE_NAME='某表名称'； --查看当前用户下某表所有字段select * from all_tab_columns where TABLE_NAME='某表名称'； select * from dba_tab_columns where TABLE_NAME='某表名称'； user_tab_columns：table_name,column_name,data_type,data_length,data_precision,data_scale,nullable,column_id等all_tab_columns ，dba_tab_columns比user_tab_columns多了一个ower 查看表注释和字段注释123select * from user_tab_comments --查看当前用户下所有表注释select * from user_col_comments where TABLE_NAME='某表名称'； --查看当前用户下某表所有字段注释 user_tab_comments：table_name,table_type,commentsuser_col_comments：table_name,column_name,comments 表内常用SQL123456789101112131415161718192021-- 单行注释/* 多行注释 */-- 普通语句select * from db.tablename where field is not null-- and/or SELECT * FROM Websites WHERE country='CN' AND (alexa &gt; 50 or name is null) ;-- oracle没有limit, 可以用rownum where rownum&lt;=5; --/* 统计数据 */select sum(1) from db.tablename-- 聚合xxx字段的所有值select xxx from db.tablename group by xxx, 'yy yy', zzz-- 排序 desc=降序 asc=升序order by xxx1 desc;-- 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[]},{"title":"elastic-search插件","slug":"人工智能/搜索引擎/elastic-search插件","date":"2019-08-01T08:37:07.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/elastic-search插件/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/elastic-search插件/","excerpt":"","text":"命令 elasticsearch-plugin123elasticsearch-plugin list #查看已经安装的插件elasticsearch-plugin install #安装插件elasticsearch-plugin remove #卸载插件 IK分词器配置 IK分词器的词典配置文件路径 1&#123;$elasticsearch_dir&#125;/config/analysis-ik/IKAnalyzer.cfg.xml 配置内容 12345678&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户词典, 注意是相对于ik插件所在目录的相对路径, 而且即便写绝对路径也会被拼接 --&gt; &lt;!--dict/date 绝对路径为 &#123;$elasticsearch_dir&#125;/config/analysis-ik/dict/date.dic --&gt; &lt;entry key=\"ext_dict\"&gt;dict/date.dic;dict/ext.dic&lt;/entry&gt;&lt;/properties&gt; 词库文件 文本文件, 一行一个词汇 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[]},{"title":"安装centos","slug":"编程基础/安装与配置/安装centos","date":"2019-07-22T09:47:47.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/安装与配置/安装centos/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/安装centos/","excerpt":"","text":"安装docker12345678910111213141516171819202122232425# 使用 root 权限登录 Centossu# yum 包更新到最新。yum update# 卸载旧版本(如果安装过旧版本的话)yum remove docker docker-common docker-selinux docker-engine# 安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的yum install -y yum-utils device-mapper-persistent-data lvm2# 设置yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# 可以查看所有仓库中所有docker版本，并选择特定版本安装yum list docker-ce --showduplicates | sort -r[结果见下图]# 安装dockeryum install docker-ce #由于repo中默认只开启stable仓库，故这里安装的是最新稳定版17.12.0yum install &lt;FQPN&gt; # 例如：sudo yum install docker-ce-17.12.0.ce# 启动并加入开机启动systemctl start dockersystemctl enable docker# 验证安装是否成功(有client和service两部分表示docker安装启动都成功了)docker version 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"vim","slug":"编程基础/软件使用备忘/使用vim","date":"2019-07-12T01:22:33.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程基础/软件使用备忘/使用vim/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/使用vim/","excerpt":"","text":"参考资料 命令模式 u 撤销操作 Ctrl+r 恢复上一步被撤销的操作 yy 复制单行 p 粘贴（到当前行下面） 数字yy 复制当前行到下面N行（适用于少量行复制） 6,9 co 12 把6至9行copy到12行下面 编辑模式修改配置 ~/.vimrc 命令 说明 set nu 显示行号","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"错误处理","slug":"编程思维/错误处理","date":"2019-07-12T01:14:28.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程思维/错误处理/","link":"","permalink":"http://yoursite.com/wiki/编程思维/错误处理/","excerpt":"","text":"参考资料 报错和日志的判断异常应该抛出而不是返回。 只判断允许出现的情况的条件分支。不要用try来包裹不允许出现的情况。 不允许出现的情况就在server的最底层报错机制中展示出来。这样能避免小错误无法找到。 异常的分类就是定几个大类，如 参数校验出错的异常CheckException，返回到前台是1，没有登录是UnloginException，返回到前台是2，没有权限是3. 前台拿到了异常，发现是1，就提示返回结果的msg，发现是2，就打开登录对话框，发现是3，然后又怎么样处理。。。 异常不能不分，但不能分太细，否则前台无法玩。","categories":[{"name":"编程思维","slug":"编程思维","permalink":"http://yoursite.com/categories/编程思维/"}],"tags":[{"name":"编程风格","slug":"编程风格","permalink":"http://yoursite.com/tags/编程风格/"}]},{"title":"编程的基础知识","slug":"编程思维/什么才是编程的基础知识？","date":"2019-07-12T01:12:19.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程思维/什么才是编程的基础知识？/","link":"","permalink":"http://yoursite.com/wiki/编程思维/什么才是编程的基础知识？/","excerpt":"","text":"参考资料 什么才是java的基础知识？https://xwjie.github.io/note/basic.html 近日里，很多人邀请我回答各种j2ee开发的初级问题，我无一都强调java初学者要先扎实自己的基础知识，那什么才是java的基础知识？又怎么样才算掌握了java的基础知识呢？这个问题还真值得仔细思考。 我做j2ee开发已经超过十载，作为过来人，心路历程估计和大家差不多。编码的前几年，很长一段时间觉得java简单，开发实现各种功能都很轻松，代码写起来根本不费劲（主要是因为写的代码都是一些功能业务逻辑）。但同时自己心里明白，自己其实没有什么水平，自己这3,4年以来学懂的东西就那么多，其他人几个月就可以学会，自己的竞争力在哪里？这种矛盾的心理，困扰了我非常长的时间，非常的苦恼！总有一种报国无门无处发力的感觉。 这个时期，热衷了使用各种框架，各种api，常以今天学习了某个api，组件，框架的使用就觉得自己学到了东西，设计模式也看过不止一次，但都没有啥感觉。一方面很努力学习，一方面又觉得不踏实，因为例如这个api我知道而你不知道，但我告诉你之后你就知道了，那我比你的优势在哪里呢？苦恼*2 过了很长一段这种惶惶不可终日的日子，决定自己要改变，改变的方向就是阅读自己用到的java相关的源代码，看看jdk是如何实现的。就从基本的数据结构看，然后看多线程相关，在学习前台等等。写的代码还是那些代码，代码还是那么简单，但我力求做到知道代码背后的真相，这就是我最开始努力的方向。于是不再把时间都花在追求各种新框架、新API的使用上，每天都花时间在看实现原理上。就这样过了大半年左右，终于不再迷茫，不会在觉得自己只懂api的使用，觉得自己没有那么肤浅了，说脱胎换骨也不为过。那段时间，是我成长最快的时期，也是最充实的一段时光。 Talk is cheap，show me the code。举例说明大家会比较有感觉。 如学习了hashmap的源代码知道了工作原理之后，使用hashmap 1Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); 代码还是那个代码，但我已经知道了hashmap背后的东西 数据结构是链表的数组（注：后面的版本为了提升性能，已经是改成链表或者树（节点较多）了） 思想上是空间换时间的算法 构造函数上有容量和负载因子2个参数以及作用 决定性能的是key的hashcode是否够快、结果够分散（不分散就会变成链表的性能了），和扩容的开销（什么时候扩容，和负载因子有关） 然后写代码的时候，如果知道了最终的容量（尤其是数据量大的时候），我都会指定初始化容量，类似如下 12List&lt;SomeBean&gt; list = doSomeThing(); Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;((int)(list.size()/0.75));//0.75为默认负载因子 如果工作中某个map使用特别多，性能还需要继续优化，我就会考虑从以下方面优化 如果key是自己定义的对象，那么hashcode方法是否够快（最少应该缓存保证只计算一次，而且放入之后不能改变，决定hashcode的字段不能改变）？ hash的结果是否够分散？ 可以考虑调小负载因子，花更多的空间来换时间 学习源代码的时候，特别有意思，你会强烈感觉到一个词：举一反三！触类旁通！ 学习api使用的时候，如果你只知道使用不知道原理，很难举一反三，感觉的是死记硬背。但学习了原理之后，知识成体系后，很容易举一反三，学的越多就容易，还是以hashmap为例，我举一个hashmap反三个点。 你会知道但凡有数组的数据结构，构造函数都有一个容量的初始化参数（或者说构造函数有初始化容量的可能都是数组的数据结构）。构造函数如下 123public ArrayList(int initialCapacity) //LinkedList不是数组就没有public HashMap(int initialCapacity) public StringBuffer(int capacity) 你就会知道，数组扩容很耗性能（数据量大容易oom），尽量指定容量。 算法是空间换时间，还有没有其他算法是这种思想的？你最少能找到一个桶排序。 数据库的分库分表，思路和hashmap大同小异 各种分布式的hash一致性算法，第一步都是创建一个最大的数组（Integer.MAX_VALUE）,就是避免了hashmap最耗性能的扩容运算。 学习了hashmap之后，你很自然就会去了解其他的map，如TreeMap，LinkedHashmap（超级有用），HashTable，ConcurrentSkipListMap（算法思路很有意思），ConcurrentHashMap等，你会知道set就是用map做的，都不需要学。到了这步，map相关就可以暂告一段落。 在学习中，我发现思想上的东西是最重要的，你理解了思想，一下子就豁然开朗了，在也不需要死记硬背了。如学习CAS的时候，大家都知道这是一种指令级的免锁实现。看代码的时候，我一度疑惑为什么会有个while死循环（原谅我天资驽钝） 12345678public final int getAndUpdate(IntUnaryOperator updateFunction) &#123; int prev, next; do &#123; prev = get(); next = updateFunction.applyAsInt(prev); &#125; while (!compareAndSet(prev, next)); return prev;&#125; 后来从思想上理解，才知道乐观锁的概念，就是很乐观，假设你不会出错，但你要是出错了我就重试有办法给你修复，对应的就是悲观锁，就是很悲观，觉得不锁就会出错，如synchronize关键字和reentrantlock。这体现了2种不同截然不同的管理思想。这种思想经常体现在多个系统集成的设计，有些时候如果你用悲观的思想设计，实现起来很麻烦或者无法实现，但如果你用乐观的思想，减少出错条件，然后出错了能解决，代价就会小很多。 说了这么多，我想说的就是，j2ee的基础知识就是你做项目中代码背后的东西。提高自己水平的方法很简单，就是把大部分时间去了解实现原理，了解思想，让自己的知识串起来，形成体系。j2ee的知识特别多，学得人想哭，千万不要一开始把时间花在各种框架、组件的使用上，在我看来那是本末倒置。简单来说：先修内功再练招式。 我觉得重要的、工作会用得到的知识就是一个请求从前台到后台处理的过程需要用到的东西，最少包括以下点：js，html，css，ajax，ajax跨域，跨站脚本，web缓存，web优化，nginx，apache作用，鉴权方式，cookie，session，servlet，filter，基本数据结构，线程池，线程并发，缓存，io等等，知识点非常多。如你前台用jq，你应该了解他的选择器和ajax是如何实现的（其实去了解就会发现不复杂）？而不是只是会用。后台你用springmvc，你要了解他是如何工作，每一个配置是做什么，为什么？ j2ee知识点特别多，每一个都能写很多，我也在不断学习中。具体要写我还真不知道如何下手，我就列举一下我觉得基础的东西（面试的时候问的问题），有简单有难，你觉得偏可能是你没有做过这块的开发或者做得比较浅： map有哪些，特点和使用场景？（只知道hashmap，hashtable是不够的。。。） 哪些方面会影响hashmap的性能？ 线程安全的map有哪些，concurrenthashmap是如何实现线程安全的（jdk1.8大不同）？ 锁有哪几种？ 公平锁，读写锁等如何实现？ synchronize能加在哪些地方？什么区别？ 死锁的形成条件？现在很少死锁了，很少问 原子数据对象的原理？ reentrantlock相关知识，condition如何使用？（很重要的知识点，强烈推荐阅读ArrayBlockingQueue源码，教科书般） volatile的相关知识（内存屏障，重排） ThreadLocal原理和使用？（超级有用的知识点，工作中使用很多，让代码漂亮很多，后面专门开贴写） 多个线程同步等待？（CountDownLatch，CyclicBarrier，Semaphore信号量很多语言都有，实际上使用不是很多，线程池就可以实现大部分等待功能） 线程池？（种类，重要的方法，这个一般是使用层面，简单） 动态代理？反射？内省？（考察知识面） session相关知识？和cookie关系？分布式session实现原理？ cookie相关知识？有哪些属性？（有些属性很有用，只是我们很少留意而已！） nginx，apache 实际项目能做哪些？（鉴权，转发，缓存，反向代理等）和tomcat什么关系？最少了解 ajax跨域原因？解决方式？（重点知识，做SE避免不了的问题。这里很多知识点。） jsonp原理？后台需要改动吗？（jsonp虽然现在落伍了，但还是会问问） web优化知识点？（常规知识点） 前台缓存相关？（200cache,304，ajax缓存，如何实现缓存） 一列举就根本停不下来了。。。其他的spring框架的东西也很多，还有jvm的东西，系统集成相关，数据库相关，io做得很少也不懂问，后面再慢慢把我的学习过程和偶得写下来。很多东西我也是了解个大概，就是看看你有没有学习过，不断学习是程序员最重要的特征。 我不算高手，只能算一个合格的老程序员。这里只是说了一下自己之前学习的方向和列举了几个学习中的例子，大家见仁见智。帖子也是针对迷茫的初学者有感而发，希望能帮助到大家。 最后我总结一下：初学者先广在精，关注代码背后的实现，关注内功修炼，了解实现原理和思想，形成自己完整的技术体系，知识成片之后就容易触类旁通，进步的速度就会越来越快。最后以我在每一个项目组和开发人员聊天都会说的几个例子结尾：“少林功夫里面有功和拳之分，马步功，石锁功是功，蛇拳猴拳是拳，你不可能练会了蛇拳猴拳就能打人，你必须先重点练功。乔峰在聚贤庄用太祖长拳把大家打得落花流水，我们用太祖长拳就只是个广播体操。同样，我们要分清编程里面那些是功那些是拳，代码背后的实现和思想是功，各个框架、api使用是拳。初学者应该大部分时间花在练功上，功到了拳自然就有了，切勿本末倒置。”谢谢大家阅读！","categories":[{"name":"编程思维","slug":"编程思维","permalink":"http://yoursite.com/categories/编程思维/"}],"tags":[{"name":"编程风格","slug":"编程风格","permalink":"http://yoursite.com/tags/编程风格/"}]},{"title":"pytorch报错","slug":"人工智能/pytorch/pytorch报错","date":"2019-07-12T01:09:53.000Z","updated":"2019-10-09T15:12:06.703Z","comments":true,"path":"wiki/人工智能/pytorch/pytorch报错/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/pytorch报错/","excerpt":"","text":"参考资料 异常: ModuleNotFoundError: No module named ‘past’ 解决方案: pip3 install future 参考: https://blog.csdn.net/qq_31282773/article/details/78672584 https://www.cnblogs.com/huolifeng/p/6412183.html eq() received an invalid combination of arguments - got (type), but expected one of:(float other) net-Module内部的 self.type 不能被覆盖。 不能self.type=net() 这样～～～ RuntimeError: multi-target not supported at /pytorch/torch/lib/THNN/generic/ClassNLLCriterion.c:22 CrossEntropyLoss(y, truey)应该： y二维(n, 分类数) truey一维(n) RuntimeError: invalid argument 1: input is not contiguous at /pytorch/torch/lib/TH/generic/THTensor.c:231 对象不是连续的（在显存或内存里） input.contiguous() 即可 RuntimeError: save_for_backward can only save input or output tensors, but argument 0 doesn’t satisfy this condition 函数需要需要反向传播时，输入必须是Variable，不能是tensor ValueError: Expected more than 1 value per channel when training 在训练时，期望多余1个值（例如nn.BatchNorm1d必须输入batch&gt;1的数据） 可以用model.eval() 来进入预测模式，避免此问题","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"}]},{"title":"pytorch--tensor","slug":"人工智能/pytorch/pytorch-tensor","date":"2019-07-12T01:09:44.000Z","updated":"2019-10-08T11:43:15.171Z","comments":true,"path":"wiki/人工智能/pytorch/pytorch-tensor/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/pytorch-tensor/","excerpt":"","text":"参考资料 Tensor 作用 tensor的方法() 查看具体的数据类型 tensor.type() 转成list tensor.tolist() 取值（只有一个元素） tensor.item() 使tensor可以求导 tensor. requires_grad() 作用 tensor的属性 是否求导 tensor.requires_grad 创建tensor 创建0维张量（标量scalar） torch.tensor(3.1416) （注意小写）","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"文本操作","slug":"python/文本操作","date":"2019-07-12T01:07:47.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/文本操作/","link":"","permalink":"http://yoursite.com/wiki/python/文本操作/","excerpt":"","text":"参考资料 format 代码 说明 print(‘{:0&gt;2}’.format(3)) 2位数，空的用0补充，&gt;在左边补","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python基本概念","slug":"python/迭代器和生成器","date":"2019-07-12T01:07:00.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/迭代器和生成器/","link":"","permalink":"http://yoursite.com/wiki/python/迭代器和生成器/","excerpt":"","text":"参考资料 迭代器 Iterator12345i = iter(nums) next(i) 迭代器没有长度，它们不能被索引。惰性，只能使用一次，只能循环遍历一次。可以无限长，因为不调用next时什么也不做，不占多余的内存 生成器 Generator1234生成器是迭代器的子类。g = (n**2 for n in nums)生成器所使用的场景是迭代前不生成这些数据，迭代后不再需要这些数据。如果需要多次使用的数据，不应使用生成器。 可迭代对象 Iterable1可以用for遍历的对象 Python 中的每一种迭代都依赖于迭代器协议，因此理解迭代器协议是理解 Python 中的循环的关键。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python-xml解析","slug":"python/python-xml","date":"2019-07-12T00:58:58.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python-xml/","link":"","permalink":"http://yoursite.com/wiki/python/python-xml/","excerpt":"","text":"参考资料 asd xml–可扩展标记语言12345678910111213&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;catalog&gt; &lt;maxid&gt;4&lt;/maxid&gt; &lt;login username=\"pytest\" passwd='123456'&gt; &lt;caption&gt;Python&lt;/caption&gt; &lt;item id=\"4\"&gt; &lt;caption&gt;test&lt;/caption&gt; &lt;/item&gt; &lt;/login&gt; &lt;item id=\"2\"&gt; &lt;caption&gt;Zope&lt;/caption&gt; &lt;/item&gt;&lt;/catalog&gt; xml是由标签对组成，1- 标签可以有属性： ``` &lt;aa id=&apos;123&apos;&gt;&lt;/aa&gt; 标签对可以嵌入数据：1234567891011121314151617181920 - 标签可以嵌入子标签（具有层级关系）### 获取标签和属性```python#coding: utf-8import xml.dom.minidomdom = xml.dom.minidom.parse(&quot;xxx.xml&quot;) #打开xml文档root = dom.documentElement #得到xml文档对象print(&quot;nodeName:&quot;, root.nodeName) #每一个结点都有它的nodeName，nodeValue，nodeType属性print(&quot;nodeValue:&quot;, root.nodeValue) #nodeValue是结点的值，只对文本结点有效print(&quot;nodeType:&quot;, root.nodeType)print(&quot;ELEMENT_NODE:&quot;, root.ELEMENT_NODE)# 输出 nodeName: catalog# 输出 nodeValue: None# 输出 nodeType: 1# 输出 ELEMENT_NODE: 1 nodeType是结点的类型。catalog是ELEMENT_NODE类型 节点编号： 节点类型的名称： 说明 返回的节点名称 返回的节点值 1 Element 表示一个元素 元素名称 null 2 Attribute 代表一个属性 属性名称 属性值 3 Text 代表元素或属性的文本内容 #text 节点的内容 4 CDATA Section 代表文档中的 CDATA 区段（文本不会被解析器解析） #cdata-section 节点的内容 5 Entity Reference 代表一个实体引用 实体引用名称 null 6 Entity 代表一个实体 实体名称 null 7 Processing Instrucion 代表一个处理指令 目标 节点的内容 8 Comment 代表一个注释 #comment 注释文本 9 Document 代表整个文档（DOM 树的根节点） #document null 10 Document Type 为文档中定义的实体提供了一个接口 文档类型名称 null 11 Document Fragment 代表”轻量级”的 Document 对象，它可以保留文档中的一部分 #document fragment null 12 Notation 定义一个在 DTD 中声明的符号 符号名称 null w3school 菜鸟教程–更详细 获取子标签123456bb = root.getElementsByTagName('maxid')print(type(bb)) # === &lt;class 'xml.dom.minicompat.NodeList'&gt;print(bb) # === [&lt;DOM Element: maxid at 0x2707a48&gt;]b = bb[0]print(b.nodeName) # === maxidprint(b.nodeValue) # === None 12345678910111213141516171819202122232425262728minidom.parse(filename)加载读取XML文件 doc.documentElement获取XML文档对象 node.getAttribute(AttributeName)获取XML节点属性值 node.getElementsByTagName(TagName)获取XML节点对象集合 node.childNodes #返回子节点列表。 node.childNodes[index].nodeValue获取XML节点值 node.firstChild#访问第一个节点。等价于pagexml.childNodes[0] doc = minidom.parse(filename)doc.toxml('UTF-8')返回Node节点的xml表示的文本 Node.attributes[\"id\"]a.name #就是上面的 \"id\"a.value #属性的值访问元素属性","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"xml","slug":"xml","permalink":"http://yoursite.com/tags/xml/"}]},{"title":"C++基础","slug":"编程基础/C++/base-cpp","date":"2019-07-12T00:57:45.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/编程基础/C++/base-cpp/","link":"","permalink":"http://yoursite.com/wiki/编程基础/C++/base-cpp/","excerpt":"","text":"参考资料 安装g++编译器12sudo apt-get install build-essentialgcc --version 编译源文件1234567891011121314151617//《hello.cpp》源文件内容：#include &lt;iostream&gt;using namespace std;int main() &#123; cout &lt;&lt; \"Hello, world!!!!\" &lt;&lt; endl; return 0;&#125;// 编译hello.cpp （-c 表示只编译，不链接）g++ -c hello.cpp// 链接hello.o生成hello.out (-o 表示输出文件)g++ -o hello.out hello.cpp// 运行hello.out./hello.out 构建项目1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// .h和.cpp文件相互配合 需要用 Make工具来构建项目# hw2.cpp#include \"solution.h\"int main () &#123; Solution sln; sln.Say(); return 0;&#125;# solution.h/* solution.h */class Solution &#123;public: void Say();&#125;;# solution.cpp/* solution.cpp */#include &lt;iostream&gt;#include \"solution.h\"void Solution::Say()&#123; std::cout &lt;&lt; \"HI!!!!!!\" &lt;&lt; std::endl;&#125;// 创建一个makefile文件，以告诉Make如何编译和链接程序# 生成可执行文件build，prerequisites有两个.o文件，是因为代码里hw2引用了solution.h。build : hw2.o solution.o g++ -o build hw2.o solution.o #注意前面必须是tab，不能是空格# 编译hw2.cpp，生成hw2.o文件，-g 表示生成的文件可用gdb调试，如果没有-g，调试时无法命中断点hw2.o : hw2.cpp solution.h g++ -g -c hw2.cpp# 编译solution.cpp文件，生成solution.o文件。solution.o : solution.h solution.cpp g++ -g -c solution.cpp# 清除标签: make不找冒号后的依赖关系，也不自动执行命令。如果要执行该命令，必须在make后指出动作名字，如make cleanclean : rm hw2.o solution.o build # makefile的格式:target ... : prerequisites ...(空格区分多个文件) command #注意前面是tab // target这一个或多个目标，依赖于prerequisites列表中的文件，其执行规则定义在command里。// 如果prerequisites列表中文件比target要新，就会执行command，否则就跳过。// make命令的流程1 make在当前目录下找名为makefile或Makefile的文件；2 如果找到，它会找文件中的第一个target，如上述文件中的build，并作为终极目标文件;3 如果第一个target的文件不存在，或其依赖的.o 文件修改时间要比target这个文件新，则会执行紧接着的command来生成这个target文件;4 如果第一个target所依赖的.o文件不存在，则会在makefile文件中找target为.o的依赖，如果找到则执行command，.o的依赖必是.h或.cpp，于是make可以生成 .o 文件了5 回溯到第2步执行最终目标./build # 执行文件// makefile:2: *** missing separator。 停止。 可能是Tab被替换成了空格~ VSCode 配置 makefile用tab(不替换成空格)“[makefile]”: { &quot;editor.quickSuggestions&quot;: false, &quot;editor.formatOnSave&quot;: true, &quot;editor.renderWhitespace&quot;: &quot;all&quot;, &quot;editor.acceptSuggestionOnEnter&quot;: &quot;off&quot;, &quot;editor.detectIndentation&quot;: false, }","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"C++","slug":"编程基础/C","permalink":"http://yoursite.com/categories/编程基础/C/"}],"tags":[]},{"title":"javascript基础","slug":"编程基础/javascript/javascript","date":"2019-07-12T00:55:19.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/javascript/javascript/","link":"","permalink":"http://yoursite.com/wiki/编程基础/javascript/javascript/","excerpt":"","text":"参考资料 网页中的JavaScript位置 onclick1234567891011121314152. ```&lt;a href=&quot;javascript:脚本的文本内容&quot;&gt;&lt;/a&gt; ``` 超链接href属性3. ```&lt;head&gt;&lt;script&gt;脚本内容&lt;/script&gt;&lt;/head&gt; ``` 内部脚本4. ```&lt;head&gt;&lt;script src=&quot;脚本位置&quot;&gt;&lt;/script&gt;&lt;/head&gt; ``` 引用脚本文件（若有src，内部脚本无效）### 网页中的JavaScript命令```JavaScript# 基本语法规则# 每个语句以分号结尾# 空格和换行都会被无视# 显示命令alert(&quot;提醒框文本&quot;)console.log(&quot;控制台输出文本&quot;)document.write(&quot;网页body中要写入的内容&quot;) 代码/代码块/函数12345678910111213141516171819202122232425262728293031# 语句用分号分隔(分号可选)。JavaScript语句是发给浏览器的命令。语句的作用是告诉浏览器该做什么。# JavaScript代码是 JavaScript语句的序列。 浏览器按照编写顺序依次执行每条语句# 代码块把代码分批地组合起来。 代码块以左花括号开始，以右花括号结束。# 函数是由事件驱动的或者当它被调用时执行的可重复使用的代码块。// 注释用2个撇// JavaScript的关键字必须以 字母 下划线_ 美元符$ 开始。// 函数字面量 或 定义一个函数function myFunction(a, b) &#123; return a * b;&#125; // 定义一个函数const Test = function () &#123; &#125;// ES6 新增箭头函数! 使用箭头函数定义函数时可以省略 function 关键字const Test = (...params) =&gt; &#123; &#125;// ES6 新增箭头函数! 该函数只有一个参数时可以简写成：const Test = param =&gt; &#123; return param; &#125;x = 123 + 'wxy' # x = '123wxy' JavaScript会自动把数字转化成字符串// 文本字符串中使用反斜杠对代码行进行换行document.write(\"你好 \\世界!\");// 字典在JavaScript里就是一个对象// 访问一个对象的属性 obj.param obj['param'] x = obj.param 变量1234567891011121314151617181920// var 声明一个变量，如果再次声明这个变量值不变# 未赋值的变量实际 = undefined var lastname=\"Doe\", age=30, job=\"carpenter\"; // 赋值未null来清空变量：cars=null;// let 声明一个块级作用域的变量、语句或者表达式。在Function中局部变量推荐使用let变量，避免冲突// 作用域规则 let 声明的变量只在其声明的块或子块中可用作用域1 var var1 在其子作用域 var var1=2 则在作用域1的var1也=2作用域1 let var2 在其子作用域 let var2=2 则在作用域1的var2=undefined# 全局 JavaScript 变量// 在函数外声明的变量是全局变量，网页上的所有脚本和函数都能访问它。# JavaScript 变量的生存期// JavaScript 变量的生命期从它们被声明的时间开始。// 局部变量会在函数运行以后被删除。// 全局变量会在页面关闭后被删除。# 如果您把值赋给尚未声明的变量，该变量将被自动作为 window 的一个属性。 对象12345678910111213141516171819202122232425// JavaScript 对象是属性和方法的容器。// 对象的属性之间一定要用逗号隔开(同名属性只保留最后赋的值,方法也是属性)// 对象的方法定义了一个函数，并作为对象的属性存储。对象方法通过添加 () 调用 (作为一个函数)。// 创建一个对象,并且有一个方法:fullnamevar person = &#123; firstName: \"John\", lastName : \"Doe\", id : 5566, fullName : function() &#123; return this.firstName + \" \" + this.lastName; &#125;&#125;;# 对象构造器function person(firstname,lastname,age,eyecolor)&#123; this.firstname=firstname; this.lastname=lastname; this.age=age; this.eyecolor=eyecolor;&#125;var myFather=new person(\"John\",\"Doe\",50,\"blue\");//JavaScript中，this通常指向的是我们正在执行的函数本身，或者是指向该函数所属的对象（运行时） 对象123456789101112131415HTML 事件是发生在 HTML 元素上的事情。HTML 事件可以是浏览器行为，也可以是用户行为。 HTML 页面完成加载 HTML input 字段改变时 HTML 按钮被点击//HTML 元素中可以添加事件属性，使用 JavaScript 代码来添加 HTML 元素。单引号/双引号:&lt;some-HTML-element some-event='JavaScript 代码'&gt;&lt;some-HTML-element some-event=\"JavaScript 代码\"&gt; 事件可以用于处理表单验证，用户输入，用户行为及浏览器动作: 页面加载时触发事件 页面关闭时触发事件 用户点击按钮执行动作 验证用户输入内容的合法性 等等 ... 常见事件 描述 onchange HTML 元素改变 onclick 用户点击 HTML 元素 onmouseover 用户在一个HTML元素上移动鼠标 onmouseout 用户从一个HTML元素上移开鼠标 onkeydown 用户按下键盘按键 onload 浏览器已完成页面的加载 更多事件列表: JavaScript 参考手册 - HTML DOM 事件。 浏览器对象模型（Browser Object Model (BOM)）和 HTML DOM Document 对象123456789101112131415161718192021222324252627282930313233343536373839404142#window 对象。它表示浏览器窗口。所有 JavaScript 全局对象、函数以及变量均自动成为 window 对象的成员。 全局变量是 window 对象的属性。 全局函数是 window 对象的方法。// HTML DOM 的 document 也是 window 对象的属性之一, 下面两条语句相同:window.document.getElementById(\"header\"); document.getElementById(\"header\"); // 所有浏览器的窗口宽度和高度（不包括工具栏/滚动条）(兼容)var w=window.innerWidth|| document.documentElement.clientWidth|| document.body.clientWidth;var h=window.innerHeight|| document.documentElement.clientHeight|| document.body.clientHeight;# 注意 window的属性作为保留字,不能定义成变量//screen 对象包含有关用户屏幕的信息。 screen.availWidth //可用的屏幕宽度 screen.availHeight //可用的屏幕高度//location 对象用于获得当前页面的地址 (URL)，并把浏览器重定向到新的页面。 location.hostname //返回 web 主机的域名 location.pathname //返回当前页面的路径和文件名 location.port //返回 web 主机的端口 （80 或 443） location.protocol //返回所使用的 web 协议（http:// 或 https://）//history 对象包含浏览器的历史。Window History history.back() //与在浏览器点击后退按钮相同 history.forward() //与在浏览器中点击向前按钮相同//navigator 对象包含有关访问者浏览器的信息。alert(\"sometext\"); //警告框r=confirm(\"sometext\"); //确认框p=prompt(\"请输入你的名字\",\"Harry Potter\"); //（提示框） 输入框#document 对象当浏览器载入 HTML 文档, 它就会成为 document 对象。document 对象是HTML文档的根节点与所有其他节点（元素节点，文本节点，属性节点, 注释节点）。document 对象使我们可以从脚本中对 HTML 页面中的所有元素进行访问。 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt;&lt;body&gt;&lt;h1&gt;我的网页&lt;/h1&gt; &lt;p id=\"demo\"&gt;我的第一个段落。&lt;/p&gt; &lt;p id=\"demo2\"&gt;我的第2个段落。&lt;/p&gt;&lt;script&gt; # 向 id=\"demo\" 的 HTML 元素输出文本 \"你好 Dolly\" ： document.getElementById(\"demo\").innerHTML = \"你好 Dolly\";&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 调试123F12 打开浏览器调试器, 控制台查看日志.console.log('some thing') JavaScript 语句标识符 (关键字) ： 语句 描述 break 用于跳出循环。 catch 语句块，在 try 语句块执行出错时执行 catch 语句块。 continue 跳过循环中的一个迭代。 do … while 执行一个语句块，在条件语句为 true 时继续执行该语句块。 for 在条件语句为 true 时，可以将代码块执行指定的次数。 for … in 用于遍历数组或者对象的属性（对数组或者对象的属性进行循环操作）。 function 定义一个函数 if … else 用于基于不同的条件来执行不同的动作。 return 退出函数 switch 用于基于不同的条件来执行不同的动作。 throw 抛出（生成）错误 。 try 实现错误处理，与 catch 一同使用。 var 声明一个变量。 while 当条件语句为 true 时，执行语句块。","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"javascript","slug":"编程基础/javascript","permalink":"http://yoursite.com/categories/编程基础/javascript/"}],"tags":[{"name":"javascript语法","slug":"javascript语法","permalink":"http://yoursite.com/tags/javascript语法/"}]},{"title":"node","slug":"编程基础/javascript/node","date":"2019-07-12T00:55:09.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/javascript/node/","link":"","permalink":"http://yoursite.com/wiki/编程基础/javascript/node/","excerpt":"","text":"参考资料 npm最常用命令12# 安装模块, 并将模块名加入到package.jsonnpm install xxxxxx --save npm常用命令123456# 查看已安装模块的版本(-g查看全局,否则查看本地)npm ls jquery -gnpm view jquery version #查看jquery的最新的版本npm view jquery versions #查看npm服务器上所有的jquery版本信息npm info jquery #查看npm服务器上所有的jquery版本信息","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"javascript","slug":"编程基础/javascript","permalink":"http://yoursite.com/categories/编程基础/javascript/"}],"tags":[{"name":"node","slug":"node","permalink":"http://yoursite.com/tags/node/"},{"name":"npm","slug":"npm","permalink":"http://yoursite.com/tags/npm/"}]},{"title":"ubuntu的KDE开启触摸板","slug":"编程基础/软件使用备忘/ubuntu的KDE开启触摸板","date":"2019-07-11T14:37:20.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/ubuntu的KDE开启触摸板/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/ubuntu的KDE开启触摸板/","excerpt":"","text":"安装KDE后触摸板失效了, 今天终于解决了问题 xinput123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 查看设备xinput# TouchPad是触摸板, Mouse是鼠标, Touchscreen是触摸屏, 根据名字找到触摸板的设备ID(我这里是17)# 禁用xinput --disable 17# 启用xinput --enable 17# 查看设备的属性xinput --list-props 17# 结果如下: Device 'SynPS/2 Synaptics TouchPad': Device Enabled (143): 1 Coordinate Transformation Matrix (145): 1.000000, 0.000000, 0.000000, 0.000000, 1.000000, 0.000000, 0.000000, 0.000000, 1.000000 libinput Tapping Enabled (302): 0 libinput Tapping Enabled Default (303): 0 libinput Tapping Drag Enabled (304): 1 libinput Tapping Drag Enabled Default (305): 1 libinput Tapping Drag Lock Enabled (306): 0 libinput Tapping Drag Lock Enabled Default (307): 0 libinput Tapping Button Mapping Enabled (308): 1, 0 libinput Tapping Button Mapping Default (309): 1, 0 libinput Natural Scrolling Enabled (280): 0 libinput Natural Scrolling Enabled Default (281): 0 libinput Disable While Typing Enabled (310): 1 libinput Disable While Typing Enabled Default (311): 1 libinput Scroll Methods Available (282): 1, 1, 0 libinput Scroll Method Enabled (283): 1, 0, 0 libinput Scroll Method Enabled Default (284): 1, 0, 0 libinput Click Methods Available (312): 1, 1 libinput Click Method Enabled (313): 1, 0 libinput Click Method Enabled Default (314): 1, 0 libinput Middle Emulation Enabled (287): 0 libinput Middle Emulation Enabled Default (288): 0 libinput Accel Speed (289): 0.000000 libinput Accel Speed Default (290): 0.000000 libinput Left Handed Enabled (294): 0 libinput Left Handed Enabled Default (295): 0 libinput Send Events Modes Available (265): 1, 1 libinput Send Events Mode Enabled (266): 0, 0 libinput Send Events Mode Enabled Default (267): 0, 0 Device Node (268): \"/dev/input/event4\" Device Product ID (269): 2, 7 libinput Drag Lock Buttons (296): &lt;no items&gt; libinput Horizontal Scroll Enabled (297): 1其中 libinput Tapping Enabled (302) 这个属性是触摸板点击生效属性# 设置设备17的属性302的值为1xinput --set-prop 17 302 1 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"NAS","slug":"读书笔记/NAS","date":"2019-07-05T00:40:32.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/读书笔记/NAS/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/NAS/","excerpt":"","text":"软件系统对比 系统 类别 开源 优缺点 群晖 平台NAS 否 硬盘格式独特, 无法直接给其他系统使用 FreeNAS 平台NAS 开源 操作系统是FreeBSD Kodi 播放器 开源 plex 播放器 收费 OpenMediaVault 平台NAS nPlayer 播放器 30元 infuse 播放器 oplayer 播放器 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"elastic-debug","slug":"人工智能/搜索引擎/elastic-debug","date":"2019-06-27T03:20:54.000Z","updated":"2019-10-10T07:24:14.092Z","comments":true,"path":"wiki/人工智能/搜索引擎/elastic-debug/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/elastic-debug/","excerpt":"","text":"启动失败可能是映射数据文件夹的权限不是777, 因为es容器内启动的账号不是root, 所以需要把映射数据文件夹设置成777权限 搜索结果的 _score都是None因为使用了自定义的排序, 例如使用时间戳排序, 这时elastic会省略_socre. 如果你希望排序并查看分数, 可以使用 track_scores 参数. 参考 stackoverflow 无法写入数据: cluster_block_exception [FORBIDDEN/12/index read-only / allow delete (api)];因为ES默认监控硬盘使用空间超过95%时会自动设置为自读状态. 解决方案: 清理磁盘空间 配置阈值 123456789PUT _cluster/settings&#123; \"transient\": &#123; \"cluster.routing.allocation.disk.watermark.low\": \"100gb\", \"cluster.routing.allocation.disk.watermark.high\": \"50gb\", \"cluster.routing.allocation.disk.watermark.flood_stage\": \"10gb\", \"cluster.info.update.interval\": \"1m\" &#125;&#125; 123456789# 取消ES的只读状态import requestsimport jsonbody = &#123;\"index.blocks.read_only_allow_delete\": False&#125;url = 'http://192.168.31.192:50082/_all/_settings'resp = requests.put(url, json=body)print(json.loads(resp.text)) 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[]},{"title":"配置chrome","slug":"编程基础/安装与配置/配置chrome","date":"2019-06-27T01:51:37.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/安装与配置/配置chrome/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/配置chrome/","excerpt":"","text":"插件Ctrl-Z Reopen Closed Tab使用 快捷键 Ctrl+Z 恢复上次关闭的标签页 Reopen closed tab Button点击插件图标 恢复上次关闭的标签页 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"nvidia_docker","slug":"人工智能/深度学习/nvidia-docker","date":"2019-06-14T01:54:32.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/人工智能/深度学习/nvidia-docker/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/nvidia-docker/","excerpt":"","text":"GPU相关命令查看当前GPU使用情况 nvidia-smi 设置使用哪个GPU CUDA_VISIBLE_DEVICES=”1” 查看CUDA版本 nvcc –version 12# 查看显卡版本lspci | grep -i nvidia nvidia-docker简介Docker容器与平台无关, 但也与硬件无关.这有一个问题, 当使用专门的硬件,如NVIDIA显卡时, 需要内核模块和用户级别的库来操作. 因此docker本身并不支持容器内的NVIDIA显卡. NVIDIA提供了 nvidia-docker: 驱动无关的CUDA镜像 一个docker命令行包装, 在启动时将驱动程序的用户模式组件和GPU设备装入容器 安装nvidia-docker12 使用镜像镜像仓库https://hub.docker.com/r/nvidia/cuda 12345678910CUDA镜像的三种风格:base: 基础镜像runtime: 扩展于base, 增加了CUDA toolkit 共享库devel: 扩展于runtime, 增加了debugging工具/编译器工具链/头文件和静态库# 选择版本后, 拉取nvidia/cuda镜像nvidia-docker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04# 查看拉取镜像的版本nvidia-docker run --rm -ti 镜像名称 nvcc --version 安装pytorch获取安装文件https://pytorch.org/get-started/locally/ 1根据上面的版本, 选择1.1, linux, python3.7 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"},{"name":"显卡","slug":"显卡","permalink":"http://yoursite.com/tags/显卡/"}]},{"title":"tornado","slug":"python/tornado","date":"2019-06-12T01:46:09.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/tornado/","link":"","permalink":"http://yoursite.com/wiki/python/tornado/","excerpt":"","text":"监控文件改动, 自动重载服务12345# 设置 debug=True 或 autoreload=True 会监控所有.py文件, 在改动后自动重载tornado服务app = tornado.web.Application(handlers, debug=True)# 添加监控文件列表tornado.autoreload.watch(filename: str) 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"pytorch常用代码","slug":"人工智能/pytorch/pytorch常用代码","date":"2019-06-05T01:35:55.000Z","updated":"2019-10-08T11:43:15.171Z","comments":true,"path":"wiki/人工智能/pytorch/pytorch常用代码/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/pytorch常用代码/","excerpt":"","text":"pytorch版本: 1.1.0 张量创建一个张量12345678910111213141516171819202122232425262728293031# 通用注释: x表示其他张量 # size0,size1表示张量维度, 数量不固定# data 表示张量的数组值(python数组类型)zeros = torch.zeros( size0, size1, dtype=x.dtype, device=x.device, requires_grad=True)zeros = torch.zeros(data, dtype=torch.long) # 指定具体地数据类型# 随机化生成x = torch.rand(size0, size1) # 默认 torch.float32x = torch.rand(size0, size1, dtype=torch.float)x = torch.rand(size=[size0, size1], dtype=torch.float)t=torch.rand(t.size()) //均匀分布t=torch.randn(t.size()) //标准正态分布t=torch.normal(mean,std) //size同t.Tensor()，每个数以对应的均值mean和标准差std[i,j,...]正态采样。x = torch.randint(low=1, high=100, size=[12, 2], dtype=torch.long)x = torch.LongTensor(size0, size1) # 在pycharm中有错误提示#均分区间生成Tensor、t=T.arange(m,n,step_length) //[m,n)中m开始以步长step_length生成t=T.range(m,n,step_length) //[m,n-1]中m开始以步长step_length生成t=T.linspace(m,n,step_num) //[m,n]中以m为首项，n为末项，均分区间为step_num段# 可以与list或numpy中的array互相转化：t=T.Tensor(list)t=T.Tensor(np.array)t=T.from_numpy(np.array)list=T.tolist(t)array=t.numpy() 观察一个张量1234567t.size() //返回size类型t.numel() //返回总元素个数t.view(d1,d2,d3....)//维度重整t.unsqueeze(di) //在di个维度处升维、t.squeeze(di) //若di维是1，压缩，否则不变。若无参数，压缩所有“1”维torch.cat((t,t,...),dim=1) //按第di的维度按照tuple的格式复制ttorch.chunk(t,i,dim) //在di维上将t分成i份，最后一份的维度不定（若不能整除） 采数据 123torch.index_select(t, di, indices) //在第di维上将t的indices抽取出来组成新Tensor。torch.masked_select(t, mask) //按照0-1Tensor mask的格式筛选t，返回一维Tensortorch.nonzero(t) //输出n×2维Tensor，非零元素的index 计算 功能 代码 P范数（N方求和后开方） torch.norm(input, p=2) 指定GPU（3种方式） 123torch.cuda.set_device(id)os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;CUDA_VISIBLE_DEVICES=1 python main.py 网络层 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[]},{"title":"RNN","slug":"人工智能/深度学习/RNN","date":"2019-05-31T08:26:31.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/深度学习/RNN/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/RNN/","excerpt":"","text":"参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"}],"tags":[]},{"title":"debug思路备忘","slug":"编程思维/debug思路备忘","date":"2019-05-31T01:39:46.000Z","updated":"2019-10-17T08:10:25.389Z","comments":true,"path":"wiki/编程思维/debug思路备忘/","link":"","permalink":"http://yoursite.com/wiki/编程思维/debug思路备忘/","excerpt":"","text":"如何快速对接联调功能(最小可执行代码)想要快速的对接联调某个功能, 可以先把该功能独立出来, 新建一个该功能的最小可执行代码, 给对接方, 双方先使最小可执行代码联调成功, 再对接真正的项目代码 debug整体思路(如何升级)出现bug后, 一定要查询到bug原因 半懂不懂的解决bug是非常错误的行为. 即便误打误撞解决了bug, 下次这个bug大概率还会重现, 因为没有针对根本原因进行解决. 代码相同, 结果不同的可能性硬盘空间是否不足例如elastic search当硬盘空间占用大于95%时不能写入数据 网络情况不同例如某些前端框架使用了某些网络资源, 当不联网时网页显示会出错 代码调用的网络接口不同例如调用API的ip和uri等配置 调试方法: 实时查看被调用APi的日志, 手工触发调用, 查看调用过程的区别 调用数据库的数据不同例如调用了不同的数据库, 或者相同数据库里的数据不一样 读取文件不同例如配置文件, 数据文件, 字典等内容 环境变量不同例如oracle需要读取一些环境变量才能成功启动 电脑性能不同电脑运行速度导致一些依赖程序启动的慢, 或者文件复制没有结束, 都可能导致代码运行结果不一致 调试方法: 在一些差异点代码之前sleep几秒, 然后再执行 第三方库的版本不同例如tornado, pytorch版本等 语言版本不同例如Python, node版本等 操作系统不同例如Windows和linux的多进程机制不同, linux种类不同, linux小版本不同 例如mac上npm install后的node_modules文件夹里包含软连接, 当把node_modules复制到windows上时软连接会失效, 导致前端网页无法找到模块 前端: 浏览器不同例如浏览器不同, 浏览器版本不同 前端: 浏览器安装的插件不同例如一个浏览器可以用,另一个浏览器报错对象缺少方法, 可能是需要浏览器安装插件才能支持 Shell脚本命令不能加引号，如果是变量名表示执行命令则可以赋值时可以使用引号 网页可以浏览, 命令行不能联网可能是环境变量里面的PROXY设置有问题 123env |grpe PROXY # 查看代理的环境变量unset HTTP_PROXY # 删除对应的环境变量unset HTTPS_PROXY print无法输出内容(特别是docker容器里)设置环境变量: PYTHONUNBUFFERED=1 参考资料","categories":[{"name":"编程思维","slug":"编程思维","permalink":"http://yoursite.com/categories/编程思维/"}],"tags":[{"name":"debug","slug":"debug","permalink":"http://yoursite.com/tags/debug/"}]},{"title":"使用 htop","slug":"编程基础/软件使用备忘/use-htop","date":"2019-05-29T06:56:59.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-htop/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-htop/","excerpt":"","text":"隐藏重复的进程操作: F2(setup)—&gt;Display options—&gt;使用空格勾选”Hide userland process threads” 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"使用Adblock广告屏蔽插件","slug":"编程基础/软件使用备忘/use-Adblock","date":"2019-05-27T13:49:42.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-Adblock/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-Adblock/","excerpt":"","text":"## 插件图标： 屏蔽规则12345678# 隐藏元素：host--网站域名 class_name--类名 id_name--id名host##.class_name host###id_name # 属性选择符##div[title*=\"adv\"] 隐藏 title 属性包含 adv 字符的 div 元素##table[width=\"80%\"] 隐藏 width 属性值为 80% 的表格元素##div[title^=\"adv\"][title$=\"ert\"] 隐藏 titile 属性以 adv 开始并且以 ert 结束的 div 元素 我的自定义配置文件：123sohu.com###right-side-barsohu.com###float-btnsohu.com##.groom-read 参考资料 https://adblockplus.org/filters","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"python源码转UML图","slug":"python/python源码转UML图","date":"2019-05-25T11:08:46.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python源码转UML图/","link":"","permalink":"http://yoursite.com/wiki/python/python源码转UML图/","excerpt":"","text":"用conda制作模块安装12345# 需要安装的软件：## Graphviz：贝尔实验室开源的图形绘制工具包## Pyreverse：用来分析Python代码和类关系的工具，包含在Pylint中sudo apt install graphvizpip install pylint 调用12345678910pyreverse -o png -ASmy xxx.py# xxx.py 要解析的源代码文件# -o png 指定输出图片格式。 默认的dot格式。# -o tail.png 也可以携带tail 输出文件名= classes.tail.png# -A, --all-ancestors 展示项目中的所有祖先类# -a Ｎ 展示项目中的Ｎ代祖先类# -S, --all-associated 以递归方式显示所有关联的关联类# -m, --module-names=[yn] 在表示类中包含模块名称# 用法 -m y 或者 -my# -k, --only-classnames 类框中不显示属性和方法;这会禁用-f值","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"debug：pytorch_梯度出现NaN","slug":"人工智能/pytorch/debug-pytorch-梯度出现NaN","date":"2019-05-22T11:00:45.000Z","updated":"2019-10-13T02:17:50.569Z","comments":true,"path":"wiki/人工智能/pytorch/debug-pytorch-梯度出现NaN/","link":"","permalink":"http://yoursite.com/wiki/人工智能/pytorch/debug-pytorch-梯度出现NaN/","excerpt":"","text":"计算 梯度出现NaN 梯度出现异常值：NaN定位方法：使用如下代码设置，在出现NaN异常时程序会报错，便于定位错误代码 1234567import torch# 正向传播时：开启自动求导的异常侦测torch.autograd.set_detect_anomaly(True)# 反向传播时：在求导时开启侦测with torch.autograd.detect_anomaly(): loss.backward() 原因：很多网友提到，pytorch的求标准差的函数STD可能有问题。如果使用了类似会调用STD函数的各种Norm层就可能导致NAN问题。 官方文档参考","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"pytorch","slug":"人工智能/pytorch","permalink":"http://yoursite.com/categories/人工智能/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"NaN","slug":"NaN","permalink":"http://yoursite.com/tags/NaN/"}]},{"title":"git基本概念","slug":"编程基础/git/git基本概念","date":"2019-05-17T11:05:30.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/编程基础/git/git基本概念/","link":"","permalink":"http://yoursite.com/wiki/编程基础/git/git基本概念/","excerpt":"","text":"概念 remote 远程仓库 repository 本地仓库 index/stage 暂存区 workspace 工作区 查看日志12# 如果恢复老版本, 想查看之前的新版本ID可以使用refloggit reflog 恢复文件到暂存区的版本123git checkout -- $filename# 如果未add到暂存区的内容会被丢弃# 这个命令也可以恢复在工作区删除的文件(删除操作未add)","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"git","slug":"编程基础/git","permalink":"http://yoursite.com/categories/编程基础/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"http://yoursite.com/tags/gitlab/"}]},{"title":"使用虚拟机","slug":"编程基础/软件使用备忘/使用虚拟机","date":"2019-05-09T02:55:35.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程基础/软件使用备忘/使用虚拟机/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/使用虚拟机/","excerpt":"","text":"安装虚拟机VirtualBox1234567891011# 首先添加VirtualBox的源sudo sh -c 'echo \"deb http://download.virtualbox.org/virtualbox/debian xenial contrib\" &gt;&gt; /etc/apt/sources.list.d/virtualbox.list'# 添加秘钥wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -# 执行更新sudo apt update# 安装virtualboxsudo apt install virtualbox 卸载1234567sudo apt remove virtualbox*# TODO: 未卸载完全, 需要删除文件/home/vbox/home/fish3/VirtualBoxVMs/usr/bin/VirtualBox -&gt; VBox*/usr/bin/VBox 使用方法简介 设置一个虚拟机 载入一个系统iso，安装系统 把系统导出，方便以后导入使用 ubtuntu虚拟机–设置共享文件夹宿主机上操作 123456789# 下载增强插件（注意插件版本要和虚拟机程序版本一致）https://www.virtualbox.org/wiki/DownloadsVirtualBox 6.0.6 Oracle VM VirtualBox Extension Pack All supported platforms # 老版本下载：Extension Pack https://www.virtualbox.org/wiki/Download_Old_Builds_5_2 # 安装宿主机上的插件 管理--全局设定--扩展--加号---选择下载插件-安装 选中虚拟机项目，右键—设置—共享文件夹—点击最右侧的加号按钮 在共享文件夹路径里设置宿主机里的某个路径，再设置一个共享文件夹名称 虚拟机内操作 安装增强插件 创建虚拟机内路径，然后挂载到该路径 1234567891011# ubuntu16.04sudo apt-get install virtualbox-guest-additions-isoapt-get install virtualbox-guest-utils# 参考# 创建路径mkdir /home/user/share# 挂载# 其中“share”就是共享文件夹的名称sudo mount -t vboxsf share /home/$user/share 参考网页 设置虚拟机的网络和宿主机网络的映射点击虚拟机的设置, 选择”网络”标签 默认是NAT模式, 选择’高级’, 端口转发 在虚拟机内部, 输入命令ip -4 addr 查看到虚拟机的IP 在端口转发规则中主机IP是127.0.0.1, 主机端口选择1万以上(太小的端口容易被禁止) 输入子系统的IP, 端口=22 就可以在宿主机上用ssh服务链接到虚拟机了 debug: 升级ubuntu的kernel后无法使用卸载老版本, 安装新版本(6.0以上支持kernel5.0) debug:系统界面卡关闭虚拟机, 点击该虚拟机的设置, 切换到”显示”标签 然后把显存设置成最大, 并勾选3D加速和2D加速 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"use_tmux","slug":"编程基础/软件使用备忘/use-tmux","date":"2019-05-01T08:19:49.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-tmux/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-tmux/","excerpt":"","text":"安装1sudo apt install tmux 概念 会话（session）：建立一个tmux工作区会话 窗口（window）：容纳多个窗格 窗格（pane）： 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"use-vscode","slug":"编程基础/软件使用备忘/use-vscode","date":"2019-04-27T09:49:58.000Z","updated":"2019-10-08T09:22:51.362Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-vscode/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-vscode/","excerpt":"","text":"使用正则表达式替换搜索文本：static/(.*).css 替换文本中用 $1来表示括号里匹配的内容 安装插件Setting Sync 在不同电脑同步你的配置和插件。上传配置： 安装插件 用默认浏览器登录github网站 shift+alt+U 上传配置：vscode会打开github网站的tokens页面 在网页选择 Generate new token 输入token key 勾选gist 点击Generate token 创建token 复制网页返回的token文本 在vscode的命令可输入token文本，回车 上传成功 下载备份： 在另一个电脑的vscode安装插件 shift+alt+D 下载配置：vscode会打开github网站的tokens页面 先输入token key 再输入token id 回车即可 如果在下载备份过程中出错，需要执行 Sync:Reset Extension Settings 重置选项才可以再次执行 shift+alt+d SFTP查看远程代码 ctrl+shift+P 输入 SFTP: config 如果提示要打开文件夹，则在某个位置打开一个存储SFTP配置的文件夹 然后进行配置json，示范如下： 12345678910&#123; \"name\": \"远程项目名称\", \"host\": \"192.168.1.10\", \"protocol\": \"sftp\", \"syncMode\": \"update\", \"username\": \"demo\", \"remotePath\": \"/home/demo/demo1\", \"privateKeyPath\": \"/home/user/.ssh/id_rsa\", \"uploadOnSave\": true&#125; Path Intellisense自动提示文件路径，支持各种快速引入文件。 Paste Json As Code把粘贴内容格式化。打开命令板，输入Paste Json As Code的命令即可 Gitlens查看代码的最后一行git日志。点击右上角图标即可对比展示你的代码与原repo的区别 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"尼康D5600相机","slug":"读书笔记/nikon-d5600","date":"2019-04-23T03:49:54.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/读书笔记/nikon-d5600/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/nikon-d5600/","excerpt":"","text":"操作设置查看照片信息菜单–播放菜单–播放显示选项 对焦点 无（仅图像） 查看照片播放按钮，左右是切换，上下是查看信息 闪光灯按钮： 镜头前左侧，最上方闪电图标的按钮 按下闪光按钮，波动拨盘，可以调整闪光幅度 拍摄P档：自动曝光拨盘：修改曝光参数组合（曝光时间、光圈） A档：光圈优先拨盘：修改光圈 F3.5大光圈（拍人物，背景会虚化） F22小光圈（大场景的风景照） S档：快门优先拨盘：修改快门时间 当改变快门到光圈的极限，光圈值会闪烁。这时候可以修改ISO （ISO增大，相同快门时间匹配更小的光圈） M档：手动曝光光圈、快门分别手工控制，不会关联变动。 曝光补偿使照片更亮，或更暗。 在AS档，曝光补偿会联动改变光圈或快门。 测光模式测光模式影响整个效果 ×××××××××××× i按钮info按钮BKT按钮 包围曝光的张数3F： 曝光补偿 0 1 2档 分别拍摄1张 -3F： 曝光补偿 0 -1 -2档 分别拍摄1张 QUAR–放大镜 按钮QUAR设置照片大小和格式 色温值 白平衡 固定参数：拍人像，用光圈优先模式和手动模式 室内灯光（曝光补偿会白）快门=1/50 光圈=F4.5 ISO=3200 曝光补偿=+0.7 白平衡4日光灯 快门=1/30 光圈=F4.5 ISO=3200 曝光补偿=+1.3 白平衡4日光灯 曝光锁定|对焦锁定 AE-L AF-L AF 自动对焦 d5600没有？ 模式AF-C AF-A AF-S 对焦区域 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"安装python","slug":"python/安装python","date":"2019-04-17T00:55:46.000Z","updated":"2019-10-14T02:25:56.105Z","comments":true,"path":"wiki/python/安装python/","link":"","permalink":"http://yoursite.com/wiki/python/安装python/","excerpt":"","text":"在ubuntu中安装python3.61234# 安装python3.6sudo add-apt-repository ppa:jonathonf/python-3.6sudo apt-get updatesudo apt-get install python3.6 123456789# 安装python3.6 并替换3.5sudo apt update \\ &amp;&amp; apt install -y software-properties-common \\ &amp;&amp; add-apt-repository -y ppa:jonathonf/python-3.6 \\ &amp;&amp; apt update \\ &amp;&amp; apt install -y python3.6 \\ &amp;&amp; update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 100 \\ &amp;&amp; update-alternatives --install /usr/bin/python python /usr/bin/python3.6 100 \\ &amp;&amp; apt install -y python3-pip \\ update-alternatives是Ubuntu中管理软件版本的工具 如果不设置名称, 则默认的python3实际是3.5 这时候安装python3-pip时会针对python3.5安装 update-alternatives用法说明: 12345678910# 查看python3这个名称对应了什么软件update-alternatives --display python3 # 把软件路径设置到某个链接和某个名称update-alternatives --install &lt;链接&gt; &lt;名称&gt; &lt;路径&gt; &lt;优先级&gt;链接是类似/usr/bin/python3 这样的软链接路径名称是类似python3 这样的软件名称路径是指软件的实际位置优先级数字越大越优先 用conda制作模块有时安装不上某个模块，可以尝试用conda安装。conda安装不上时，可以尝试去anaconda.org查询是否有这个模块是否在某个源 12conda install -c 某个源 模块名conda install -c conda-forge jsonnet conda创建虚拟环境conda创建虚拟环境，jupyter可以使用。 这个过程jupyter貌似不用重启，刷新页面即可。 12345678910111213141516171819# 1、创建环境conda create -n py36 python=3.6# 2、进入环境source activate py36# 3、然后添加kernel# 错误命令: 如果当前虚拟环境不是root用户的，而仅用有user权限。则sudo安装的kernel位置不会是当前虚拟环境的位置。sudo python -m ipykernel install --name py36 # 正确命令: 仅安装到当前用户，这个位置正确。如果提示没有权限，则给要写入的文件夹权限。python -m ipykernel install --user --name py36 # 如果提示没有ipykernel，使用:python -m pip install ipykernel# 4、退出虚拟环境source deactivate# 5、删除虚拟环境conda remove -n 环境名称 --all 删除kernel先找到配置文件kernel.json sudo find / -name “kernel.json” 判断倒数第一行是我的配置，所以删除/usr/local/share/jupyter/kernels/py36/的话即是删除了这个kernel。 /usr/local/share/jupyter/kernels/py36/kernel.json /home/fish/.local/share/jupyter/kernels/py27 用virtualenv安装虚拟环境1234567891011121314151617# 安装virtualenvpip install virtualenv# 列出所有环境lsvirtualenv# 创建环境virtualenv venvname# 创建环境，以 /usr/bin/python3 为python程序virtualenv -p /usr/bin/python3 venvname# 激活环境activate venvname# 退出环境deactivate# 删除环境rmvirtualenv venvname","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"python常见错误","slug":"python/python常见错误","date":"2019-04-15T00:46:54.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python常见错误/","link":"","permalink":"http://yoursite.com/wiki/python/python常见错误/","excerpt":"","text":"用系统内置函数作为变量名比如下面的一些语句，会使你失去系统内置的功能： 123# 把内置类型或函数作为变量复制，导致原有功能失效set = &#123;1, 2, 3&#125;print = 'Hello' 将.py文件命名为内置模块的名称常常被覆盖的模块名有：abc、match、turtle等。比如，为了计算一个公式的值，把源代码文件命名为math.py，内容如下： 123456789import mathprint(math.sqrt(2)*2)# 报错Traceback (most recent call last):File \"/Users/chenbin/Documents/homework/math.py\", line 1, in &lt;module&gt;import mathFile \"/Users/chenbin/Documents/homework/math.py\", line 2, in &lt;module&gt;print(math.sqrt(2)*2)AttributeError: module 'math' has no attribute 'sqrt' 以为input函数是万能的初学者经常会以为input函数可以随心所欲得到想要的那种类型数值，特别是整数，比如： 12n = input(\"请输入年龄：\")print(\"明年你就\", n + 1, \"岁了！\") 结果出错，因为python3中的input函数返回的是字符串，必须要套一层int()才能得到整数。 可变类型的连续赋初值初学者觉得a=b=c=1这样赋初值特别cool，然后： 123456# 初始化了3个空列表，但这其实是幻觉# 因为它们仨指向了同一个可变类型的列表容器对象a=b=c=[] # 给a添加了一个元素a.append(123) # 其实a,b,c指向的是同一个列表，结果a,b,c都是[123] 记住，只有不可变类型的对象可以这么赋初值，或者你确实需要几个指向同一个可变对象的变量（这几乎不会出现）。 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"npm","slug":"编程基础/软件使用备忘/npm","date":"2019-04-12T08:38:37.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/npm/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/npm/","excerpt":"","text":"npm run dev 热更新代码123./config/index.jsmodule.exports = &#123;poll: true&#125; # 确定改成true即可热更新 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"python常用代码","slug":"python/python常用代码","date":"2019-04-10T14:46:50.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python常用代码/","link":"","permalink":"http://yoursite.com/wiki/python/python常用代码/","excerpt":"","text":"自带的静态服务器1python -m http.server 8081 计数器对象 class collections.Counter12c = Counter() # 创建c.most_common(3) # 返回频次前3的二维数组，降序排列。不加参数则返回全部。 [(word, count)] 向上取整1(分子 + 分母 - 1) // 分母 tqdm进度条12345from tqdm import tqdmgen_tqdm = tqdm(gener, total=len(gener))for i in gen_tqdm: # 实时修改进度条上的描述文本 gen_tqdm.set_description(description, refresh=False) 交换变量快捷写法：最多支持4个变量互换123a, b, c = 1,2,3a,b,c = c,a,b# 结果： a=3 b=1 c=2 如果交换内容涉及：对象及其属性，需要考虑先后顺序123456789101112131415161718class ListNode: def __init__(self, x): self.val = x self.next = None # 正确的交换 node = ListNode(1) ; node.next = 2node.next, node = node.val, node.next# 结果： node=2 # 错误的交换 node = ListNode(1) ; node.next = 2node, node.next = node.next, node.val# 结果报错信息：# AttributeError: 'int' object has no attribute 'next'# node = node.next (2)# node.next = node.val int没有next属性 原因 交换的不是变量，而是变量的地址。地址变化是有顺序的，并不是同时完成的。 如果没有涉及到对象及其属性，地址变化不会影响取值过程，所以不会报错。所以看起来像是一句代码同时完成了一样。 python解释得出的执行码中有4个指令：ROT_TWO /ROT_THREE/ROT_FOUR 所以交换赋值语句最多支持4个变量 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"jupyter_notebook","slug":"python/jupyter-notebook","date":"2019-04-07T09:58:44.000Z","updated":"2019-10-08T09:22:51.338Z","comments":true,"path":"wiki/python/jupyter-notebook/","link":"","permalink":"http://yoursite.com/wiki/python/jupyter-notebook/","excerpt":"","text":"配置远程密码12345678910111213141516171819202122232425# 生产配置文件jupyter notebook --generate-config# 生成密码# 1. 打开ipython 执行下面2行代码from notebook.auth import passwdpasswd()# 输入2两次密码，生成sha值# 编辑配置文件 ~/.jupyter/jupyter_notebook_config.pyc.NotebookApp.ip='127.0.0.1'c.NotebookApp.allow_remote_access=Truec.NotebookApp.password=u'sha:xxxxxx生成内容'c.NotebookApp.open_browser=Falsec.NotebookApp.port=8888# 配置代码根目录c.ContentsManager.root_dir = '/home/aifish/FishCode'# 启动服务jupyter notebook# 设置开机启动# 加入 rc.localnohup /home/aifish/anaconda3/bin/jupyter notebook&gt;/home/aifish/.jupyter/notebook.log 2&gt;&amp;1 &amp; 把虚拟环境添加到jupyter的kernel1234567进入虚拟环境# 安装 ipykernelpip install ipykernel# 找到python位置（因为加入kernel时需要sudo权限，要制定python路径which python# 使用python绝对路径, 把虚拟环境XXXX加入kernelsudo /anaconda/env/python -m ipykernel install --name XXXX 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"jupyter","slug":"jupyter","permalink":"http://yoursite.com/tags/jupyter/"},{"name":"notebook","slug":"notebook","permalink":"http://yoursite.com/tags/notebook/"}]},{"title":"linux刻录光盘","slug":"编程基础/软件使用备忘/linux_write_DVD","date":"2019-04-02T07:26:08.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/linux_write_DVD/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/linux_write_DVD/","excerpt":"","text":"ubuntu–Brasero 在Ubuntu软件商店安装brasero 光驱插入光盘 打开brasero 选择：数据项目(A)：创建一个视频DVD或SVCD 点击添加按钮，加入文件夹或文件 给光盘命名（默认是日期文本） 显示进度条，等待刻录完成 注意：带很多代码和数据文件的文件夹，要先压缩再刻盘。 不然有可能刻盘失败。 光盘也不能用了。。。。大小：4000M比较靠谱（存储空间比真实文件更大）sudo tar cjf - police_model |split -b 4000m - police.v9. 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"allennlp 类","slug":"人工智能/深度学习/allennlp/allennlp-classes","date":"2019-03-31T12:14:52.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/深度学习/allennlp/allennlp-classes/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/allennlp/allennlp-classes/","excerpt":"","text":"allennlp/data/vocabulary.py allennlp/data/tokenizers/token.py allennlp/data/dataset_readers/dataset_reader.py allennlp/models/simple_tagger.py allennlp.data.token_indexers.TokenIndexerallennlp.data.token_indexers.single_id_token_indexerallennlp.common.util.get_frozen_and_tunable_parameter_namesallennlp.common.params.Paramsallennlp.training.trainer.Trainerallennlp.models.archival.archive_modelallennlp.data.iterators.data_iteratorallennlp.models.modelallennlp.data.vocabularyallennlp.data.dataset_reader 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"},{"name":"allennlp","slug":"人工智能/深度学习/allennlp","permalink":"http://yoursite.com/categories/人工智能/深度学习/allennlp/"}],"tags":[]},{"title":"allennlp 命令执行流程","slug":"人工智能/深度学习/allennlp/allennlp-command","date":"2019-03-31T11:09:04.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/人工智能/深度学习/allennlp/allennlp-command/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/allennlp/allennlp-command/","excerpt":"","text":"Successfully installed Jinja2-2.10.1 MarkupSafe-1.1.1 PyYAML-5.1 Pygments-2.4.2 Werkzeug-0.15.4 alabaster-0.7.12 allennlp-0.8.4 atomicwrites-1.3.0 attrs-19.1.0 awscli-1.16.190 babel-2.7.0 blis-0.2.4 boto3-1.9.180 botocore-1.12.180 click-7.0 colorama-0.3.9 conllu-0.11 cycler-0.10.0 cymem-2.0.2 docutils-0.14 editdistance-0.5.3 flaky-3.6.0 flask-1.0.3 flask-cors-3.0.8 ftfy-5.5.1 gevent-1.4.0 greenlet-0.4.15 h5py-2.9.0 imagesize-1.1.0 importlib-metadata-0.18 itsdangerous-1.1.0 jmespath-0.9.4 joblib-0.13.2 jsonnet-0.13.0 jsonpickle-1.2 jsonschema-3.0.1 kiwisolver-1.1.0 matplotlib-3.1.0 more-itertools-7.1.0 murmurhash-1.0.2 nltk-3.4.3 numpy-1.16.4 numpydoc-0.9.1 overrides-1.9 packaging-19.0 parsimonious-0.8.1 plac-0.9.6 pluggy-0.12.0 preshed-2.0.1 protobuf-3.8.0 py-1.8.0 pyasn1-0.4.5 pyparsing-2.4.0 pyrsistent-0.15.2 pytest-5.0.0 python-dateutil-2.8.0 pytorch-pretrained-bert-0.6.2 pytz-2019.1 regex-2019.6.8 responses-0.10.6 rsa-3.4.2 s3transfer-0.2.1 scikit-learn-0.21.2 scipy-1.3.0 snowballstemmer-1.9.0 spacy-2.1.4 sphinx-2.1.2 sphinxcontrib-applehelp-1.0.1 sphinxcontrib-devhelp-1.0.1 sphinxcontrib-htmlhelp-1.0.2 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.2 sphinxcontrib-serializinghtml-1.1.3 sqlparse-0.3.0 srsly-0.0.7 tensorboardX-1.7 thinc-7.0.4 torch-1.1.0 tqdm-4.32.2 unidecode-1.1.1 wasabi-0.2.2 wcwidth-0.1.7 word2number-1.1 zipp-0.5.1 安装tensorboard12345# 先要安装tensorboard和tensorflowpip install tensorflow tensorboard# 再安装tensorboardxpip install tensorboardX Allennlp代码整体逻辑 自定义对象 datareader(数据读取器) 必须有方法.read dataset = datareader.read(数据文件) 默认dataset.instances 是数据集的每个实例，Instance 是由一个命名的field集合组成的 instance.fields 是一个实例的所有字段 fields[‘title’].tokens 是文本标记序列 tokens[0].text 是一个词汇文本 fields[‘label’].label 是实例的正确分类标签文本key=‘label’ 能让数据变得更加通用（而不写具体数据种类） Field字段类(字段数据) 这些字段的数据是普通字符串，但是进入模型前会转化成ID TextField文本字段类 LabelerField 范畴标签字段类 Vocabulary类(词典) 有多个命名空间 默认命名空间 token 输入的词汇标记 默认命名空间 label 输出的标签 Predictor类(预测器) 包装一个模型，进行预测：json输入，json输出（而非张量） 需要重写predicotr.predict_json()函数，从输入json转成instance（带张量） 预测命令：predict```12345678910111213141516171819202122232425262728293031323334 1. 需要一个归档文件 ( 即一个训练好的模型 ) 2. 需要一个输入文件 ( 每行有一个 JSON 输入 ) 6. 网页演示 1. 需要一个训练好的模型 2. 需要写好预测器2. 自定义对象 model 1. 输入输出字典的value是张量3. 配置一个json文件：写入模型和训练等参数## 命令执行流程```python# 程序入口： allennlp.commands.main()# subparser 子命令:&quot;configure&quot;: allennlp.commands.configure.Configure()&quot;train&quot;: allennlp.commands.train.Train()&quot;evaluate&quot;: allennlp.commands.evaluate.Evaluate()&quot;predict&quot;: allennlp.commands.predict.Predict()&quot;make-vocab&quot;: allennlp.commands.make_vocab.MakeVocab()&quot;elmo&quot;: allennlp.commands.elmo.Elmo()&quot;fine-tune&quot;: allennlp.commands.fine_tune.FineTune()&quot;dry-run&quot;: allennlp.commands.dry_run.DryRun()&quot;test-install&quot;: allennlp.commands.test_install.TestInstall()&quot;find-lr&quot;: allennlp.commands.find_learning_rate.FindLearningRate() 训练流程调用的类和方法123456789&gt;&gt;&gt; allennlp train XXXallennlp/commands/__init__.py main()allennlp/commands/train.py Train()allennlp/commands/train.py Train.train_model_from_args()allennlp/commands/train.py Train.train_model_from_file()allennlp/commands/train.py Train.train_model()allennlp/training/trainer.py Trainer()allennlp/training/trainer.py Trainer.train()allennlp/training/trainer.py Trainer._train_epoch() 12345678# 训练# -r就会使用之前已经创建好的词典allennlp train XXX -s XXX# 启动配置助手(网页)allennlp configure --port 8123 词汇对照表 英文 中文 tag 序列标记（NER输出等） label 类别标签（分类任务输出等） index 索引 padding 填充 Trainer初始化参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Trainer初始化参数----------model：``Model``，必需。要优化的AllenNLP模型，或者是Pytorch模块（它的forward方法必须返回字典，并包含key=“loss”，value=scalar tensor）optimizer：``torch.nn.Optimizer``，必需。 Pytorch Optimizer的一个实例，使用要优化的模型的参数进行实例化。 iterator：``DataIterator``，必需。 迭代“Dataset”的方法，产生填充并转成索引的批次数据。 train_dataset：``Dataset``，必需。 要训练的“Dataset”。数据集应该已经转成索引。 validation_dataset：``Dataset``，可选，（默认=None）。 要评估的“Dataset”。数据集应该已经转成索引。 patience：可选[int]&gt; 0，可选（默认=None） 在early stopping之前要观察的epoch数量：在“patience”个epoch都没有改善则提早停止。如果给出，它必须是“&gt; 0”。 如果为None，则禁用early stopping。 validation_metric：str，optional（default =“loss”） 用来衡量是否使用提早停止以及是否在每个epoch存储is_best模型。度量标准名称必须以“+”或“ - ”为前缀，指定度量标准是增加还是减少。 validation_iterator：``DataIterator``，可选（默认=None） 用于验证集的迭代器。如果是None，那么使用训练集的iterator。 shuffle：``bool``，可选（默认= True） 是否在迭代器中对实例进行随机洗牌。 num_epochs：int，optional（默认值= 20） 培训epoch数量。 serialization_dir：str，optional（默认=None） 用于保存和加载模型文件的目录路径。如果未传递此参数，则不会保存模型。 num_serialized_models_to_keep：``int``，可选（默认= 20） 要保留的先前模型检查点的数量。默认是保留20个检查点。 值为None或-1表示将保留所有检查点。 keep_serialized_model_every_num_seconds：``int``，可选（默认=None） 如果num_serialized_models_to_keep不是None，那么除了最后一个num_serialized_models_to_keep之外，偶尔以给定间隔保存模型也很有用。 为此，请将keep_serialized_model_every_num_seconds指定为永久保存的检查点之间的秒数。请注意，此选项仅在以下情况下使用 num_serialized_models_to_keep不是None，否则保留所有检查点。 model_save_interval：``float``，可选（默认=None） 如果提供，则在单个epoch中每隔N秒存储一次模型。如果提供serialization_dir在每个epoch结束时也会保存模型。 cuda_device：``int``，可选（默认= -1） 一个整数，指定要使用的CUDA设备。如果为-1，则使用CPU。 grad_norm：``float``，可选，（默认=None）。 如果提供，则会按最大值调整梯度。 grad_clipping：``float``，可选（默认=``无``）。 如果提供，渐变将在“向后传递”期间被剪切以具有该值的（绝对）最大值。如果你在训练期间在渐变中得到“NaNs” 使用``grad_norm``无法解决，你可能需要这个。 learning_rate_scheduler：``PytorchLRScheduler``，可选，（默认=None） Pytorch学习速率调度程序。在每个epoch结束时，学习率将相对于该时间表衰减。如果你使用`torch.optim.lr_scheduler.ReduceLROnPlateau类`，这将使用提供的`validation_metric`来确定学习是否已达到稳定状态。支持每个batch更新学习率，这可以选择实现step_batch（batch_num_total），它更新给定batch的学习率。 summary_interval：``int``，可选，（默认= 100） 记录标量到tensorboard之间的批次数 histogram_interval：``int``，optional，（default =``None``） 如果不是None，则每N个batch的柱状图记录到tensorboard。 指定此参数后，将启用以下附加日志记录： *模型参数的柱状图 *参数更新率 *层激活的柱状图 我们记录model.get_parameters_for_histogram_tensorboard_logging 返回的参数的柱状图。 对于``Model``中具有属性``should_log_activations``设置为``True``的任何模块，都会记录图层激活。记录柱状图在训练期间需要许多GPU-CPU副本，并且通常很慢，因此我们建议相对不频繁地记录柱状图。 注意：只有返回张量的模块，张量的元组或dict支持activations日志。 should_log_parameter_statistics：``bool``，可选，（默认= True）是否发送参数统计（平均值和标准差）参数和梯度）到张量板。should_log_learning_rate：``bool``，可选，（默认= False）是否将参数特定学习率发送到tensorboard。log_batch_size_period：``int``，optional，（default =``None``）如果已定义，则记录平均批量大小的频率。“” Vocabulary12345678910111213141516171819202122232425262728293031词汇表将字符串映射到整数，允许将字符串映射到OOV标记（out-of-vocabulary OOV）。词汇表适合特定的数据集，我们用它来决定哪些tokens是词汇表内的。词汇表还允许使用多个不同的命名空间，因此你可以有不同的index对应单词“a”和字符“a”。例如我们可以使用此对象将tags和labels文本映射到index，用一个统一的类：xxx_Field。此类中的大多数方法都允许你传入命名空间; 默认情况下，我们使用'tokens'命名空间，你可以在任何地方省略命名空间参数，只使用默认值。参数----------counter：`Dict [str，Dict [str，int]]`，可选（默认=`None`） 用于初始化此词汇表的计数集合。我们将检查计数，并与该类的其他参数一起使用它们来决定哪些单词是词汇表。如果这是None，我们就不会用任何东西初始化词汇表。min_count：`Dict [str，int]`，可选（默认=None） 从计数器初始化词汇表时，你可以指定最小计数，并且计数小于此值的每个标记都不会添加到词典中。这些最小计数是“特定于命名空间的”，因此你可以为标签与单词指定不同的最小值。如果命名空间在给定的字典中没有key，我们将所有看到的标记添加到该命名空间。max_vocab_size：`Union [int，Dict [str，int]]`，可选（默认=None） 如果要限制词汇表中的令牌数量，可以使用此参数执行此操作。如果指定单个整数，则每个命名空间的词汇表都将固定为不大于此值。如果指定一个字典，那么`counter`中的每个命名空间都可以有一个单独的最大词汇量。任何缺失的键都将具有值“None”，这意味着词汇量大小没有上限。non_padded_namespaces：`Iterable [str]`，可选 默认情况下，我们假设你将单词/字符标记映射为整数，因此你希望为padding和OOV保留单词索引。但是，如果要将NER或SRL的tag或类别标签映射到整数，则可能不希望保留padding和OOV索引。使用此字段指定哪些名称空间不应添加padding和OOV标记。 这个元素的格式是一个字符串，它必须与字段名称完全匹配，或者`*`后跟一个字符串，我们将它们作为字段名称的后缀。 我们尝试使默认值合理，这样你就不必考虑这一点。 默认为（tags，labels），因此只要您的命名空间以“tags”或“labels”结尾（默认情况下，此代码中的所有tag和labels字段均为true）， 不必在这里指定任何东西。pretrained_files：`Dict [str，str]`，可选 如果提供，此映射指定每个命名空间的可选预训练嵌入文件的路径。这可以用于将词汇表限制为仅出现在此文件中的单词，或者确保此文件中的任何单词都包含在词汇表中，而不管其计数如何，具体取决于“only_include_pretrained_words”的值。 出现在预训练嵌入文件中但未出现在数据中的单词不包含在词汇表中。min_pretrained_embeddings：`Dict [str，int]`，可选 如果提供，则为每个命名空间指定与预训练嵌入文件保持一致的最小行数（通常是最常用的单词），即使对于未出现在数据中的单词也是如此。only_include_pretrained_words：`bool`，可选（默认= False） 这定义了使用可能在`pretrained_files`中指定的任何预训练嵌入文件的策略。如果为False，则使用包含策略：并且将“计数器”和预训练文件中的单词添加到“词汇表”中，而不管它们的计数是否超过“min_count”。如果为True，我们使用独占策略：如果单词位于预训练嵌入文件中，则单词仅包含在词汇表中（它们的计数必须至少为'min_count`）。tokens_to_add：`Dict [str，List [str]]`，可选（默认=None） 如果给定，这是一个要添加到词汇表的标记列表，由命名空间键入以添加标记。这是一种确保某些项目出现在词汇表中的方法，无论其他任何词汇计算如何。 tensorboard12345678910111213141516171819class TensorboardWriter(get_batch_num_total: Callable[int], serialization_dir: Optional[str] = None, summary_interval: int = 100, histogram_interval: int = None, should_log_parameter_statistics: bool = True, should_log_learning_rate: bool = False)get_batch_num_total：Callable [[]，int]到目前为止返回批次数的thunk。 很可能这将是一个围绕Trainer类中的实例变量的闭包。serialization_dir：str，optional（默认=None）如果提供，则这是Tensorboard日志的写入位置。summary_interval：int，optional（默认值= 100）大多数统计数据只会在这么多批次中写出来。histogram_interval：int，optional（default = None）如果提供，则每隔这些批次就会记录激活柱状图。 如果为None，则不会写。should_log_parameter_statistics：bool，optional（default = True）是否记录参数统计信息。should_log_learning_rate：bool，optional（默认= False）是否记录学习率。 indexer索引器12345678910111213141516171819202122232425262728TokenIndexer“TokenIndexer”确定字符串标记如何表示为模型中的索引数组。这个类在a的帮助下将字符串转换为数值：class：`~allennlp.data.vocabulary.Vocabulary`，它产生实际的数组。标记可以表示为单个ID（例如，单词“cat”由数字表示34），或作为字符ID列表（例如，“cat”由数字[23,10,18]表示），或以某种其他方式，你可以提出（例如，如果你有一些结构化的输入你想要在数据数组中以特殊方式表示，你可以在这里做到这一点）。default_implementation ='single_id'def count_vocab_items（self，token：Token，counter：Dict [str，dict [str，int]]）： ：class：`Vocabulary`需要为我们在训练数据中看到的任何字符串分配索引（可能进行一些频率过滤和使用OOV，或者使用词汇表，令牌）。对于令牌中存在的任何词汇项，此方法采用令牌和计数字典和增量计数。如果这是单个令牌ID表示，则词汇表项可能是令牌本身。如果这是令牌字符表示，则词汇表项是令牌中的所有字符。def tokens_to_indices（self，tokens：List [Token]，词汇：Vocabulary，index_name：str） - &gt; Dict [str，List [TokenType]]： 获取令牌列表并将其转换为一组或多组索引。这可能只是词​​汇表中每个标记的ID。或者它可以将每个标记分成字符并返回每个字符一个ID。或者（例如，在字节对编码的情况下）可能没有从单个令牌到索引的干净映射。def get_padding_token（self） - &gt; TokenType： 当我们需要添加填充令牌时，它们应该是什么样的？此方法返回由以下函数返回的任何类型的“空白”标记：func：`tokens_to_indices`。def get_padding_lengths（self，token：TokenType） - &gt; Dict [str，int]： 此方法返回给定标记的填充字典，该字典指定需要填充的所有数组的长度。例如，对于单个ID令牌，返回的字典将为空，但对于令牌字符表示，这将返回令牌中的字符数。def pad_token_sequence（self，tokens：Dict [str，List [TokenType]]，desired_num_tokens：Dict [str，int]，padding_lengths：Dict [str，int]） - &gt; Dict [str，List [TokenType]]： 此方法将令牌列表填充到“desired_num_tokens”并返回输入令牌的填充副本。如果输入标记列表长于“desired_num_tokens”，那么它将被截断。 `padding_lengths`用于提供在某些情况下需要的补充填充参数。例如，它包含在执行字符级填充时填充字符的宽度。def get_keys（self，index_name：str） - &gt; List [str]： 返回此索引器从`tokens_to_indices`返回的键列表。 Field 字段Field 字段基类1234567891011121314151617181920212223242526272829Field“字段”是instance数据实例的一部分，最终作为模型中的张量（作为输入或输出）。 数据实例只是字段的集合。 字段最多经历两个处理步骤：（1）将标记化字段转换为标记ID，（2）填充包含标记id（或任何其他数字数据）的字段（如果需要）并转换为张量。 `Field`API有这两个步骤的方法，虽然它们可能不需要一些具体的`Field`类 - 如果你的字段没有任何需要索引的字符串，你不需要实现`count_vocab_items` 或`索引`。 这些方法默认为`pass`。 一旦计算出词汇表并对所有字段编制索引，我们将确定填充长度，然后智能地将实例批处理并将它们填充到实际张量中。def count_vocab_items（self，counter：Dict [str，dict [str，int]]）：“”“如果这个字段中的字符串需要通过：class：`Vocabulary`转换成整数，这里就是我们统计它们的位置，以确定哪些令牌在词汇表之内或之外。 如果你的`Field`没有任何需要转换为索引的字符串，你不需要实现这个方法。 关于这个`counter`的注释：因为`Fields`可以代表概念上不同的东西，我们用`namespaces`分隔词汇项。这样，我们可以使用单个共享机制来处理从字符串到所有字段中的整数的所有映射，同时保持`TextField`中的单词与`LabelField`中的标签共享相同的id（例如，\"entailment\" or \"contradiction\"是蕴涵任务中的标签” 另外，单个`Field`可能想要使用多个名称空间 - “TextFields”可以表示为单词ID和字符id的组合，并且您不希望单词和字符共享相同的vocabulary - “a”作为单词应该从“a”作为一个字符获得不同的id，并且单词和字符的词汇量大小非常不同。 因此，`counter`对象中的第一个键是`namespace'，如“tokens”，“token_characters”，“tags”或“labels”，第二个键是实际的词汇表项item。 def index（self，vocab：Vocabulary）： 给定一个：class：`Vocabulary`，将该字段中的所有字符串转换为（通常）整数。这个`修改``Field`对象，它不返回任何东西。 如果你的`Field`没有任何需要转换为索引的字符串，你不需要实现这个方法。def get_padding_lengths（self） - &gt; Dict [str，int]： 如果此字段中有需要填充的内容，请在此处记下。为了填充一批实例，我们从批处理中获取所有长度，取最大值，并将所有内容填充到该长度（或使用预先指定的最大长度）。返回值是将键映射到长度的字典，例如&#123;'num_tokens'：13&#125;。 这总是在：func：`index`之后调用。 msgstr“”“引发NotImplementedErrordef as_tensor（self，padding_lengths：Dict [str，int]） - &gt; DataArray： 给定一组指定的填充长度，实际填充此字段中的数据并返回正确形状的割炬张量（或更复杂的数据结构）。我们还采用了一些在构建火炬传感器时很重要的参数。 参数---------- padding_lengths：`Dict [str，int]`这个字典将具有与func：`get_padding_lengths`相同的键。这些值指定填充每个相关维度时使用的长度，这些维度在批处理中的所有实例之间聚合。 msgstr“”“引发NotImplementedErrordef empty_field（self） - &gt;'Field'： 因此`ListField`可以填充列表中的字段数（例如，答案选项`TextFields`的数量），我们需要表示每种类型的空字段。这会返回。这只会在我们调用时调用：func：`as_tensor`，所以你不必担心在这个空字段上调用`get_padding_lengths`，`count_vocab_items`等等。 我们使这个实例方法而不是静态方法，这样如果Field中有任何状态，我们可以复制它（例如，`TextField`中的标记索引器）。 msgstr“”“引发NotImplementedErrordef batch_tensors（self，tensor_list：List [DataArray]） - &gt; DataArray：#type：ignore从`Instances`列表中获取`Field.as_tensor（）`的输出，并将其合并为一个批量张量为此`Field` 。这里基类的默认实现处理`as_tensor`为每个实例返回一个火炬张量的情况。如果您的子类返回除此之外的其他内容，则需要覆盖此方法。 这个操作不会修改`self`，但在某些情况下我们需要`self`中包含的信息来执行批处理，所以这是一个实例方法，而不是类方法。 “”#pylint：disable = no-self-use return torch.stack（tensor_list） SequenceField 序列字段12SequenceField`SequenceField`代表一系列事物。 这个类只是在`Field` :: func：`sequence_length`上添加了一个方法。 它的存在使得`SequenceLabelField`，`IndexField`和其他类似的`Fields`可以有一个类型要求，具有一致的API，它们是指向`TextField`中的单词，`ListField`中的项目，还是 别的。 TextField 文本字段123456TextField这个`Field`代表一个字符串标记列表。 在构造此对象之前，需要使用：class：`~allennlp.data.tokenizers.tokenizer.Tokenizer`来标记原始字符串。因为字符串标记可以通过多种方式表示为索引数组，所以我们还会使用以下字典：class：`~allennlp.data.token_indexers.token_indexer.TokenIndexer`对象，用于将标记转换为索引。 每个“TokenIndexer”可以将每个标记表示为单个ID，或者字符ID列表或其他内容。该字段将被转换为数组字典，每个`TokenIndexer`一个。 `SingleIdTokenIndexer`生成一个形状数组（num_tokens，），而`TokenCharactersIndexer`生成一个形状数组（num_tokens，num_characters）。 SequenceLabelField1234567891011121314151617181920212223SequenceLabelField`SequenceLabelField`为a中的每个元素分配一个分类标签产品类别：`〜allennlp.data.fields.sequence_field.SequenceField`。因为它是某个其他字段的标签，我们在此处将该字段作为输入，我们将其用于确定我们的填充和其他东西。此字段将转换为整数类ID列表，表示正确的类对于序列中的每个元素。参数----------标签：`Union [List [str]，List [int]]` 一系列分类标签，编码为字符串或整数。这些可能是POS标签，如[NN，JJ，...]，BIO标签，如[B-PERS，I-PERS，O，O，...]，或任何其他分类标签序列。如果标签被编码为整数，则不会使用词汇对其进行索引。sequence_field：`SequenceField` 包含此SequenceLabelField`标记序列的字段。大多数情况下，这是一个“TextField”，用于标记句子中的单个标记。label_namespace：`str`，optional（default ='labels'） 用于将标记字符串转换为整数的命名空间。我们将标记字符串转换为整数，并且此参数告诉`Vocabulary`对象从字符串到整数的映射使用（因此“O”作为标记不会获得与“O”作为单词相同的id） 。“””＃用户可能希望将此字段与使用OOV / PAD令牌的命名空间一起使用。＃对于此类的每个实例化（即每个数据），将重复此警告#instance），喷出很多警告，所以这个类变量只用于记录单个变量每个命名空间＃警告。 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"},{"name":"allennlp","slug":"人工智能/深度学习/allennlp","permalink":"http://yoursite.com/categories/人工智能/深度学习/allennlp/"}],"tags":[{"name":"allennlp","slug":"allennlp","permalink":"http://yoursite.com/tags/allennlp/"},{"name":"command","slug":"command","permalink":"http://yoursite.com/tags/command/"},{"name":"命令","slug":"命令","permalink":"http://yoursite.com/tags/命令/"}]},{"title":"allennlp_json_config","slug":"人工智能/深度学习/allennlp/allennlp-json-config","date":"2019-03-31T11:04:31.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/人工智能/深度学习/allennlp/allennlp-json-config/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/allennlp/allennlp-json-config/","excerpt":"","text":"第一层配置1234567&#123; \"dataset_reader\": &#123;&#125;, \"train_data_path\": \"\", \"model\": &#123;&#125;, \"iterator\": &#123;&#125;, \"trainer\": &#123;&#125;&#125; 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"},{"name":"allennlp","slug":"人工智能/深度学习/allennlp","permalink":"http://yoursite.com/categories/人工智能/深度学习/allennlp/"}],"tags":[{"name":"allennlp","slug":"allennlp","permalink":"http://yoursite.com/tags/allennlp/"},{"name":"配置文件","slug":"配置文件","permalink":"http://yoursite.com/tags/配置文件/"}]},{"title":"allennlp_models","slug":"人工智能/深度学习/allennlp/allennlp-models","date":"2019-03-29T09:29:52.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/人工智能/深度学习/allennlp/allennlp-models/","link":"","permalink":"http://yoursite.com/wiki/人工智能/深度学习/allennlp/allennlp-models/","excerpt":"","text":"crf_taggerallennlp/models/crf_tagger.py class CrfTagger 12345678910111213141516171819202122232425262728293031323334`CrfTagger`使用`Seq2SeqEncoder`编码一系列文本，然后使用条件随机场模型来预测序列中每个标记的标记。参数----------vocab：`Vocabulary'，必需 用于计算输入/输出尺寸大小所需的词汇表。text_field_embedder：`TextFieldEmbedder`，必需 用于嵌入标记`TextField`，我们将其作为模型的输入。encoder：`Seq2SeqEncoder` 我们将在嵌入令牌和预测输出标签之间使用的编码器。label_namespace：`str`，optional（默认 =`labels`） 这是计算SpanBasedF1Measure指标所必需的。 除非你做了一些特殊处理，否则默认值即可。feedforward：`FeedForward`，可选，（默认=None）。 在编码器之后应用的可选前馈层。label_encoding：`str`，optional（默认=None） 在计算跨度f1时使用的标签编码，并在解码时限制CRF。有效选项是“BIO”，“BIOUL”，“IOB1”，“BMES”。 如果`calculate_span_f1`或`constrain_crf_decoding`为真，则为必需。include_start_end_transitions：`bool`，可选（默认=True） 是否在CRF中包含开始和结束转换参数。constrain_crf_decoding：`bool`，optional（默认=None） 如果为“True”，则CRF在解码时被约束以产生有效的标签序列。如果这是'True`，则需要`label_encoding`。如果指定了“None”和label_encoding，则将其设置为“True”。 如果未指定`None`和label_encoding，则默认为'False`。calculate_span_f1：`bool`，可选（默认=None） 在培训期间计算跨度级F1指标。如果这是'True`，则需要`label_encoding`。如果指定了“None”和label_encoding，则将其设置为“True”。 如果未指定`None`和label_encoding，则默认为'False`。dropout：`float`，optional（默认=None）verbose_metrics：`bool`，可选（默认= False） 如果为true，则除了整体统计信息之外，还将为每个标签类返回指标。初始化程序：`InitializerApplicator`，可选（默认=`InitializerApplicator（）`） 用于初始化模型参数。正规化器：`RegularizerApplicator`，可选（默认=None） 如果提供，将用于计算训练期间的正则化惩罚。 Coreference Resolution 指代消解该模型在CoNLL测试集的F1达到63.0% 123456# 引用模型from allennlp.predictors.predictor import Predictorpredictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")predictor.predict( document=\"The woman reading a newspaper sat on the bench with her dog.\") Named Entity Recognition 命名实体识别模型使用ELMo嵌入的biLSTM 参考资料 https://allennlp.org/models","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"深度学习","slug":"人工智能/深度学习","permalink":"http://yoursite.com/categories/人工智能/深度学习/"},{"name":"allennlp","slug":"人工智能/深度学习/allennlp","permalink":"http://yoursite.com/categories/人工智能/深度学习/allennlp/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"allennlp","slug":"allennlp","permalink":"http://yoursite.com/tags/allennlp/"}]},{"title":"python代码加密","slug":"python/python代码加密","date":"2019-03-29T07:45:48.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python代码加密/","link":"","permalink":"http://yoursite.com/wiki/python/python代码加密/","excerpt":"","text":"目前的加密手段： 源代码混淆：只降低源码可读性，对破解有一定的干扰作用 只发行 pyc: 可以用现成工具复原 打包 exe: 可以用现成工具复原 cython打包so文件: 要加密单一的模块 /特制算法很有效，不过对很多复杂模块无法兼容，比如 django 写的 app 修改python解释器: 未丢失信息，容易复原 总结：python语言在设计理念上倾向于开源，没有很完美的加密方案，只能有限程度上增加破解者的难度（对于有经验的破解者形同虚设） 其他方案： 核心代码逻辑替换成 C++ 或 go语言 终极方案： 只提供api服务，不提供本地运行的程序","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"命令行参数和环境变量","slug":"python/命令行参数和环境变量","date":"2019-03-29T01:04:15.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/命令行参数和环境变量/","link":"","permalink":"http://yoursite.com/wiki/python/命令行参数和环境变量/","excerpt":"","text":"命令行参数argv12345import sysnum = len(sys.argv) # 参数个数script_name = sys.argv[0] # 脚本命令名称arg1 = sys.argv[1] # 参数1arg2 = sys.argv[2] # 参数2 argparse命令行参数模块基础用法：必须参数12345678# 导入命令行解析的库文件import argparse # pkg是一个必须的位置参数(因为前面没有横杠，所以是位置参数。因为没有默认值，所以是必须参数)parse.add_argument('pkg',help='help')# 命令行执行 --help时，会查看的说明parse = argparse.ArgumentParser(description=\"test!!\") 可选参数1234567# nargs是默认值，有默认值的参数是可选参数parse.add_argument('keyoukewu',help='xx'，nargs='?') # 前缀是‘-’的参数名是缩写，前缀是‘--’的参数名是全称parse.add_argument('-a','--abc',help='xx',nargs='?')print(args.abc)print(args.a) # 是错误的：因为解析的时候必须用全称 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"thread多线程模块","slug":"python/多线程、多进程、协程","date":"2019-03-27T08:30:26.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/多线程、多进程、协程/","link":"","permalink":"http://yoursite.com/wiki/python/多线程、多进程、协程/","excerpt":"","text":"概述threading用于提供线程相关的操作，线程是应用程序中工作的最小单元。 python当前版本的多线程库没有实现优先级、线程组，线程也不能被停止、暂停、恢复、中断。 threading模块提供的类： Thread, Lock, Rlock, Condition, [Bounded]Semaphore, Event, Timer, local。 threading 模块提供的常用方法： threading.currentThread(): 返回当前的线程变量。 threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 ​ threading.currentThread().getName()获取当前程序的线程名称 threading 模块提供的常量： threading.TIMEOUT_MAX 设置threading全局超时时间。 Thread类构造方法： Thread(group=None, target=None, name=None, args=(), kwargs={}) group: 线程组，目前还没有实现，库引用中提示必须是None； target: 要执行的方法； name: 线程名； args/kwargs: 要传入方法的参数。 实例方法： isAlive(): 返回线程是否在运行。正在运行指启动后、终止前。 get/setName(name): 获取/设置线程名。 start(): 线程准备就绪，等待CPU调度 is/setDaemon(bool): 获取/设置是后台线程（默认前台线程（False））。（在start之前设置） 如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，主线程和后台线程均停止 如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止 start(): 启动线程。 join([timeout]): 阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout（可选参数）。 123456789101112131415161718192021222324import threadingimport timedef action(arg): time.sleep(1) print('the arg is:%s\\r' %arg) #运行方法一：将要执行的方法作为参数传给Thread的构造方法for i in range(4): t =threading.Thread(target=action,args=(i,)) t.start()#运行方法二：从Thread继承，并重写run()class MyThread(threading.Thread): def __init__(self,arg): super().__init__()#注意：一定要显式的调用父类的初始化函数。 self.arg=arg def run(self):#定义每个线程要运行的函数===action() time.sleep(1) print('the arg is:%s\\r' % self.arg)for i in xrange(4): t = MyThread(i) t.start() 123456789101112thread_list = [] #线程存放列表for i in xrange(4): t =threading.Thread(target=action,args=(i,)) t.setDaemon(True) thread_list.append(t)for t in thread_list: t.start()for t in thread_list: t.join()# join()阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout，即使设置了setDeamon（True）主线程依然要等待子线程结束。 参考资料 [python–threading多线程总结]","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"Queue 队列模块相关","slug":"python/queue队列","date":"2019-03-25T00:13:50.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/queue队列/","link":"","permalink":"http://yoursite.com/wiki/python/queue队列/","excerpt":"","text":"heap 堆 stack 栈 queue 队列 堆的逻辑结构就是完全二叉树，并且二叉树中父节点的值小于等于该节点的所有子节点的值。 特征：heap[k] &lt;= heap[2k+1] 并且 heap[k] &lt;= heap[2k+2] （其中 k 为索引，从 0 开始计数） heapq 堆队列基本操作12345678910111213141516171819202122import heapq heap = []#向堆中插入元素，heapq会维护列表heap中的元素保持堆的性质 heapq.heappush(heap, item) #heapq把列表x转换成堆 O(n)复杂度heapq.heapify(x) # 最小堆heapq._heapify_max(x) # 最大堆#从可迭代的迭代器中返回最大的n个数，可以指定比较的key heapq.nlargest(n, iterable[, key]) #从可迭代的迭代器中返回最小的n个数，可以指定比较的key heapq.nsmallest(n, iterable[, key]) #从堆中删除元素，返回值是堆中最小或者最大的元素 heapq.heappop(heap)heapq.heappushpop(heap, item)：向 heap 中加入 item 元素，并返回 heap 中最小元素。heapq.heapreplace(heap,item): python3中heappushpop的更高效版。 原理使用的比较函数：lt, gt, cmp 内置数据类型和自定义类型，默认使用 lt （小于比较函数）进行比较 元组类型默认使用 cmp 比较 （先比较第1列，相同再比较第2列，以此类推……） 代码示范12345678import heapqh = []# 默认是最小堆heapq.heappush(h, (5, 'write code'))heapq.heappush(h, (7, 'release product'))heapq.heappush(h, (1, 'write spec'))heapq.heappush(h, (3, 'create tests'))min_item = heapq.heappop(h) # (1, 'write spec') PriorityQueue 优先队列 queue库是线程安全的 1234567891011121314151617181920#向队列中添加元素Queue.put(item[, block[, timeout]])#从队列中获取元素Queue.get([block[, timeout]])#队列判空Queue.empty()#队列大小Queue.qsize()try: import Queue as Q #python version &lt; 3.0except ImportError: import queue as Q #python3.*q = Q.PriorityQueue()q.put(19)q.put(1)q.put(5)while not q.empty(): print(q.get()) 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"python基本数据类型","slug":"python/python基本数据类型","date":"2019-03-21T06:35:53.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/python基本数据类型/","link":"","permalink":"http://yoursite.com/wiki/python/python基本数据类型/","excerpt":"","text":"时间复杂度注释n代表容器中元素的数量，k代表参数的值，或者参数的数量。 [注1] =这些业务依赖于“摊销最坏情况”的“Amortized摊销”部分。 根据容器的历史，个别动作可能需要很长时间。 [注2] =对于这些操作，最坏情况n是容器达到的最大尺寸，而不仅仅是当前尺寸。 例如，如果将N个对象添加到字典中，则删除N-1，仍然会为N个对象（至少）调整字典的大小，直到进行另一次插入为止。 list 列表是以数组（Array）实现的。最大的开销发生在超过当前分配大小的增长，这种情况下所有元素都需要移动；或者是在起始位置附近插入或者删除元素，这种情况下所有在该位置后面的元素都需要移动。如果你需要在一个队列的两端进行增删的操作，应当使用collections.deque（双向队列） 操作 平均情况 最坏情况 复制 O(n) O(n) append[注1] O(1) O(1) 插入 insert O(n) O(n) 取元素 O(1) O(1) 更改元素 O(1) O(1) 删除元素 O(n) O(n) 遍历 O(n) O(n) 取切片 O(k) O(k) 删除切片 O(n) O(n) 更改切片 O(k+n) O(k+n) extend[注1] O(k) O(k) 排序 O(n log n) O(n log n) 列表乘法 O(nk) O(nk) x in s O(n) min(s), max(s) O(n) 获取长度 O(1) O(1) 双向队列（collections.deque） deque （double-ended queue，双向队列）是以双向链表的形式实现的 (Well, a list of arrays rather than objects, for greater efficiency)。双向队列的两端都是可达的，但从查找队列中间的元素较为缓慢，增删元素就更慢了。 操作 平均情况 最坏情况 复制 O(n) O(n) append O(1) O(1) appendleft O(1) O(1) pop O(1) O(1) popleft O(1) O(1) extend O(k) O(k) extendleft O(k) O(k) rotate O(k) O(k) remove O(n) O(n) 字典（dict） 下列字典的平均情况基于以下假设: 1 对象的散列函数足够撸棒（robust），不会发生冲突。2 字典的键是从所有可能的键的集合中随机选择的。 小窍门：只使用字符串作为字典的键。这么做虽然不会影响算法的时间复杂度，但会对常数项产生显著的影响，这决定了你的一段程序能多快跑完。 操作 平均情况 最坏情况 复制[注2] O(n) O(n) 取元素 O(1) O(n) 更改元素[注1] O(1) O(n) 删除元素 O(1) O(n) 遍历[注2] O(n) O(n) dict.setdefault(key, default=None)返回dict[key] 。如果没有key，则设置dict[key]=default并返回dict[key] 集合（set） 未列出的操作可参考 dict —— 二者的实现非常相似。 操作 平均情况 最坏情况 x in s O(1) O(n) 并集 s\\ t O(len(s)+len(t)) 交集 s&amp;t O(min(len(s), len(t)) O(len(s) * len(t)) 差集 s-t O(len(s)) s.difference_update(t) O(len(t)) 对称差集 s^t O(len(s)) O(len(s) * len(t)) s.symmetric_difference_update(t) O(len(t)) O(len(t) * len(s)) 由源码得知，求差集（s-t，或s.difference(t)）运算与更新为差集（s.difference_uptate(t)）运算的时间复杂度并不相同！前者是将在s中，但不在t中的元素添加到新的集合中，因此时间复杂度为O(len(s))；后者是将在t中的元素从s中移除，因此时间复杂度为O(len(t))。因此，使用时请留心，根据两个集合的大小以及是否需要新集合来选择合适的方法。 集合的s-t运算中，并不要求t也一定是集合。只要t是可遍历的对象即可。 参考资料Python内置方法的时间复杂度","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"配置vscode","slug":"编程基础/安装与配置/set-vscode","date":"2019-03-20T05:59:18.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/安装与配置/set-vscode/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/set-vscode/","excerpt":"","text":"配置快捷键12向下复制一行---改成Ctrl+Deditor.action.copyLinesDownAction 插件 插件名 说明 Project Manager 项目管理 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"双指针找环","slug":"编程基础/leetcode/141. Linked_List_Cycle","date":"2019-03-19T14:08:18.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/leetcode/141. Linked_List_Cycle/","link":"","permalink":"http://yoursite.com/wiki/编程基础/leetcode/141. Linked_List_Cycle/","excerpt":"","text":"题目https://leetcode.com/problems/linked-list-cycle/ 141.给一个链表的头节点,判断链表是否有环 解法1: 用set判断重复优点: 逻辑直观, 容易理解 比双指针更快 缺点: 空间占用比双指针更多 解法2: 双指针 用两个指针指向head 进行循环: 每次慢指针下移一次,快指针下移两次 如果快指针指向结尾则无循环,如果两个指针指向同一个节点则有循环","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"leetcode","slug":"编程基础/leetcode","permalink":"http://yoursite.com/categories/编程基础/leetcode/"}],"tags":[]},{"title":"使用docker","slug":"编程基础/软件使用备忘/use-docker","date":"2019-03-19T06:49:42.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-docker/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-docker/","excerpt":"","text":"配置### debug: 直接运行命令会报错, 进入bash再执行可用现象: docker的command=apachectl -D FOREGROUND执行失败 docker的command=bash, 在bash手工执行命令可成功 原因: 电脑性能差, apachectl依赖程序未全部启动, 此时执行命令所以报错. 解决方法: 把command命令放在command.sh 中, 在最开始执行sleep 5 等待5秒. 然后把command.sh映射到容器内, command=bash command.sh 服务设置刚安装完成后，需要重启机器，才能启动服务 1234567# Ubuntusudo service docker start # 启动服务# manjarosudo systemctl start docker # 启动服务sudo systemctl status docker # 查看服务状态systemctl enable docker # 开机启动 设置信任本地仓库12345678910### 方法1 (如果不行可以尝试方法2)# 1. 在/etc/default/docker添加：-- insecure-registry 127.0.0.1:5000-- insecure-registry 192.168.31.103:5000# 2. 再重启docker 服务### 方法2# 1. 创建文件/etc/docker/daemon.json&#123; \"insecure-registries\":[\"192.168.163.131:5000\"]&#125;# 2. 再重启docker 服务 容器-常用命令123456789101112# 查看容器列表 ## -a 查看全部，否则查看运行中的docker ps# 删除容器 -f强制删除docker rm -f xxx# 实时查看容器占用的CPU和内存资源docker stats# 从容器生成镜像docker commit -m \"change somth\" -a \"somebody info\" container_id(docker ps -a获取id) 新镜像名字 容器自启动设置12docker run --restart=on-failure:10 xxxdocker run --restart=always xxx no 容器退出时不要自动重启。这个是默认值。 on-failure[:max-retries] 只在容器以非0状态码退出时重启。可选的，可以退出docker daemon尝试重启容器的次数。在每次重启容器之前，重启延迟比上次增加一倍，从100毫秒开始，来防止影响服务器。这意味着daemon将等待100ms,然后200ms，直到超过on-failure限制，或执行docker stop或docker rm -f 。如果容器重启成功[容器启动后并运行至少10秒]，然后delay重置为默认的100ms。ms, 400, 800, 1600等等，直到超过on-failure限制，或执行docker stop或docker rm -f always 不管退出状态码是什么始终重启容器。当指定always时，docker daemon将无限次数地重启容器。容器也会在daemon启动时尝试重启，不管容器当时的状态如何。(和unless-stopped 参数值效果一样) 镜像-常用命令123456789101112# 查看镜像列表docker images# 导出镜像的压缩文件（可以压缩多个镜像，例如xxx和yyy）docker save xxx:tag yyy:tag2 | gzip &gt; img.tar.gz # 镜像重命名docker tag xxx:tag xxx2:tag2# 删除镜像 ## -f强制删除docker rmi -f xxx:tag DockerFile123# 使用dockerfile生成镜像 -t添加标签名称(可以多个) docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest . 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"es-搜索操作","slug":"人工智能/搜索引擎/es-search","date":"2019-03-17T01:25:03.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/es-search/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/es-search/","excerpt":"","text":"搜索备忘一原生的url接口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137# 搜索位置(url)/_search # 在所有的索引中搜索所有的类型/gb/_search # 在 gb 索引中搜索所有的类型/gb,us/_search # 在 gb 和 us 索引中搜索所有的文档/g*,u*/_search # 在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search # 在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search # 在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search # 在所有的索引中搜索 user 和 tweet 类型# 分页(url)POST /_search # 默认size=10, from=0 从0返回POST /_search?size=5 # 第二页POST /_search?size=5&amp;from=5POST /_search?size=5&amp;from=10# term 精确查找,不计算相关度.&#123; \"term\" : &#123; \"price\" : 20 &#125;&#125;# 用constant_score 把term包装成filterPOST /my_store/products/_search&#123; \"query\" : &#123; \"constant_score\" : &#123; \"filter\" : &#123; \"term\" : &#123; \"productID\" : \"XHDK-A-1293-#fJ3\" &#125; &#125; &#125; &#125;&#125;# range 过滤器(filter): age &gt; 30# 过滤器执行速度非常快，不会计算相关度. 精确的筛选.POST /megacorp/employee/_search&#123; \"query\" : &#123; \"bool\": &#123; \"must\": &#123; \"match\" : &#123;\"last_name\" : \"smith\" &#125; &#125;, \"filter\": &#123; \"range\" : &#123; \"age\" : &#123; \"gt\" : 30 &#125; &#125; &#125; &#125; &#125;&#125;# 全文搜索: 返回相关性排序的结果． 如果有rock没有climbing也可能会返回结果.POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125;&#123; \"query\": &#123; \"match_phrase\": &#123; \"content\" : &#123; \"query\" : \"我的宝马多少马力\", \"slop\" : 1 &#125; &#125; &#125;&#125;# 实际上下面的query才能正确返回结果，搜索的是content这个字段里包含对应文本的文档# 精确匹配一系列单词或者短语POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125;# 高亮搜索POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;, \"highlight\": &#123; \"fields\" : &#123; \"about\" : &#123;&#125; &#125; &#125;&#125;# 返回:&#123; ... \"hits\": &#123; \"total\": 1, \"max_score\": 0.23013961, \"hits\": [ &#123; ... \"_score\": 0.23013961, \"_source\": &#123; \"first_name\": \"John\", \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125;, \"highlight\": &#123; \"about\": [ \"I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;\" ] &#125; &#125; ] &#125;&#125;# 聚合（aggregations）: 统计某些标签的数量(是在搜索结果中进行统计,可以结合其他query)POST /megacorp/employee/_search&#123; \"aggs\": &#123; \"all_interests\": &#123; \"terms\": &#123; \"field\": \"interests\" &#125; &#125; &#125;&#125; 搜索备忘二http://www.cnblogs.com/yjf512/p/4897294.html 12345678910111213组合式搜索&#123; \"query\": &#123; &#123; \"bool\": &#123; \"must\": &#123; \"match\": &#123; \"tweet\": \"elasticsearch\" &#125;&#125;, \"must_not\": &#123; \"match\": &#123; \"name\": \"mary\" &#125;&#125;, \"should\": &#123; \"match\": &#123; \"tweet\": \"full text\" &#125;&#125;, # 这些match可以是数组 \"filter\": &#123; \"range\": &#123; \"age\" : &#123; \"gt\" : 30 &#125;&#125; &#125; &#125; &#125; &#125; &#125; elasticsearch 查询（match和term）es中的查询请求有两种方式，一种是简易版的查询，另外一种是使用JSON完整的请求体，叫做结构化查询（DSL）。由于DSL查询更为直观也更为简易，所以大都使用这种方式。DSL查询是POST过去一个json，由于post的请求是json格式的，所以存在很多灵活性，也有很多形式。这里有一个地方注意的是官方文档里面给的例子的json结构只是一部分，并不是可以直接黏贴复制进去使用的。一般要在外面加个query为key的机构。 match最简单的一个match例子： 查询和”我的宝马多少马力”这个查询语句匹配的文档。 123456789&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 上面的查询匹配就会进行分词，比如”宝马多少马力”会被分词为”宝马 多少 马力”, 所有有关”宝马 多少 马力”, 那么所有包含这三个词中的一个或多个的文档就会被搜索出来。并且根据lucene的评分机制(TF/IDF)来进行评分。 match_phrase比如上面一个例子，一个文档”我的保时捷马力不错”也会被搜索出来，那么想要精确匹配所有同时包含”宝马 多少 马力”的文档怎么做？就要使用 match_phrase 了 123456789&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 完全匹配可能比较严，我们会希望有个可调节因子，少匹配一个也满足，那就需要使用到slop。 12345678910&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;slop&quot; : 1 &#125; &#125; &#125;&#125; multi_match如果我们希望两个字段进行匹配，其中一个字段有这个文档就满足的话，使用multi_match 12345678&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;fields&quot; : [&quot;title&quot;, &quot;content&quot;] &#125; &#125;&#125; 但是multi_match就涉及到匹配评分的问题了。 我们希望完全匹配的文档占的评分比较高，则需要使用best_fields12345678910111213&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 意思就是完全匹配”宝马 发动机”的文档评分会比较靠前，如果只匹配宝马的文档评分乘以0.3的系数 我们希望越多字段匹配的文档评分越高，就要使用most_fields123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; 我们会希望这个词条的分词词汇是分配到不同字段中的，那么就使用cross_fields123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; termterm是代表完全匹配，即不进行分词器分析，文档中必须包含整个搜索的词汇 1234567&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;汽车保养&quot; &#125; &#125;&#125; 查出的所有文档都包含”汽车保养”这个词组的词汇。 使用term要确定的是这个字段是否“被分析”(analyzed)，默认的字符串是被分析的。 拿官网上的例子举例： mapping是这样的： 12345678910111213141516171819202122PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_text&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;exact_value&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125; &#125; &#125;&#125;PUT my_index/my_type/1&#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot;, &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; 其中的full_text是被分析过的，所以full_text的索引中存的就是[quick, foxes]，而extra_value中存的是[Quick Foxes!]。 那下面的几个请求： 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求的出数据，因为完全匹配 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求不出数据的，因为full_text分词后的结果中没有[Quick Foxes!]这个分词。 bool联合查询: must,should,must_not如果我们想要请求”content中带宝马，但是tag中不带宝马”这样类似的需求，就需要用到bool联合查询。联合查询就会使用到must,should,must_not三种关键词。 这三个可以这么理解 must: 文档必须完全匹配条件 should: should下面会带一个以上的条件，至少满足一个条件，这个文档就符合should must_not: 文档必须不匹配条件 比如上面那个需求： 12345678910111213141516&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;宝马&quot; &#125; &#125;, &quot;must_not&quot;: &#123; &quot;term&quot;: &#123; &quot;tags&quot;: &quot;宝马&quot; &#125; &#125; &#125; &#125;&#125; 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[{"name":"es","slug":"es","permalink":"http://yoursite.com/tags/es/"},{"name":"elastic_search","slug":"elastic-search","permalink":"http://yoursite.com/tags/elastic-search/"},{"name":"search","slug":"search","permalink":"http://yoursite.com/tags/search/"},{"name":"搜索","slug":"搜索","permalink":"http://yoursite.com/tags/搜索/"}]},{"title":"安装ES","slug":"人工智能/搜索引擎/install-es","date":"2019-03-17T01:21:17.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/install-es/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/install-es/","excerpt":"","text":"安装ES-E Configure a setting-V, –version-d, –daemonize 守护进程，后台启动-p, –pidfile Creates a pid file in the specified path on start-q, –quiet Turns off standard output/error streams logging in console-s, –silent show minimal output-v, –verbose show verbose output 1234567891011121314151617181920# 启动(先-s 启动成功, 再用-d后台启动)elasticsearch/bin/elasticsearch -s # 配置文件elasticsearch/config/elasticsearch.yml# 检查状态curl -XGET '192.168.31.185:9200/_cat/health?v'# 测试是否启动成功curl 'http://192.168.31.185:9200/?pretty'# 查看所有索引！！curl -XGET '192.168.31.185:9200/_cat/indices?v'# 创建一个名字=ip_focus 的索引 pretty参数让返回结果更易读curl -XPUT '192.168.31.185:9200/ip_focus?pretty'# 删除一个索引curl -XDELETE '192.168.31.185:9200/customer?pretty'# 新建/修改一个文档（一行数据） _id=1 如果索引不存在，会自动新建索引=customer# 当我们没有明确指定ID的时候，我们需要使用POST方法代替PUT来发送请求PUT /customer/doc/1 &#123; \"name\": \"John Doe\" &#125; 启动时报错：elasticsearch max virtual memory areas vm.max_map_count [65530] is too low 12345sudo vim /etc/sysctl.conf # 在文件末尾加入vm.max_map_count=655360# 然后执行sudo sysctl -p 安装Kibana1234567891011121314151617去官网下载https://www.elastic.co/cn/downloads/kibana# 解压文件tar –zxvf kibana-5.5.2-linux-x86_64.tar.gz–C ./kibana/# 去config文件夹编辑kibana.yml#配置本机ip server.host: \"192.168.252.129\" #配置es集群url elasticsearch.url: \"http://192.168.252.129:9200\" # 启动程序 使用&amp;命令启动后，退出当前窗口时需要使用exit退出cd /bin./kibana &amp;访问：http://ip:port ip为kibana安装节点ip，端口默认为5061 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[{"name":"es","slug":"es","permalink":"http://yoursite.com/tags/es/"},{"name":"elastic_search","slug":"elastic-search","permalink":"http://yoursite.com/tags/elastic-search/"},{"name":"Kibana","slug":"Kibana","permalink":"http://yoursite.com/tags/Kibana/"}]},{"title":"使用linux","slug":"编程基础/软件使用备忘/use-linux","date":"2019-03-17T01:05:41.000Z","updated":"2019-11-01T09:12:55.295Z","comments":true,"path":"wiki/编程基础/软件使用备忘/use-linux/","link":"","permalink":"http://yoursite.com/wiki/编程基础/软件使用备忘/use-linux/","excerpt":"","text":"tty终端1234# 进入tty终端Ctrl+Alt+F1 到F6 进入tty1～～tty6# 从tty回到桌面环境Ctrl+Alt+F7 Shell脚本Shell特殊变量 变量 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(“ “)包含时，与 $* 稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 注意:$10 不能获取第十个参数，获取第十个参数需要${10} 常用shell脚本1234567891011121314151617181920212223242526272829变量默认值#当变量a为null或为空字符串时则var=b var=$&#123;a:-b&#125; # $a的默认值='b'var=$&#123;a:-'b'&#125; # $a的默认值='b'var=$&#123;a:-$b&#125; # $a的默认值=$bvar=$&#123;1:-b&#125; # $1的默认值='b'替换变量里的文本内容b=$&#123;a/123/321&#125;; 将$&#123;a&#125;里的第一个123替换为321b=$&#123;a//123/321&#125;; 将$&#123;a&#125;里的所有123替换为321脚本所在目录script_dir=$(cd \"`dirname $0`/.\"; pwd)递归的创建文件夹(-p)mkdir -p xxxx/xxxx关机命令sudo shutdownsudo shutdown -h 10 # 10分钟后关机sudo shutdown -h 14:15 # 希望在14:15关闭计算机sudo shutdown -c # 取消自动关机reboot # 重启电脑shell递归删除指定字符串文件或者目录：1、递归删除文件：find . -name \"*.rej\" | xargs rm -rf2、递归删除目录：find . -type d -name \"*.rej\" | xargs rm -rf shell脚本自动输入sudo密码1echo '密码内容' | sudo 具体命令内容 查看进程的命令12345# 查看内存占用最多的5个程序ps auxw|head -1;ps auxw|sort -rn -k4|head -5# 查看CPU占用最多的5个程序ps auxw|head -1;ps auxw|sort -rn -k3|head -5 常用shell函数 函数定义前可选加”function “ 函数末尾可以加：return 返回 如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值范围 0-255 123456789101112131415161718192021222324252627282930313233# 第一个echo函数demoFun()&#123; echo \"这是我的第一个 shell 函数!\"&#125;# 需要键盘输入的函数funWithReturn()&#123; echo \"这个函数会对输入的两个数字进行相加运算...\" echo \"输入第一个数字: \" read aNum echo \"输入第二个数字: \" read anotherNum echo \"两个数字分别为 $aNum 和 $anotherNum !\" return $(($aNum+$anotherNum))&#125;funWithReturn# 函数返回值在调用该函数后通过 $? 来获得。echo \"输入的两个数字之和为 $? !\"# 分支if condition1then command1elif condition2 then command2else commandNfi if [ $a == $b ] # \"a 等于 b\"if [ $a -gt $b ] # \"a 大于 b\"if [ $a -lt $b ] #\"a 小于 b\" 免密码SSH登录远程服务器 创建自己的私钥和公钥对 1ssh-keygen -C “备注信息” -f ~/.ssh/私钥名称 #【密码输入空】 设置私钥对应的网站,在~/.ssh/config 文件中写入： 12345Host 远程服务器 空格链接多个地址 HostName： 目标主机地址 User：指定的登陆用户名 Port：指定的端口号(可选) IdentifyFile：指定的私钥地址(可选) 免密码SSH远程登录服务器 1ssh-copy-id -i ~/.ssh/私钥名称 远程帐号@远程服务器 -p ssh的端口 12345678910111213141516171819202122 把公钥文件复制到远程服务器，并输入密码后，下次就可以自动验证私钥文件 开机启动设置软件：**Stacer**## 常用命令### 进程相关​```bash# 查看占用内存CPUtop -p **进程ID# 查看所有进程 a=allps -ax# 查看占用socket端口的程序netstat -ap\\|grep **端口号** 文件相关1234567891011# 查看硬盘使用情况df -h# 删除大于100M的文件 或者100k -type=f表示文件类型find ./ -type f -size +100M | xargs rm -rf# 删除文件名末尾是mp3的文件find / -name \"*.mp3\" |xargs rm -rf# 也可以先去掉后面的删除命令看一遍结果，再删除（安全一些）# 查看当前文件夹递归1层大小/末尾可加文件夹 | sort 默认升序 -r降序 -h则按真实大小排序du -h --max-depth=1 | sort -hr 压缩文件 tar123456789101112查看压缩文件内容tar tvf xxx.tar# 切分并压缩文件 pre_xxx是切分文件的前缀tar cjf - file_or_dir |split -b 2000m - pre_xxx.##### 排除的目录（写file_or_dir后面的相对路径即可）tar cjf --exclude=排除的目录 file_or_dir |split -b 2000m - pre_xxx.# 合并然后解压文件 -C 输出到相对位置cat pre_xxx.* |tar xj -C ./../aim_dir/xxx_dir/ 权限相关12345# 给xxx账号设置root权限（sudo）sudo *user*mod-aG sudo xxx# 修改文件的所有者 -R表示递归目录下所有文件chown 用户名:用户组 文件名或目录名 -R 发送网络请求 curl1234567## post 方法# curl -i -X POST -H head文本 -d body_json_data# 示范如下:curl -i -X POST -H 'Content-type':'application/x-www-form-urlencoded; charset=UTF-8' -d &#123;\"json-body\":\"\"&#125; http://192.168.31.189:5858/handle/## get方法curl http://192.168.31.189:5858/ 远程挂载 说明 命令 参数 安装工具：sshfs sudo apt install sshfs 开始挂载 sshfs 用户名@host:远程目录 本地挂载点 -o -p端口 取消挂载 sudo umount -l 挂载点 取消挂载 fusermount -u 挂载点 rename perl版本程序 2个参数 参数一：’s/aaa/bbb/‘ 把aaa替换为bbb 参数二：用* 匹配1个或多个字符 rename ‘s/aaa/bbb/‘ *.json 一条命令kill某个进程1234567891011ps -aux|grep 50050|grep -v grep|cut -c 9-15|xargs kill -9# 截取输入行的第9个字符到第15个字符，而这正好是进程号PID。# xargs命令是用来把前面命令的输出结果（PID）作为“kill -9”命令的参数，并执行该命令# 用正则表达式来kill进程。而不用PIDpkill nginx# 用进程名字kill多个进程。killall nginx 1234# 新建用户sudo adduser 用户名# 增加root权限sudo usermod -aG sudo 用户名 sed-正则表达式 awk,sed都可以做字符串各种操作。 ^行的开头 $行的结尾 . 任意单个字符 * 匹配0-多次 + 匹配1次以上 ? 匹配0/1次 debugbash无法输入某个字符可能是/etc/inputrc 里有非法的句子, 非法句子的第一个字符会无法在shell里面正常输入 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"软件使用备忘","slug":"编程基础/软件使用备忘","permalink":"http://yoursite.com/categories/编程基础/软件使用备忘/"}],"tags":[]},{"title":"使用git","slug":"编程基础/git/gitlab","date":"2019-03-17T01:05:30.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/编程基础/git/gitlab/","link":"","permalink":"http://yoursite.com/wiki/编程基础/git/gitlab/","excerpt":"","text":"gitlab使用设置保护分支(master分支不能推送) 默认的master分支是收保护的，不能直接push 进入setting—&gt;Repository—&gt;Protected Branches 设置保护分支 设置issue模板在项目repository的代码文件夹新建一个文件夹: 1234.gitlab/.gitkeep (空文本)```bash .gitlab/issue_templates 文件夹 .gitlab/issue_templates/.gitkeep (空文本) 12 .gitlab/issue_templates/bug.md 12345678910111213#### 系统信息（代码版本等）#### 重现步骤#### 期望结果#### 报错信息 .gitlab/issue_templates/feature.md 123456789101112131415161718192021#### 要解决什么问题#### 要实现什么样的功能#### 用户的应用场景是什么样的#### 注意事项（用户可能有哪些骚操作，等等）#### 对现有功能有什么影响#### 依赖什么模块 labels配置 kind（类型） kind/bug kind/新需求 priority（优先级） priority/紧急 priority/不紧急 size（工作量）：表示 issue 需要大约花费多少时间/精力，可以用来做简单的工作量评估参考。 size/0小 size/1中 size/2大 CRLF/LF/CR三种换行模式： 模式 操作系统 缩写 CRLF windows \\n\\r CR mac OSX \\r LR Linux \\n pycharm可以在状态栏显示换行符使用的模式，如下图： 点击 LF 可以切换文件的换行模式 git–AutoCRLF12345678#提交时转换为LF，检出时转换为CRLFgit config --global core.autocrlf true #windows推荐方式#提交时转换为LF，检出时不转换git config --global core.autocrlf input #提交检出均不转换git config --global core.autocrlf false #Linux/Mac推荐方式 SafeCRLF12345678#拒绝提交包含混合换行符的文件git config --global core.safecrlf true #推荐方式#允许提交包含混合换行符的文件git config --global core.safecrlf false #提交包含混合换行符的文件时给出警告git config --global core.safecrlf warn","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"git","slug":"编程基础/git","permalink":"http://yoursite.com/categories/编程基础/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"http://yoursite.com/tags/gitlab/"}]},{"title":"使用git","slug":"编程基础/git/use-git","date":"2019-03-17T01:05:30.000Z","updated":"2019-10-08T09:22:51.350Z","comments":true,"path":"wiki/编程基础/git/use-git/","link":"","permalink":"http://yoursite.com/wiki/编程基础/git/use-git/","excerpt":"","text":"git命令行标签123456789101112131415161718192021标签分为带附注和不带附注的。我们尽量使用带附注的。# 本地新建一个tag 名称=V1.2git tag -a V1.2 -m 'xxxxx'# 查看本地taggit tag# 查看tag详细信息git tag show V1.2# 推送到远程仓库git push origin --tags# 如果发现有问题，可以删除标签（本地）git tag -d V1.2# 推送空的同名版本到远程仓库，等同于删除远程库里的版本git push origin :refs/tags/V1.2# 获取远程版本，精确拉取某一个版本的代码git fetch origin tag V1.2 在board中添加分栏Issues–&gt;Board–&gt;Add list –&gt;选择要监控的labels 本地分支和远程分支1234567891011121314# 查看当前跟踪关系git branch -vv# 克隆时自动将创建好的`master`分支追踪`origin/master`分支git clone 服务器地址# 建立本地分支 xxx, 追踪远程分支origin/yyygit checkout -b xxx origin/yyy# 将 xxx 分支追踪远程分支 origin/yyygit branch --set-upstream xxx origin/yyy# 设置当前分支跟踪远程分支 origin/yyyygit branch -u origin/yyyy 设置git默认使用的编辑器12提交是出现nano界面，可以退出后设置默认编辑器成VIMgit config --global core.editor \"vim\" 配置远程仓库的密钥 创建自己的私钥和公钥对 -C “备注信息” -f ~/.ssh/私钥名称 ``` 【密码输入空】12. 设置私钥对应的网站,在~/.ssh/config 文件中写入： Host deeplycurious.ai 多个远程仓库地址用空格分隔IdentityFile ~/.ssh/私钥名称 12345678910113. 上传公钥4. 在phabricator里个人--setting--SSH Public Keys-- SSH Key Actions -- Upload Public Key5. Name 随便取， Public Key 是你的公钥的文本内容### 强制修改分支位置```bash可以直接使用 -f 选项让分支指向另一个提交# 例如下面。将 master 分支强制指向 HEAD 的第 3 级父提交。（代码恢复到老版本）git branch -f master HEAD~3 删除git子模组/子模块123456789101112131415161718菜鸟流程:1. 把子模组文件夹剪切到项目外2. add/commit/push(删除对应文件夹)3. 然后把文件夹剪切回来4. add/commit/push(增加对应文件夹)官方流程:# 删除子模块目录及源码rm -rf 子模块目录 # 删除项目目录下.gitmodules文件中子模块相关条目vi .gitmodules # 删除配置项中子模块相关条目vi .git/config # 删除模块下的子模块目录，每个子模块对应一个目录，注意只删除对应的子模块目录即可rm .git/module/* # 如果仍然报错，执行如下：git rm --cached 子模块名称 TODO: 学习 git hooks自动更新提交空文件夹 空的.gitignore文件可以作为占位符，使git只创建一个文件夹，里面是空的（一个空.ignore文件） 删除文件的git控制 如果已经加到版本控制中（push或add过）：用该命令去除控制（不删除本地文件） git rm -r –cached 文件路径 如果刚删除caehed，或未add或push过：直接设置 .gitignore 忽略即可。 .git/info/exclude 该文件和.ignore格式相同，但是不会被提交，不会影响他人的忽略名单。 恢复到删除并add删除操作之前1git checkout -- 文件的名字 忽略已加入控制的文件改动 git update-index –assume-unchanged 文件名 用户名和密码123456# 设置用户名和密码(--global全局配置,否则为本地配置)git config --global user.email \"you@example.com\"git config --global user.name \"Your Name\"# 查看用户名和密码设置命令,不写最后的文本值,即为查看 给本地代码添加远程git仓库1234567891011# 添加远程仓库 origin是git默认仓库名称git remote add origin 远程仓库地址# 重设远程仓库git remote set-url origin URL# 查看远程仓库的地址 git remote -v# 第一次推送到远程仓库（并把默认远程仓库设置为origin）git push -u origin master 修改commit注释12# 如果commit内容还未push:git commit --amend 可视化交互git学习，知识点如下 创建和切换分支1234567# 创建分支（当前分支不变）git branch newxxx# 切换当前分支到xxxgit checkout xxx# 创建并切换到newxxx分支git checkout -b newxxx rebase1234567# 当前在bugFix分支，命令会把bugFix节点的父节点指向master# C1--&gt;C2（master） 命令结果：C1--&gt;C2(master)--&gt;C3‘(bugFix*)# --&gt;C3（bugFix*） --&gt;C3git rebase master# 当前C2(master)--&gt;C3‘(bugFix*) 则指向同一个git rebase bugFix HEAD 123456789101112131415161718HEAD 是指git当前正在操作的节点指针HEAD 可以指向某个分支名，也可以指向某个节点名# 查看当前HEAD cat .git/HEAD# 查看HEAD指向的引用git symbolic-ref HEAD# 查看提交树的节点的哈希值git log# 切换HEAD指针（绝对值切换）git checkout 节点-哈希值/分支名# 切换HEAD指针（相对切换） # 使用 ^ 向上移动 1 个提交记录 git checkout master^ git checkout HEAD^ # 使用 ~&lt;num&gt; 向上移动多个提交记录，（不加数字则向上1个） git checkout master~3 修改分支位置123# -f 选项让分支指向另一个提交# 下面命令将 master 分支强制指向 HEAD 的第 3 级父提交。git branch -f master HEAD~3 恢复到merge之前123456# ORIG_HEAD 是git在做危险操作时候给HEAD做的备份git reset --hard ORIG_HEAD# ORIG_HEAD 等价于 HEAD@&#123;1&#125; # 直接恢复到对应版本, 本地修改/删除/新增的文件都会消失git reset --hard 版本的哈希id 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"git","slug":"编程基础/git","permalink":"http://yoursite.com/categories/编程基础/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"http://yoursite.com/tags/gitlab/"}]},{"title":"操作系统重装记录","slug":"编程基础/安装与配置/system-record","date":"2019-03-17T00:56:59.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/安装与配置/system-record/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/system-record/","excerpt":"","text":"警告 应该备份.ssh文件夹, 重装后将消失 Ubuntu 重装记录 chrome插件 SwitchyOmega 有道词典Chrome划词插件 LingoCloud （彩云小译） smartUp Gesture （手势） Tampermonkey （油猴） 有道云笔记网页剪报 360浏览器 插件360屏幕截图 Adblock Super Proxy SwitchyOmega Tampermonkey 彩云小译 截图助手 有道云笔记网页剪报 有道划词翻译 油猴脚本导出文件（tampermonkey-backup-chrome-2019-03-13T01-41-21.545Z.zip） Omega备份文件 adblock备份文件 3、把ubuntu的设置记录下来 安装Tweaks(Ubuntu软件商店–GNOME Tweaks) https://extensions.gnome.org/ aifish f1 插件 Activities configurator 1.3 Scale Icon 0 Icon Padding ON Hide Text 6 Text Padding 100 Hot Corner Threhold 100 Panel Transparency 黑色 Panel Shadow Color 100 Transparency 0 Vertical Length 0 Spread Radius OFF Move Activities to the Right ON Enable Conflict Detection Clipboard indicator Places status indicator Drop down terminal（没怎么用过） 3、常用软件记录 typora pycharm vscode 插件 Beautify Better TOML TOML Language Support Docker Encode Decode Gitlens Image preview JavaScript (ES6) code snippets Mithril Emmet Prettier - Code formatter Project Manager Python Terminal Vetur wps 搜狗输入法 百度云（deepin） postman meld（文本对比） systemMonitor（系统监控器，进程、文件夹监控） virtuaBox（虚拟机） 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"配置vim","slug":"编程基础/安装与配置/set-vim","date":"2019-03-17T00:56:04.000Z","updated":"2019-10-08T09:22:51.358Z","comments":true,"path":"wiki/编程基础/安装与配置/set-vim/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/set-vim/","excerpt":"","text":"vim配置（linux/manjaro/ubuntu/deepin通用）1234567891011121314151617181920212223242526272829303132333435363738sudo pacman -S vim\":关闭与vi的兼容模式set nocompatible \":显示行号set number \":显示匹配的括号set showmatch \":距离顶部和底部3行set scrolloff=3 \":编码set encoding=utf-8 set fenc=utf-8 \"编码设定Encodingset fileencoding=utf-8set fileencodings=utf-8,gbk,utf-16,big5 set langmenu=zh_CN.UTF-8source $VIMRUNTIME/delmenu.vimsource $VIMRUNTIME/menu.vimlanguage messages zh_CN.UTF-8\"忽略大小写检索set ignorecase\":搜索高亮set hlsearch \"输入检索时动态变化set incsearch\":语法高亮syntax on \":命令显示历史set history=500\"开启插件和缩进filetype plugin indent on\":鼠标set autoreadset mouse=set mousehide bug: backspace退格键不能使用把 deleteleft 快捷键设置会 backspace 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"配置和使用 pycharm","slug":"python/pycharm","date":"2019-03-17T00:55:57.000Z","updated":"2019-11-02T05:48:30.522Z","comments":true,"path":"wiki/python/pycharm/","link":"","permalink":"http://yoursite.com/wiki/python/pycharm/","excerpt":"","text":"配置常用代码片段配置 配置路径 Setting—&gt;Editor—&gt;Live Templates—&gt;加号按钮 12345678910111213141516171819~nos # noinspection PyMethodMayBeStatic# ~errerror = '\\n'.join(traceback.format_exception(*sys.exc_info()))# ~pathpath = os.path.join(os.path.dirname(__file__), '')# ~rootdef root(*f, relative_root='../../../'): # relative_root 当前代码目录相对root的相对路径 for t in f: if t[:1] == '/': print('Warning: root()包含绝对路径 参数=&#123;&#125;'.format(f)) break code_dir = os.path.dirname(os.path.realpath(__file__)) long_path = os.path.join(code_dir, relative_root, *f) return long_path 快捷键配置Keymap—方案设置成NetBeans 名称 快捷键 说明 Reformat Ctrl+Alt+L 需设置 Code格式化代码 Move Line Up Alt+Up 需设置 当前行往上挪一行 Move Line Down Alt+Down 需设置 当前行往上挪一行 Optimize Imports Ctrl+Shift+I 优化导入代码 Ctrl+F12 跳转到方法（列表） Ctrl + F11 设置书签 SHIFT F11 显示所有书签 Go to Bookmark 1 Ctrl+Alt+1 需设置 跳转到 该书签 Next Bookmark Ctrl+Shift+. 跳转到下个书签 Previous Bookmark Ctrl+Shift+逗号 跳转到上个书签 Decrease Font Size Ctrl+Alt+减号 减小字体大小 Increase Font Size Ctrl+Alt+= 增加字体大小 常用快捷键 把展开代码块，设置成递归的： 设置》Keymap》Main menu 》Code》Folding 》Expand 删除快捷键 设置》Keymap》Main menu 》Code》Folding 》Expand Recursively 增加快捷键 Ctrl+= 快捷键 作用 Ctrl+Q 查看当前函数有什么参数 Ctrl+Alt+L 格式化代码 Ctrl+Shift+加号/减号 展开/折叠所有代码块 Ctrl + 加号/减号 展开/折叠代码块（当前位置的：函数，注释等） Ctrl + Alt + E Show Useage 跳转到上个光标所在位置​ 打开 View—toolbar 有左右箭头按钮。悬停可以查看快捷键 插件File—Setting—Plugins 数据库插件： ​ 搜索datebase ​ 安装 Database Navigator ​ 安装Mongo Plugin ​ Grep Console 控制台文本颜色 默认logger的stream是输出到sys.stderr, 这是pycharm会把文字设置成红色. 如果设置stream到sys.stdout, 字体颜色就正常了 ​ 显示界面和字体配置12345678界面的字体大小：Setting--&gt;Appearance&amp;Behavior--&gt;Appearance--&gt;Use custom font代码的字体大小：Setting--&gt;Editor--&gt;Font控制台的字体大小:Setting--&gt;Editor--&gt;Color Scheme--&gt;Console Font 常见BUG更新了库以后跳转功能不一致(例如torch缺少torch.nn) 关闭pycharm 删除pycharm的提示缓存文件: /home/fish3/.PyCharmCE2019.2/system/python_stubs 然后再重新启动一次pycharm, 这时候pycharm会重新更新updating python interpreter pycharm跳转源码时会跳转到pyi文件首先在File-&gt;Setting-&gt;Editor-&gt;Code Style-&gt;File Types中找到Python Stub 然后将Registered Patterns里面的内容清空就好了。 pycharm中所有代码无法自动提示处理方法解决方案：1）打开pycharm后必须等待indexing全部结束；2）检查file菜单里，确保节能模式“power save mode”不被勾选。 某个文件不能代码高亮原因: 自动识别成了text文件(误操作添加了文件类型) 解决方案: 设置—&gt;Editor—&gt;File Types—&gt;找到文件名删除(一般在Text里面) 关闭PEP8某些错误提醒点击有错误提示的代码, 前面的下拉菜单, 选择忽略该错误, 会自动添加到 PEP8的忽略错误列表中 关闭错误检测提醒PEP的错误提示，鼠标到错误提示行开头位置，出现灯泡，选择ignore like this setting—&gt;Editor—&gt;inspections 说明 英文 捕捉异常范围太广 Too broad exception clauses 方法可能是静态的 Method may be static 拼写检测 Typo 关闭某些错误提醒让PyCharm 在代码检查时人为跳过某些特定部分的代码检查，便于强迫症和优化代码提示 使用方法：在需要跳过代码校验的部分加上注释即可， 如图： 注释 对应检查说明 # noinspection PyMethodMayBeStatic This inspection detects any methods which may safely be made static. # noinspection PyAbstractClass This inspection detects when not all abstract properties/methods are defined in a subclass # noinspection PyArgumentList This inspection reports discrepancies between declared parameters and actual arguments, as well as incorrect arguments (e.g. duplicate named arguments) and incorrect argument order. Decorators are analyzed, too. # noinspection PyArgumentEqualDefault This inspection highlights situations, where argument passed to function is equal to default parameter value # noinspection PyAssignmentToLoopOrWithParameter Checks for cases when you rewrite loop variable with inner loop for i in xrange(5): for i in xrange(20, 25): print(“Inner”, i) print(“Outer”, i)It also warns you if variable declared in with statement is redeclared inside of statement body: with open(“file”) as f: f.read() with open(“file”) as f: # noinspection PyAsyncCall This inspection highlights coroutines which were called without await # noinspection PyAugmentAssignment This inspection highlights assignment that can be replaced with augmented assignment. # noinspection PyAttributeOutsideInit This inspection detects instance attribute definition outside init method # noinspection PyBroadException This inspection highlights too broad exception clauses such as no exception class specified, or specified as ‘Exception’. # noinspection PyByteLiteral This inspection detects characters &gt; 255 in byte literals. # noinspection PyCallByClass This inspection checks for calls of a method by class while passing an instance of a different class as self parameter: foo = Foo() Bar.baz(foo, *more)Sometimes this may be intentional and correct. But when unintentional, this leads to subtle bugs. # noinspection PyCallingNonCallable This inspection highlights attempts to call objects which are not callable, like, for example, tuples. # noinspection PyChainedComparisons This inspection highlights chained comparisons that can be simplified. # noinspection PyClassHasNoInit This inspection used when a class has no init method, neither its parent classes. # noinspection PyClassicStyleClass This inspection detects classic style classes usage. # noinspection PyComparisonWithNone This inspection highlights comparisons with None. That type of comparisons should always be done with ‘is’ or ‘is not’, never the equality operators. # noinspection PyCompatibility Enable this inspection if you need your code to be compatible with a range of Python versions (for example, if you’re building a library). The range of Python versions with which the code needs to be compatible can be specified in the inspection settings. # noinspection PyDataclass This inspection detects invalid definitions and usages of classes created with dataclasses or attr modules. # noinspection PyDecorator This inspection reports usages of @classmethod or @staticmethod decorators on functions outside of a class. # noinspection PyDefaultArgument This inspection detects when a mutable value as list or dictionary is detected in a default value for an argument. Default argument values are evaluated only once at function definition time, which means that modifying the default value of the argument will affect all subsequent calls of the function. # noinspection PyDeprecation This inspection highlights usages of Python functions, classes or methods which are marked as deprecated (which raise a DeprecationWarning or a PendingDeprecationWarning). # noinspection PyDictCreation This inspection detects situations when dictionary creation could be rewritten with dictionary literal. # noinspection PyDictDuplicateKeys This inspection highlights using the same value as dictionary key twice. # noinspection PyDocstringTypes This inspection highlights types in docstring which don’t match dynamically inferred types. # noinspection PyDunderSlots This inspection detects invalid definition of slots in a class. # noinspection PyExceptClausesOrder This inspection highlights situations when except clauses are not in the correct order (from the more specific to the more generic) or one exception class is caught twice. If you don’t fix the order, some exceptions may not be catched by the most specific handler. # noinspection PyExceptionInherit This inspection detects when a custom exception class is raised but doesn’t inherit from the builtin “Exception” class. # noinspection PyFromFutureImport This inspection detects ‘from future import’ statements which are used not in the beginning of a file. # noinspection PyGlobalUndefined This inspection is used when a variable is defined through the “global” statement but the variable is not defined in the module scope. # noinspection PyInconsistentIndentation This inspection reports inconsistent indentation in Python source files (for example, use of a mixture of tabs and spaces). # noinspection PyIncorrectDocstring This inspection detects mismatched parameters in a docstring. Please note that it doesn’t warn you of missing parameters, if none of them is mentioned in a docstring. # noinspection PyInitNewSignature This inspection checks mutual compatibility of new and init signatures. # noinspection PyInterpreter This inspection notifies you if the current project has no Python interpreter configured or an invalid Python interpreter. # noinspection PyListCreation This inspection detects situations when list creation could be rewritten with list literal. # noinspection PyMandatoryEncoding This inspection detects lack of encoding magic comment for file. # noinspection PyMethodFirstArgAssignment This inspection detects cases when first parameter, such as ‘self’ or ‘cls’, is reassigned in a method. In most cases imaginable, there’s no point in such reassignment, and it indicates an error. # noinspection PyMethodOverriding This inspection detects inconsistencies in overriding method signatures. # noinspection PyMethodParameters This inspection looks for methods that lack a first parameter (which is usually named self ). # noinspection PyMissingConstructor This inspection warns if call to super constructor in class is missed # noinspection PyMissingOrEmptyDocstring This inspection detects lack of docstring and an empty docstring. # noinspection PyMissingTypeHints This inspection detects lack of type hints for function declaration in one of the two formats: parameter annotations or a type comment # noinspection PyNamedTuple This inspection detects invalid definition of namedtuple. # noinspection PyNestedDecorators This inspection looks for certain decorators that don’t nest well. # noinspection PyNonAsciiChar This inspection detects file contains non-ASCII characters and doesn’t have an encoding declaration at the top. # noinspection PyNoneFunctionAssignment This inspection is similar to pylint inspection E1111. It highlights situations when an assignment is done on a function call but the inferred function doesn’t return anything. # noinspection PyOldStyleClasses This inspection highlights occurrences of new-style class features in old-style classes. # noinspection PyOverloads This inspection validates overloads in regular Python files. # noinspection PyPackageRequirements This inspection warns about imported or required, but not installed packages. # noinspection PyPep8 This inspection runs the pep8.py tool to check for violations of the PEP 8 coding style guide. # noinspection PyPep8Naming This inspection checks the PEP8 naming conventions. # noinspection PyPropertyAccess This inspection checks that properties are accessed correctly: read-only not set, write-only not read, non-deletable not deleted. # noinspection PyPropertyDefinition This inspection checks that arguments to property() and functions annotated with @property and friends look reasonably. # noinspection PyProtectedMember This inspection warns if a protected member is accessed outside the class, a descendant of the class where it’s defined or a module. # noinspection PyProtocol This inspection detects invalid definitions and usages of protocols introduced in PEP-544. # noinspection PyRedeclaration This inspection detects unconditional redeclarations of names without being used in between, like this: def x(): passx = 2It applies to function and class declarations, and top-level assignments. # noinspection PyRedundantParentheses This inspection highlights redundant parentheses in statements. # noinspection PyReturnFromInit This inspection reports occurrences of return statements with a return value inside init methods of classes. A constructor should not return any value. # noinspection PySetFunctionToLiteral This inspection detects call for function “set” which can be replaced with set literal. # noinspection PyShadowingBuiltins This inspection detects shadowing built-in names, such as ‘len’ or ‘list’. # noinspection PyShadowingNames This inspection detects shadowing names defined in outer scopes # noinspection PySimplifyBooleanCheck This inspection detects equality comparison with a boolean literal. # noinspection PySingleQuotedDocstring This inspection highlights docstrings not using triple double-quoted string format. # noinspection PyStatementEffect This inspection detects statements without any effect. # noinspection PyStringException This inspection detects when a string exception is raised. # noinspection PyStringFormat This inspection detects errors in string formatting operations. # noinspection PySuperArguments This inspection check that in any call to super(A, B), B either is an instance of A or a subclass of A. # noinspection PyTestParametrized Test function, decorated with @pytest.mark.parametrize, must have arguments to accept parameters from decorator # noinspection PyTrailingSemicolon This inspection detects trailing semicolons in statements. # noinspection PyTupleAssignmentBalance This inspection check that the number of expressions on right-hand side and targets on left-hand side are the same. # noinspection PyTupleItemAssignment This inspection detects assignments to tuple item. # noinspection PyTypeChecker This inspection detects type errors in function call expressions. Due to dynamic dispatch and duck typing, this is possible in a limited but useful number of cases. Types of function parameters can be specified in docstrings or in Python 3 function annotations. # noinspection PyTypeHints This inspection detects invalid usages of type hints. # noinspection PyUnboundLocalVariable This inspection warns about local variables referenced before assignment. # noinspection PyUnnecessaryBackslash This inspection highlights backslashes in places where line continuation is implicit (inside (), [], {}). # noinspection PyUnreachableCode This inspection detects code which can not be normally reached. # noinspection PyUnresolvedReferences This inspection detects names that should resolve but don’t. Due to dynamic dispatch and duck typing, this is possible in a limited but useful number of cases. Top-level and class-level items are supported better than instance items. # noinspection PyUnusedLocal This inspection highlights local variables,parameters or local functions unused in scope. 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[]},{"title":"安装ubuntu","slug":"编程基础/安装与配置/install-ubuntu","date":"2019-03-17T00:55:46.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/安装与配置/install-ubuntu/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/install-ubuntu/","excerpt":"","text":"12sudo systemctl start NetworkManager.servicesudo systemctl enable NetworkManager.service 零 安装kernel（修复界面卡顿问题）问题：4.18版本kernel的CPU核心显卡存在bug，会导致界面卡顿 解决：安装5.1版本kernel（不要安装最新的rc版本，稳定一些） 查看当前内核版本1234567891011# 查看当前内核版本uname -r&gt;&gt;&gt;4.18.0-20-generic# 搜索可用的内核apt-cache showpkg linux-headersapt-cache showpkg linux-image找到这两个命令里，版本号相同的最新版本# 示例：选择5.1.0版本sudo apt install linux-headers-5.1.0-050100-generic linux-image-unsigned-5.1.0-050100-generic --fix-missing 查看硬件12345# 查看内存 槽位sudo lshw -c Memory# 查看主板sudo dmidecode -t 2 一 安装系统被锁住时的操作12345678E: 无法获得锁 /var/lib/dpkg/lock-frontend - open (11: 资源暂时不可用)E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?# 杀掉apt-get进程ps -e|grep apt-get然后kill掉进程ID# 强制解锁sudo rm /var/cache/apt/archives/locksudo rm /var/lib/dpkg/lock 使Ubuntu支持exfat格式的优盘1sudo apt-get install exfat-utils 安装thefuck1234sudo apt install thefuckthefuck -a f # 获取alias文本， 别名=f（可以自己设定）把alias文本设置到.bashrc内 安装tldr （太长不看，查看linux命令行帮助信息的工具）123sudo pip install tldr# 查看tar的帮助信息tldr tar 安装docker123456789sudo apt install docker.io# 创建文件 /etc/docker/daemon.json 增加私有镜像仓库&#123; \"insecure-registries\" : [\"192.168.31.103:5000\"]&#125;sudo systemctl restart docker 支持exfat文件格式12# 因为版权问题, 不能默认支持, 需要安装如下软件: sudo apt-get install exfat-utils 制作启动盘12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 查看硬盘列表sudo fdisk -l# 结果依次显示每个硬盘的信息，可知 /dev/sdb就是优盘Disk /dev/nvme0n1：232.9 GiB，250059350016 字节，488397168 个扇区Disk model: Samsung SSD 970 EVO 250GB 单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：gpt磁盘标识符：C56964F0-A438-4609-8521-BB80B61A498F设备 起点 末尾 扇区 大小 类型/dev/nvme0n1p1 109258752 488392031 379133280 180.8G Linux 文件系统/dev/nvme0n1p2 4196352 4401151 204800 100M Linux 文件系统/dev/nvme0n1p3 4401152 109258751 104857600 50G Linux 文件系统/dev/nvme0n1p4 2048 616447 614400 300M EFI 系统/dev/nvme0n1p5 616448 4196351 3579904 1.7G Linux swap分区表记录没有按磁盘顺序。Disk /dev/sda：1.8 TiB，2000398934016 字节，3907029168 个扇区Disk model: WDC WD20EZAZ-00G单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 4096 字节I/O 大小(最小/最佳)：4096 字节 / 4096 字节磁盘标签类型：gpt磁盘标识符：B6164008-B283-4767-831C-8784AB8854F7设备 起点 末尾 扇区 大小 类型/dev/sda1 2048 835022847 835020800 398.2G Linux 文件系统/dev/sda2 835022848 3907024031 3072001184 1.4T Linux 文件系统Disk /dev/sdb：14.4 GiB，15483273216 字节，30240768 个扇区Disk model: DataTraveler 2.0单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0x3fbeba85设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdb1 * 0 4774783 4774784 2.3G 0 空/dev/sdb2 4774784 4929791 155008 75.7M 1 FAT12# 创建启动盘： if=镜像文件 of=优盘盘符路径sudo dd if=/home/fish/下载/deepinamd64.iso of=/dev/sdb 二 安装软件12345678910111213141516171819202122232425262728293031323334353637383940414243卸载软件命令： apt-get remove softname1 softname2 softname3……卸载并清除配置命令： apt-get remove --purge softname1更新软件信息数据库命令： apt-get update进行系统升级命令： apt-get upgrade搜索软件包命令： apt-cache search softname1 softname2 softname3……安装deb软件包命令： dpkg -i xxx.deb删除软件包命令： dpkg -r xxx.deb连同配置文件一起删除命令： dpkg -P xxx.deb (purge)查看软件包信息命令： dpkg -info xxx.deb查看文件拷贝详情命令： dpkg -L xxx.deb查看系统中已安装软件包信息命令： dpkg -l重新配置软件包命令： dpkg-reconfigure xxx 科学上网1234567# 安裝ss的命令行工具sudo apt install -y shadowsockssslocal -c xxx.json# 浏览器安装科学插件# 找到chrome执行程序目录，加代理启动，安装 SwitchyOmega插件./chrome --proxy-server='socks5://127.0.0.1:1080' 开机启动：打开“启动应用程序”添加命令-clink1234567891011121314151617181920212223242526272829303132333435363738394041424344其中命令和配置文件需要全局路径### 设置开机自启动的内容```bash# 建立 /etc/systemd/system/rc-local.service 文件内容如下：_________________________________________________[Unit]Description=/etc/rc.local CompatibilityConditionPathExists=/etc/rc.local [Service]Type=forkingExecStart=/etc/rc.local startTimeoutSec=0StandardOutput=ttyRemainAfterExit=yesSysVStartPriority=99 [Install]WantedBy=multi-user.target_________________________________________________# 创建启动命令脚步文件 /etc/rc.local——————————————————————————————————————————————————#!/bin/sh -eecho &quot;看到这行字，说明添加自启动脚本成功。&quot; &gt; /usr/local/test.logexit 0——————————————————————————————————————————————————# 添加权限sudo chmod +x /etc/rc.local# 开机启动服务sudo systemctl enable rc-local# 启动服务并检查状态sudo systemctl start rc-local.servicesudo systemctl status rc-local.service# 修改service配置文件后，重载sudo systemctl daemon-reload# 查看日志cat /usr/local/test.log 开启ssh服务 12345678sudo apt install openssh-serversudo systemctl restart sshd# 设置不允许root帐号登录 修改文件 /etc/ssh/sshd_config PermitRootLogin no# 重启sshd服务sudo systemctl restart sshdsudo systemctl enable sshd 安装git git-lfs12345678910# 安装gitsudo apt install git# 安装git-lfs ########### ubuntu## 1. 设置url源curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash## 2. 安装lfssudo apt-get install git-lfs## 3. 初始化git lfs install 安装google输入法(不好用)12sudo apt-get install fcitx-googlepinyin然后注销再登陆操作系统 安装搜狗输入法KDE界面很难配置输入法， 可以进入gnome界面安装好输入法，再重新进入KDE 1234567891011121314# 搜狗拼音依赖fcitxsudo apt-get install fcitx-bin fcitx-table1. 去搜狗拼音官网,下载linux版本安装文件(.deb)2. 双击打开界面安装3. 登出后登录操作系统！ 4. 右键点击顶栏的键盘图标，选择配置5. 添加搜狗输入法如果候选栏显示乱码、无法显示中文，可按如下方式处理：1. 如果是刚装完搜狗输入法,则输入命令:sudo apt-get install -f,进行依懒性检查,判断是否却是依赖项.2. 如果是之前一直可以输入中文的,突然无法输入,则不是依赖项的问题,此时输入命令:killall fcitx3.如果2仍不能解决问题,则删除配置文件,Ubuntu下搜狗的配置文件在~/.config下的3个文件夹里：SogouPY、SogouPY.users、sogou-qimpanel 删除这3个文件夹,然后重启搜狗即可. 选择需要的输入法： 点击Ubuntu右上角顶栏的小键盘图标中打开，配置，如下图 安装zsh 和 autojump12345678910111213141516# 如果是Ubuntu需要先安装（manjaro自带zsh）sudo apt install zshsh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"重启操作系统后终端变成zsh如果要切换回去bashchsh -s /bin/bash# 安裝autojumpgit clone git://github.com/wting/autojump.gitcd autojump./install.py or ./uninstall.py手工把提示腳本添加到 ~/.zshrc# 登出操作系统，再登录，打开shell即会进入zsh 安装WPS去wps官网下载deb文件，双击后安装。 然后下载font文件：链接如下 国内地址 国外地址 1234567#a. 将得到文件复制到/usr/share/fontssudo cp * /usr/share/fonts#b. 执行以下命令,生成字体的索引信息sudo mkfontscalesudo mkfontdir#c. 更新字体缓存sudo fc-cache 安装node/npm1234567# 1. 安装默认版本sudo apt-get install npm# 2. 安装版本管理工具sudo npm install -g n# 3. 安装对应版本sudo n latest #最新版本sudo n stable #最新的稳定版本 安装ssh服务123456789101112131415161718192021222324sudo apt install openssh-serversudo systemctl start sshdsudo vim /etc/ssh/sshd_config # 关闭root用户登录PermitRootLogin nosudo systemctl restart sshdwangxiaoyu@dc5:~$ ssh xxxxx@192.168.31.68@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the ECDSA key sent by the remote host isSHA256:yHxBrXIZ6upZaOPDf3PcaOO+aaEthNQiw4O4CCt7gSM.Please contact your system administrator.Add correct host key in /home/wangxiaoyu/.ssh/known_hosts to get rid of this message.Offending ECDSA key in /home/wangxiaoyu/.ssh/known_hosts:14 remove with: ssh-keygen -f \"/home/usr1/.ssh/known_hosts\" -R 192.168.31.68 目标ssh服务器重装系统后，远程登录时会报错。使用报错信息里的remove with 后面的命令即可清除报错ssh-keygen -f \"/home/usr1/.ssh/known_hosts\" -R 192.168.31.68 安装postman123456789101112# 创建软链接sudo ln -s /home/fish3/soft/Postman/Postman /usr/bin/postman# 创建 /usr/share/applications/postman.desktop 内容如下:[Desktop Entry]Encoding=UTF-8Name=postmanExec=postmanIcon=/home/fish3/soft/Postman/app/resources/app/assets/icon.pngTerminal=falseType=ApplicationCategories=Development; 安装chrome12345678910111213sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -sudo apt updatesudo apt install google-chrome-stable# 离线安装包下载## 1. 点击链接 (手工添加的standalone是离线下载参数)https://www.google.cn/chrome/?standalone=1https://www.google.cn/chrome/?standalone=1&amp;platform=win64## 2. 点击接受并安装, 下载deb文件 安装deepin的百度云、截图和终端123456# 百度云 安装失败！https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu# 截图、深度终端在ubuntu软件上级搜索 dee 安装和挑选终端123sudo apt install xfce4-terminalterminator #gnomekonsole 配置Gnome界面123456sudo apt install chrome-gnome-shellsudo apt install gnome-tweak-tool打开Ubuntu软件商店安装：GNOME Tweakshttps://extensions.gnome.org/ 账号aifish f1使用火狐浏览器安装插件 使用chrome打开： 使用火狐打开： 点击安装插件 Click here to install browser extension 安装gnome插件User Themes Clipboard Indicator topbar剪贴板列表 Recent Items topbar最近文件夹 Places Status Indicator topbar文件夹收藏 Datetime Format topbar显示日期 配置格式：%b-%d %A %R 快捷键设置 快捷键 配置路径 说明 Super+E 启动器—主目录 文件管理器 Super+D 导航—隐藏所有正常窗口 回到桌面 Super+W 启动网页浏览器 设置开机时自动登录设置—详细信息—用户—自动登录 alias配置123alias get1='pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ 'alias get2='pip install -i http://mirrors.aliyun.com/pypi/simple/ 'alias get3='pip install -i https://pypi.douban.com/simple/ ' KDE界面安装1sudo apt install plasma-desktop 任务栏设置删除“程序启动器”， 添加“应用程序菜单”（有搜索框） 快捷键显示桌面默认是Ctrl+F12 改成Win+D 系统设置—&gt;快捷键—&gt;全局快捷键—&gt;Plasma—&gt;显示桌面 锁屏默认是Ctrl+Alt+L 改为Win+L 系统设置—&gt;快捷键—&gt;全局快捷键—&gt;ksmserver—&gt;锁定会话 开机自启动系统设置－－－＞工作空间－－－＞开机和关机－－－＞自动启动 （勾选：在终端中运行） 界面设置12345678# 关闭: 自动锁屏时间系统设置---&gt;工作空间---&gt;桌面行为---&gt;锁屏(快捷键)# 快捷键系统设置---&gt;工作空间---&gt;快捷键# 鼠标双击打开文件系统设置---&gt;硬件---&gt;输入设备---&gt;鼠标---&gt;双击打开# 锁屏壁纸系统设置---&gt;桌面行为---&gt;锁屏---&gt;外观---&gt;添加图片 bug: 没有无线网络选择图标12开机自启动增加:nm-applet 程序图标： nm-applet是network-manager的一个桌面组件 先要保证网络管理使用的是network-manager 1234567# /etc/NetworkManager/NetworkManager.conf[ifupdown]managed=true# 然后开启network-manager# sudo service network-manager stopsudo service network-manager start 确认任务栏组件添加了: “系统托盘”(访问在系统托盘中隐藏的桌面小程序) bug: 无法打开wps表格文件12345# 写入文件: ~/.xprofileexport LC_ALL=zh_CN.UTF-8export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=\"@im=fcitx\" bug: 文件管理器里文件不能剪贴和复制nautilus –new-window 管理器可以剪贴 PCManFMC 管理器不能剪贴 所有设置—&gt;应用程序—&gt;默认程序—&gt;文件管理器—&gt;其他 xfce界面安装123sudo apt-get install xfce4 注销当前账号在输入密码界面，选择不同的界面系统 Xfce 触摸板启用“点击”|apt安装软件失败的处理方法12345678910111213141516171819# 触摸板启用\"点击\"# 1.首先，保证安装了synaptics驱动：sudo apt install xserver-xorg-input-synaptics-hwe-18.04 # 因为是ubuntu18.04所以添加后缀\"-hwe-18.04\"编辑 /usr/share/x11/xorg.conf.d/*synaptics-quirks.conf 增加# Disable generic Synaptics device, as we're using# \"DLL0704:01 06CB:76AE Touchpad\"# Having multiple touchpad devices running confuses syndaemonSection \"InputClass\"Identifier \"SynPS/2 Synaptics TouchPad\"MatchProduct \"SynPS/2 Synaptics TouchPad\"MatchIsTouchpad \"on\"MatchOS \"Linux\"MatchDevicePath \"/dev/input/event*\"Option \"Ignore\" \"on\"EndSection# 然后重启机器 https://www.dell.com/support/article/us/en/04/sln308258/precision-xps-ubuntu-general-touchpad-mouse-issue-fix?lang=en LXDE界面致命缺陷: 不能设置屏幕缩放, 对于高分辨率屏幕太不方便 安装 12345678LXDE有很多定制版，可任选# Lubuntu定制LXDE和vanilla LXDE（实际安装不成功，缺少依赖）# sudo apt-get install lubuntu-desktop# 香草LXDEsudo apt-get install lxde# 选择使用lightdm（见图） （gdm3 快捷键报错cannot configure keys remotely）如果设置错了也可以切换lightdmsudo dpkg-reconfigure lightdm # sudo dpkg-reconfigure gdm3 设置数字时钟格式：%b-%d %A %R设置快捷键：菜单—首选项—set hot key 功能 快捷键 说明 文件管理器 Win+E 默认有 nautilus –new-window 显示桌面 Win+D 默认有 锁屏 Win+L lxlock 打开浏览器 Win+W browser360 或 /usr/bin/google-chrome-stable 打开终端 Win+T lxterminal 或 deepin-terminal 或 gnome-terminal vscode Win+V code pycharm Win+P 打开pycharm–Tools–Create CMD line typora Win+M typora 截图 Ctrl+Shift+Print deepin-screenshot 三. 特定问题解答没有声音123456789 sudo apt-get remove --purge alsa-base pulseaudiosudo apt-get install alsa-base pulseaudio pavucontrolsudo alsa force-reload然后在命令行打开\"pavucontrol\"在界面中点击配置: 切换到“配置”选项卡，根据实际情况禁用不需要的声卡。禁止第一项，第二项选择analogy stereo output（模拟立体输出。推荐选择）或者analogy stereo duplex（模拟立体声双工）reboot 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"安装manjaro","slug":"编程基础/安装与配置/install-manjaro","date":"2019-03-17T00:55:36.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/安装与配置/install-manjaro/","link":"","permalink":"http://yoursite.com/wiki/编程基础/安装与配置/install-manjaro/","excerpt":"","text":"零. 个人使用体验只使用了一周，还不太熟悉。再尝试一周，如果没有什么特别的优点就换成deepin试试。 优点 硬件支持好 软件版本非常新（滚动更新） 缺点 缺少软件360浏览器 对中文支持较差，需要自己配置的内容较多 很多软件都是deepin的，例如微信和naivicat（那我为什么不直接用deepin呢） 一. 安装manjaro系统二. 中文相关配置注意事项如果代码更新一直等待状态可以尝试删除/var/lib/pacman/db.lck 配置镜像源 测试国内的镜像源1sudo pacman-mirrors -i -c China -m rank 设置 archlinuxcn 源1234567891011# 修改 /etc/pacman.conf 添加以下内容[archlinuxcn]SigLevel = Optional TrustedOnlyServer = http://repo.archlinuxcn.org/$arch## 添加cn源签名key(这步不做，会报签名错误）sudo pacman -S archlinuxcn-keyring# 完成后执行下面的命令使配置生效## 更新源列表sudo pacman-mirrors -g## 更新pacman数据库并全面更新系统sudo pacman -Syyu #（必须先更新系统，不然无法安装输入法） 使界面可以输入中文123456在~/.xprofile中添加export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=@im=fcitx如果还是无法使用，可能需要安装fcitx-gtk2sudo pacman -S fcitx-gtk2 把系统界面设置为中文1234点击设置--Manjaro Settings Manager--本地化设置：添加 中国-中文，然后在语言包中点击安装软件包在~/.xprofile中添加export LC_ALL=\"zh_CN.UTF-8\"export LANG=zh_CN.UTF-8 三. 安装软件和系统配置安装yay1234# 1. 下载代码git clone https://aur.archlinux.org/yay.gitcd yaymakepkg -si 安装git12sudo pacman -S gitsudo pacman -S git-lfs 安装 wps12sudo pacman -S wps-officesudo pacman -S ttf-wps-fonts 安装 typora1234567891011121314## 下载二进制文件（x64） 解压https://www.typora.io/#linux## 创建程序的软链接sudo ln -s /home/fish/soft/Typora-linux-x64/Typora /usr/bin/typora## 编辑/usr/share/applications/typora.desktop 文件[Desktop Entry]Version=1.0Terminal=falseIcon=/home/aifish2/soft/typora/Typora-linux-x64/resources/app/asserts/icon/icon_256x256.pngType=ApplicationCategories=Office;Exec=/usr/bin/typora %UName=TyporaComment=MarkDown Editor 安装node/npm1234sudo pacman -S nvm nvm install --latest-npm然后把/home/fish/.nvm/versions/node/v11.12.0/bin添加到系统路径 安装jdk123456sudo pacman -S jdk8# 配置环境export JAVA_HOME=/usr/lib/jvm/defaultexport JRE_HOME=$&#123;JAVA_HOEM&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib 命令简单安装： vscode/ vim/ 微信/ shadownsocks/ 网易云音乐/ qq/ chrome/docker12345678sudo pacman -S codesudo pacman -S electronic-wechat （网页版不能复制粘帖图片）sudo pacman -S shadownsocks-qt5sudo pacman -S vimsudo pacman -S netease-cloud-music # 网易云音乐sudo pacman -S deepin.com.qq.office # 可以先搜索qq 看看版本sudo pacman -S google-chromesudo pacman -S docker 安装下载工具Gwget12打开系统工具：添加/删除软件搜索工具后下载 百度云下载1234567打开系统工具：添加/删除软件：安装baidupcs-go-git说明：https://github.com/iikira/BaiduPCS-Go# 进入命令交互工具baidupcs# 帮助help 安装截图工具123sudo pacman -S deepin-screenshot添加快捷键： deepin-screenshot 安装zsh autojump1234567# zshsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"# autojump（zsh设置ubuntu通用）sudo pacman -S autojump再在~/.zshrc中添加: plugins=(git autojump) 安装googlepinyin谷歌拼音123456sudo pacman -S fcitx-im fcitx-configtool fcitx-googlepinyinsudo vim ~/.xprofile 输入下面命令：exportGTK_IM_MODULE=fcitxexportQT_IM_MODULE=fcitxexportXMODIFIERS=\"@im=fcitx\" 安装搜狗拼音12345678910# 前置：设置中国软件源sudo pacman -S fcitx-im #默认全部安装sudo pacman -S fcitx-configtoolsudo pacman -S fcitx-sogoupinyin在~/.xprofile添加如下内容：export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=@im=fcitx 关闭ssh远程root登录1234# 设置不允许root帐号登录 修改文件 /etc/ssh/sshd_config PermitRootLogin no# 重启sshd服务sudo systemctl restart sshd 安装pycharm（ubuntu通用）12341.下载并解压pycharm.tar文件2.进入bin文件夹3.执行./pycharm.sh4.打开pycharm，在菜单栏选择tool---&gt;添加桌面快捷方式 时间显示设置操作：右键点击任务栏右下角的时间，选择属性：tips配置：%m-%d %j/365 第%V周时钟配置：周%u %H:%M ### 切换deepin桌面12# 安装桌面，然后重启电脑，选择桌面sudo pacman -S deepin deepin-extra lightdm 参考arch-wiki chrome安装插件 去官网下载插件文件 xxx.crx 去http://crxextractor.com/ 网站上传crx文件，获得zip文件 解压zip文件获得一个文件夹 打开chrome，打开开发者模式，加载已解压的扩展程序，选中解压文件夹，安装即可 四. 常用快捷键Ctrl+Alt+D 回到桌面（在设置界面里看不见这个快捷键，但是超级方便） Ctrl+Alt+F exo-open –launch FileManager 打开文件管理器 我改成了 Super ECtrl+Alt+M xfce4-taskmanager 资源监控器Ctrl+Alt+Delete xflock4 锁屏并黑屏Ctrl+Alt+X xkill 通过鼠标关闭某个程序新增： Ctrl+Alt+Q xfce4-terminal 终端Ctrl+H 文件管理器，显示隐藏文件 参考资料","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"安装与配置","slug":"编程基础/安装与配置","permalink":"http://yoursite.com/categories/编程基础/安装与配置/"}],"tags":[]},{"title":"py2_and_py3","slug":"python/py2和py3","date":"2019-03-17T00:39:15.000Z","updated":"2019-10-08T09:22:51.338Z","comments":true,"path":"wiki/python/py2和py3/","link":"","permalink":"http://yoursite.com/wiki/python/py2和py3/","excerpt":"","text":"感受: 实际上还是不好用～～～能用3就用3～～～_future__python3出来的时候，python的设计者们当然也考虑过代码之间的兼容问题。许多为为兼容性设计的功能可以通过future这个包来导入。例如： 123456789101112# 使用python3的print函数，禁用python2的print语句。from __future__ import print_function# 导入该特征，代码中的文本变量默认是Unicode（如果不导入python2的文本变量默认是str）# python2 str.decode('utf8') --&gt; Unicodefrom __future__ import unicode_literals# 参见PEP 328 -- Imports: Multi-Line and Absolute/Relativefrom __future__ import absolute_import# 像python3一样，int除以int得float，而不像Python2那样是整除from __future__ import division six 字符串类型 文本 字节 python2 unicode str python3 str bytes six six.text_type six.binary_type 1234567891011# python2if isinstance(xxx, unicode): pass # 兼容python2和python3import sixif isinstance(xxx, six.text_type): pass## 使用input代替raw_inputfrom six.moves import input 判断版本写不同的内容if sys.version&gt;’3’: pass 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"兼容性","slug":"兼容性","permalink":"http://yoursite.com/tags/兼容性/"},{"name":"python版本","slug":"python版本","permalink":"http://yoursite.com/tags/python版本/"}]},{"title":"decorator装饰器","slug":"python/decorator装饰器","date":"2019-03-17T00:38:37.000Z","updated":"2019-10-08T09:22:51.338Z","comments":true,"path":"wiki/python/decorator装饰器/","link":"","permalink":"http://yoursite.com/wiki/python/decorator装饰器/","excerpt":"","text":"最简单的模板是这样的1234567891011def outer(func): def inner(): print 'before' func() print 'after' # return r return inner@outerdef F1(): print 'test' 函数带多个参数，装饰器对应修改以适合多种情况12345678910def ftfunc(func): def timef(*s,**gs): print \"[%s] %s() called\" % (ctime(),func.__name__) return func(*s,**gs) return timef@ftfuncdef foo(*s,**gs): print(s) print(gs) 函数带多个参数，装饰器也带多个参数123456789101112def decrator(*dargs, **dkargs): def wrapper(func): def _wrapper(*args, **kargs): print \"decrator param:\", dargs, dkargs print \"function param:\", args, kargs return func(*args, **kargs) return _wrapper return wrapper@decrator(1, a=2)def foo(x, y=0): print \"foo\", x, y 函数带多个参数，装饰器能转换参数类型1234567891011121314def validate(**vkargs): def decorator(func): def wrapper(**kargs): for key in vkargs: # 根据vkargs中的参数的类型对kargs的参数进行类型转换 kargs[key] = vkargs[key](kargs[key]) return func(**kargs) return wrapper return decorator@validate(x=int, y=float, z=float)def move(x, y, z): print \"move %d (%0.2f, %0.2f)\"%(x, y, z) 参考资料","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"decorator","slug":"decorator","permalink":"http://yoursite.com/tags/decorator/"},{"name":"装饰器","slug":"装饰器","permalink":"http://yoursite.com/tags/装饰器/"}]},{"title":"elastic_search基础","slug":"人工智能/搜索引擎/elastic-search基础","date":"2019-03-05T02:39:39.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/elastic-search基础/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/elastic-search基础/","excerpt":"","text":"Document元数据（MetaData）元数据用于标注文档的相关信息： _index：文档所在的索引名_type：文档所在的类型名_id：文档的唯一id_uid：组合uid，由_type和_id组成（6.x中_type不再起作用，同_id一样）_source：文档的原始json数据，可以从这里获取每个字段的内容_all：整合所有字段内容到该字段，默认禁用 搜索结果超出1万条的报错如果数据量小于20万，那么简单方法是修改max_result_window 1234# PUT方法，$开头的是变量PUT $ip:$port/$index/_settingsbody = &#123; \"index\" : &#123; \"max_result_window\" : 200000&#125;&#125;' 异步调用, 报错 HTTP 406: Not Acceptable需要在post的时候指定headers 1234567891011121314import jsonfrom tornado.httpclient import AsyncHTTPClientclient = AsyncHTTPClient()async def search(body, explain=False): if explain: body['explain'] = True headers = &#123; 'Content-Type': 'application/json; charset=UTF-8', 'Accept': 'application/json', &#125; url=http://192.168.31.192:33303/oonp_report_case.cases/_search resp = await client.fetch(url, method='POST', headers=headers, body=json.dumps(body)) resp = json.loads(resp.body.decode()) return resp Mappings创建索引时，设定mappings1234567891011121314151617181920PUT $host:$port/$indexbody = &#123; \"settings\": &#123; \"number_of_shards\": 1, //分片节点数量 \"number_of_replicas\": 0, //复制节点数量 &#125;, \"mappings\": &#123; \"_doc\": &#123; \"properties\": &#123; \"txt\": &#123; \"type\": \"text\", \"analyzer\": \"ik_smart\" // ik-分词器 &#125; &#125; &#125; &#125;&#125;analyzer可选: \"standard\", \"ik_smart\", \"english\"如果body=&#123;&#125; 那么就使用ES的自动mappings 字段的数据类型： 简单类型：text, keyword, date, long, double, boolean or ip. json层级类型：object or nested. 专有类型：geo_point, geo_shape, or completion. ## 查看全局的信息查看ES版本1GET $host:$port/ 查看ES安装的插件列表1GET $host:$port/_cat/plugins # 返回的文本（不是json） 搜索操作搜索所有文档12345678# 某索引的某个类型的所有文档GET $host:$port/$index/$type/_search# 某索引的所有文档GET $host:$port/$index/_search# ES所有文档GET $host:$port/_search 搜索符合条件的文档 (并排序)1234567891011121314# 某索引的某个类型的所有文档POST $host:$port/$index/$type/_searchbody = &#123; \"query\": query \"from\": 0, // 从0开始 \"size\": 20, // 取20个 \"sort\": [ // 排序 &#123;\"age\": &#123;\"order\": \"asc\"&#125;&#125; //asc升序 dsc降序 ] \"_source\" : [\"key1\", \"key2\"], // 返回信息包含的键 \"highlight\": &#123;'fields': &#123;'text': &#123;\"number_of_fragments\": 0&#125;&#125;&#125;, # number_of_fragments默认是5, 会返回5段带高亮的文本, 如果设置成0就会返回全文&#125;query = &#123;\"match_all\": &#123;&#125;&#125; // 搜索全部 搜索参数12345GET /_search&#123; \"min_score\": 0.5, // 分数最小值, 查询结果必须有 _score 字段 \"query\" : query,&#125; 基础查询种类数值符合范围12345678910111213141516171819query = &#123; \"range\": &#123; \"字段名\": &#123; # 符合则score=1 否则=0 \"gte\": 20000, // gte是&gt;= gt是&gt; \"lte\": 30000, // lte是&lt;= lt是&lt; \"boost\": 1.5 // 分数权重 &#125; &#125; &#125;query = &#123; \"range\": &#123; \"字段名\": &#123; \"from\": 20000, \"to\": 30000, \"boost\": 1.5 // 分数权重 &#125; &#125;&#125; 词项匹配（不进行分词处理）12345678910111213141516171819202122query = &#123; \"term\": &#123; \"_id\": \"ZUd6zmoBr51spxZUlcFQ\", &#125;&#125;query = &#123; \"term\": &#123; \"_id\": &#123; \"value\": \"ZUd6zmoBr51spxZUlcFQ\", \"boost\": 2 &#125; &#125;&#125;# terms是筛选，输出分数=1，不管匹配了多少个词项# 如果希望有分数，则用 should 拼接 termquery = &#123; \"terms\": &#123; \"featrue\": [\"盗窃\", \"自首\"] //或的关系 &#125;&#125; 短语匹配（先分词，再查询分词结果和位置顺序都对的文档）1234567891011121314query = &#123; \"match\": &#123; \"txt\": \"关键词\" &#125;&#125;query = &#123; \"match\": &#123; \"txt\": &#123; \"query\":\"我是中国人\", \"boost\": 2.0 &#125; &#125;&#125; 前缀匹配1234567query = &#123; \"match_phase_prefix\": &#123; \"name\": &#123; \"query\": \"赵\" &#125; &#125;&#125; 复合查询nested查询（数组元素是对象，查询符合条件的对象）1234567891011121314151617query = &#123; \"nested\":&#123; \"path\": \"字段1\", \"query\": &#123; // 可以是任意query \"match\": &#123; \"字段1.字段2\": \"关键词\" //注意key需要有前置路径 &#125; &#125; &#125;&#125;# 文档结构是：&#123;\"字段1\": // nested对象 [ &#123;\"字段2\"： \"文本内容\"&#125; ] &#125; bool查询12345678910111213141516query = &#123; \"bool\" : &#123; \"must\" : [ # 与 各个查询的分数相加 &#123;\"term\": &#123;\"price\": 25&#125; &#125; // query ], \"should\" : [ # 或 各个查询的分数相加 &#123;\"term\": &#123;\"price\": 25&#125; &#125; // query ], \"must_not\" : [ # 非 &#123;\"term\": &#123;\"price\": 25&#125; &#125; // query ], \"filter\" : [filter], \"minimum_should_match\" : 1, \"boost\" : 1.0, &#125;&#125; filter12345filter = &#123; \"exists\": &#123; # 存在字段 \"filed\": \"price\" &#125;&#125; boosting 查询（soft not)123456789# 不复合negative查询的文档的得分不变# 复合negative查询的文档的得分会乘以negative_boost&#123; \"boosting\":&#123; \"positive\": p_query, \"negative\": n_query, \"negative_boost\": 0.2 &#125;&#125; 排序功能说明 123456789101112131415161718192021222324252627# score_mode 对数组的数值进行融合后排序: # min最小值, max最大值, sum求和, avg平均值, median中位数# multiply相乘, firstPOST /_search&#123; \"query\": &#123; \"function_score\": &#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;, \"boost\": \"5\", \"functions\": [ &#123; \"filter\": &#123; \"match\": &#123; \"test\": \"bar\" &#125; &#125;, \"random_score\": &#123;&#125;, \"weight\": 23 &#125;, &#123; \"filter\": &#123; \"match\": &#123; \"test\": \"cat\" &#125; &#125;, \"weight\": 42 &#125; ], \"max_boost\": 42, //计算出分数的上限 \"score_mode\": \"max\", \"boost_mode\": \"multiply\", \"min_score\" : 42 //最小分数, 用于排除得分过低的文档 &#125; &#125;&#125; function_score衰减函数 越近越好 linear线性 、 exp指数 和 gauss高斯函数 它们可以操作数值、时间以及经纬度地理坐标点这样的字段。 12345678910origin原点* 或字段可能的最佳值，落在原点 `origin` 上的文档评分 `_score` 为满分 `1.0` scale衰减率，从原点origin下落时，评分 _score 改变的速度。（例如，每 £10 欧元或每 100 米）decay从原点 origin 衰减到 scale 所得的评分 _score ，默认值为 0.5 。offset以原点 origin 为中心点，为其设置一个非零的偏移量 offset 覆盖一个范围，而不只是单个原点。在范围 -offset &lt;= origin &lt;= +offset 内的所有评分 _score 都是 1.0 。 12345678910111213141516171819202122232425&#123; \"query\": &#123; \"function_score\": &#123; \"gauss\": &#123; \"date\": &#123; \"origin\": \"2013-09-17\", // 如果不定义原点，则使用当前时间。 \"scale\": \"10d\", \"offset\": \"5d\", \"decay\" : 0.5 &#125; &#125;, 'min_score': 0.1 // 筛选条件: 分数最小值 'boost': 3.0 &#125; &#125;&#125;\"gauss\": &#123; \"字段名\": &#123; // 字段必须是数字/日期/地理位置 \"origin\": \"11, 12\", \"scale\": \"2km\", \"offset\": \"0km\", \"decay\": 0.33, &#125;&#125; function-score-参考文档 聚合查询某个字段的所有值 12345678GET /_search&#123; \"aggs\" : &#123; \"genres\" : &#123; \"terms\" : &#123; \"field\" : \"genre\" &#125; &#125; &#125;&#125; 数据查询数据删除数据123456789101112131415161718192021222324252627282930# es参考版本：elasticsearch：5.5# _delete_by_query会删除所有query语句匹配上的文档，用法如下：curl -X POST \"localhost:9200/twitter/_delete_by_query\" -H 'Content-Type: application/json' -d'&#123; \"query\": &#123; \"match\": &#123; \"name\": \"测试删除\" &#125; &#125;&#125;# 其中twitter是索引名称# 因为internal版本控制不支持0为有效数字，所以版本号为0的文档不能删除，并且请求将会失败。# 删除多个索引(twitter,blog)的多个type(_docs,post)curl -X POST \"localhost:9200/twitter,blog/_docs,post/_delete_by_query\" -H 'Content-Type: application/json' -d'&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;&#125;# 出自上向下删除1000条数据curl -X POST \"localhost:9200/twitter/_delete_by_query?scroll_size=1000\" -H 'Content-Type: application/json' -d'&#123; \"query\": &#123; \"term\": &#123; \"user\": \"kimchy\" &#125; &#125;&#125; 中文分词1234567测试ik_max_word&#123;“text”:“中华人民共和国人民大会堂”,“analyzer”:“ik_max_word” &#125;测试ik_smart&#123;“text”:“中华人民共和国人民大会堂”,“analyzer”:“ik_smart” &#125;两种分词器使用的最佳实践是：索引时用ik_max_word，在搜索时用ik_smart。即：索引时最大化的将文章内容分词（会把所有词汇切分到最短词），搜索时更精确的搜索到想要的结果。 参考资料 Elasticsearch删除数据之_delete_by_query","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[{"name":"es","slug":"es","permalink":"http://yoursite.com/tags/es/"},{"name":"elastic_search","slug":"elastic-search","permalink":"http://yoursite.com/tags/elastic-search/"},{"name":"elastic","slug":"elastic","permalink":"http://yoursite.com/tags/elastic/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://yoursite.com/tags/elasticsearch/"}]},{"title":"hexo","slug":"编程基础/博客工具/hexo","date":"2019-02-28T08:49:08.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/博客工具/hexo/","link":"","permalink":"http://yoursite.com/wiki/编程基础/博客工具/hexo/","excerpt":"","text":"安装hexo1sudo npm install hexo-cli -g 图片显示123456789101112# 配置资源文件夹# hexo n xxx 会生成同名文件夹,# hexo g 会把同名文件夹内图片打包生成静态文件_config.yml里的post_asset_folder，改成true# 安装插件npm install hexo-asset-image --save# 设置typora编辑器# 图片插入路径= ./$&#123;filename&#125;# 优先使用相对路径在typora编辑器内粘贴图片时会自动把图片存储到同名文件夹 主题配置hexo-theme-Wikitten Github地址 部署hexo123456789# 进入blog目录hexo init# 启动服务器hexo serverhexo server -p 8080 # -p 端口# 生成静态文件hexo generate | hexo g` 插件自动生存目录树 categorieshexo-auto-category 1234567891011# 安装npm install hexo-auto-category --save# 在站点根目录下的_config.yml添加：# 自动生成目录树categories (depth层级上限)auto_category: enable: true depth: # 使用hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 绑定域名1234A (Address) 记录是域名到ip的映射，即为ip起别名CNAME是域名别名到域名的映射，即为域名起别名。政策导致国内无法绑定到github~~ 参考资料 Hexo 的个人 Wiki 主题 - Wikitten hexo-theme-Wikitten github地址 hexo中文官方文档","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"博客工具","slug":"编程基础/博客工具","permalink":"http://yoursite.com/categories/编程基础/博客工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"}]},{"title":"hexo_主题配置","slug":"编程基础/博客工具/hexo_fish主题","date":"2019-02-28T08:49:08.000Z","updated":"2019-10-23T08:41:32.422Z","comments":true,"path":"wiki/编程基础/博客工具/hexo_fish主题/","link":"","permalink":"http://yoursite.com/wiki/编程基础/博客工具/hexo_fish主题/","excerpt":"","text":"下载主题http://theme.typora.io/theme/Law/ 下载后把文件夹和.css文件拖动到主题目录中即可 配置主题 点击”文件”—&gt;”偏好设置”—&gt;”打开主题目录” 新建一个文件”fish_whitey.css” 内容如下 该文件是在whitey.css基础上修改的 修改内容有添加注释”/* fish修改” 一项是文字内容显示的最大宽度改成了2700, 便于看到更多的文字内容 另一项是勾选项的修改从根号改成了圆圈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355html &#123; font-size: 19px;&#125;html, body &#123; margin: auto; background: #fefefe;&#125;body &#123; font-family: \"Vollkorn\", Palatino, Times; color: #333; line-height: 1.4; text-align: justify;&#125;#write &#123; max-width: 2700px; /* fish修改: 页面整体尺寸 */ margin: 0 auto; margin-bottom: 2em; line-height: 1.53; padding-top: 40px;&#125;/* fish 修改: 勾选项外观 ---------------------------------------- */.task-list&#123; padding-left: 0;&#125;.md-task-list-item &#123; padding-left:34px;&#125;.md-task-list-item &gt; input&#123; width: 1.25rem; height: 1.25rem; display: block; -webkit-appearance: initial; top: -0.4rem; /* 圆圈的高度 */ margin-left: -1.6em; margin-top: calc(1rem - 7px); border: none;&#125;.md-task-list-item &gt; input:focus&#123; outline: none; box-shadow: none;&#125;.md-task-list-item &gt; input:before&#123; border: 1px solid #555; border-radius: 1.2rem; /* 圆圈的大小 */ width: 1.2rem; height: 1.2rem; background: #fff; content: ' '; transition: background-color 200ms ease-in-out; display: block;&#125;.md-task-list-item &gt; input:checked:before,.md-task-list-item &gt; input[checked]:before&#123; background: #333; border-width: 2px; display:inline-block; transition: background-color 200ms ease-in-out;&#125;.md-task-list-item &gt; input:checked:after,.md-task-list-item &gt; input[checked]:after &#123; opacity: 1;&#125; .md-task-list-item &gt; input:after &#123; opacity: 1; -webkit-transition: opacity 0.05s ease-in-out; -moz-transition: opacity 0.05s ease-in-out; transition: opacity 0.05s ease-in-out; -webkit-transform: rotate(-45deg); -moz-transform: rotate(-45deg); transform: rotate(-45deg); position: absolute; top: 0.27rem; /* 对号的位置 0.27rem */ left: 0.19rem; /* 0.19rem */ width: 0.8rem; /* 对号的大小 */ height: 0.5rem; border: 3px solid #fff; border-top: 0; border-right: 0; content: ' '; opacity: 0;&#125; /* fish 修改: 勾选项外观 - [结束] --------------------------------------- *//* Typography-------------------------------------------------------- */#write&gt;h1:first-child,h1 &#123; margin-top: 1.6em; font-weight: normal;&#125;h1 &#123; font-size:3em;&#125;h2 &#123; margin-top:2em; font-weight: normal;&#125;h3 &#123; font-weight: normal; font-style: italic; margin-top: 3em;&#125;h1, h2, h3&#123; text-align: center;&#125;h2:after&#123; border-bottom: 1px solid #2f2f2f; content: ''; width: 100px; display: block; margin: 0 auto; height: 1px;&#125;h1+h2, h2+h3 &#123; margin-top: 0.83em;&#125;p,.mathjax-block &#123; margin-top: 0; -webkit-hypens: auto; -moz-hypens: auto; hyphens: auto;&#125;ul &#123; list-style: square; padding-left: 1.2em;&#125;ol &#123; padding-left: 1.2em;&#125;blockquote &#123; margin-left: 1em; padding-left: 1em; border-left: 1px solid #ddd;&#125;code,pre &#123; font-family: \"Consolas\", \"Menlo\", \"Monaco\", monospace, serif; font-size: .9em; background: white;&#125;.md-fences&#123; margin-left: 1em; padding-left: 1em; border: 1px solid #ddd; padding-bottom: 8px; padding-top: 6px; margin-bottom: 1.5em;&#125;a &#123; color: #2484c1; text-decoration: none;&#125;a:hover &#123; text-decoration: underline;&#125;a img &#123; border: none;&#125;h1 a,h1 a:hover &#123; color: #333; text-decoration: none;&#125;hr &#123; color: #ddd; height: 1px; margin: 2em 0; border-top: solid 1px #ddd; border-bottom: none; border-left: 0; border-right: 0;&#125;.ty-table-edit &#123; background: #ededed; padding-top: 4px;&#125;table &#123; margin-bottom: 1.333333rem&#125;table th,table td &#123; padding: 8px; line-height: 1.333333rem; vertical-align: top; border-top: 1px solid #ddd&#125;table th &#123; font-weight: bold&#125;table thead th &#123; vertical-align: bottom&#125;table caption+thead tr:first-child th,table caption+thead tr:first-child td,table colgroup+thead tr:first-child th,table colgroup+thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td &#123; border-top: 0&#125;table tbody+tbody &#123; border-top: 2px solid #ddd&#125;.task-list&#123; padding:0;&#125;/* fish修改 注释掉.md-task-list-item &#123; padding-left: 1.6rem;&#125;.md-task-list-item &gt; input:before &#123; content: '\\221A'; display: inline-block; width: 1.33333333rem; height: 1.6rem; vertical-align: middle; text-align: center; color: #ddd; background-color: #fefefe;&#125;.md-task-list-item &gt; input:checked:before,.md-task-list-item &gt; input[checked]:before&#123; color: inherit;&#125;*/.md-tag &#123; color: inherit; font: inherit;&#125;#write pre.md-meta-block &#123; min-height: 35px; padding: 0.5em 1em;&#125;#write pre.md-meta-block &#123; white-space: pre; background: #f8f8f8; border: 0px; color: #999; width: 100vw; max-width: calc(100% + 60px); margin-left: -30px; border-left: 30px #f8f8f8 solid; border-right: 30px #f8f8f8 solid; margin-bottom: 2em; margin-top: -1.3333333333333rem; padding-top: 26px; padding-bottom: 10px; line-height: 1.8em; font-size: 0.9em; font-size: 0.76em; padding-left: 0;&#125;.md-img-error.md-image&gt;.md-meta&#123; vertical-align: bottom;&#125;#write&gt;h5.md-focus:before &#123; top: 2px;&#125;.md-toc &#123; margin-top: 40px;&#125;.md-toc-content &#123; padding-bottom: 20px;&#125;.outline-expander:before &#123; color: inherit; font-size: 14px; top: auto; content: \"\\f0da\"; font-family: FontAwesome;&#125;.outline-expander:hover:before,.outline-item-open&gt;.outline-item&gt;.outline-expander:before &#123; content: \"\\f0d7\";&#125;/** source code mode */#typora-source &#123; font-family: Courier, monospace; color: #6A6A6A;&#125;.html-for-mac #typora-sidebar &#123; -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, .175); box-shadow: 0 6px 12px rgba(0, 0, 0, .175);&#125;.cm-s-typora-default .cm-header, .cm-s-typora-default .cm-property,.CodeMirror.cm-s-typora-default div.CodeMirror-cursor &#123; color: #428bca;&#125;.cm-s-typora-default .cm-atom, .cm-s-typora-default .cm-number &#123; color: #777777;&#125;.typora-node .file-list-item-parent-loc, .typora-node .file-list-item-time, .typora-node .file-list-item-summary &#123; font-family: arial, sans-serif;&#125;/* fish修改 注释掉.md-task-list-item&gt;input &#123; margin-left: -1.3em; margin-top: calc(1rem - 12px);&#125;*/.md-mathjax-midline &#123; background: #fafafa;&#125;.md-fences .code-tooltip &#123; bottom: -2em !important;&#125;.dropdown-menu .divider &#123; border-color: #e5e5e5;&#125;","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"博客工具","slug":"编程基础/博客工具","permalink":"http://yoursite.com/categories/编程基础/博客工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"}]},{"title":"mongo","slug":"人工智能/搜索引擎/mongo","date":"2019-02-28T08:41:56.000Z","updated":"2019-10-08T09:22:51.346Z","comments":true,"path":"wiki/人工智能/搜索引擎/mongo/","link":"","permalink":"http://yoursite.com/wiki/人工智能/搜索引擎/mongo/","excerpt":"","text":"### 服务器服务器启用mongo集群1234567891011121314151617181920212223242526# 进入mongo客户端mongo# 以下操作是在mongo客户端命令行内：# 创建集群，集群名字=“rs0”rs.initiate( &#123; _id: \"rs0\", members: [ &#123; _id: 0, host: \"mongo-r0:27017\" &#125;, &#123; _id: 1, host: \"mongo-r1:27017\" &#125;, &#123; _id: 2, host: \"mongo-r2:27017\" &#125; ] &#125;)# 查看集群状态rs.conf()rs.status() mongo客户端使用登录/验证/切换数据库12345# 进入mongo客户端(默认链接27017)mongo# 指定端口mongo --port 1234 客户端模块调用pymongo/motor调用mongo集群当 mongo 是集群时，客户端连接时需要设置好要连接的所有 mongo 节点。 12345import pymongo# uri里的“rs0”是集群名称，前面是每个节点的IP和端口uri = 'mongodb://mongo-r0:27017,mongo-r1:27017,mongo-r2:27017/?replicaSet=rs0'conn = pymongo.MongoClient()[i for i in conn.list_databases()] 注释：pymongo和motor连接时使用的uri字符串可以是相同的，因为motor实际是调用pymongo实现的。 集群–官方文档 pymongo调用基础功能1234567891011import pymongo# 链接到 数据库db = pymongo.MongoClient('mongodb://&#123;$ip&#125;:&#123;$port&#125;')[$db_name]# 链接到 数据集合db_col = db[$col_name]# 数据集合的总数量total = db_col.count_documents()# find sort()排序 limit()结果数量限制resp = db_col.find(&#123;&#125;, keys).sort([(key1, 1), (key2, -1)]).limit(10) 参考资料 官方文档","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"搜索引擎","slug":"人工智能/搜索引擎","permalink":"http://yoursite.com/categories/人工智能/搜索引擎/"}],"tags":[{"name":"mongo","slug":"mongo","permalink":"http://yoursite.com/tags/mongo/"}]},{"title":"markdown说明","slug":"编程基础/博客工具/markdown说明","date":"2019-02-27T07:03:45.000Z","updated":"2019-10-08T09:22:51.354Z","comments":true,"path":"wiki/编程基础/博客工具/markdown说明/","link":"","permalink":"http://yoursite.com/wiki/编程基础/博客工具/markdown说明/","excerpt":"","text":"$x^p_ {ij}$ 其中i表示第i个标签 $x^p_ {ij}$ 上标$^2$ 下标$_ 2$","categories":[{"name":"编程基础","slug":"编程基础","permalink":"http://yoursite.com/categories/编程基础/"},{"name":"博客工具","slug":"编程基础/博客工具","permalink":"http://yoursite.com/categories/编程基础/博客工具/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"}]},{"title":"基础库：os/sys/dis","slug":"python/基础库","date":"2019-02-26T07:00:45.000Z","updated":"2019-10-08T09:22:51.342Z","comments":true,"path":"wiki/python/基础库/","link":"","permalink":"http://yoursite.com/wiki/python/基础库/","excerpt":"","text":"时间文本123456789101112131415161718import timet = time.strftime('%Y-%m%d %H:%M')\"\"\"%Y Year with century as a decimal number. %m Month as a decimal number [01,12]. %d Day of the month as a decimal number [01,31]. %H Hour (24-hour clock) as a decimal number [00,23]. %M Minute as a decimal number [00,59]. %S Second as a decimal number [00,61]. %z Time zone offset from UTC. %a Locale's abbreviated weekday name. %A Locale's full weekday name. %b Locale's abbreviated month name. %B Locale's full month name. %c Locale's appropriate date and time representation. %I Hour (12-hour clock) as a decimal number [01,12]. %p Locale's equivalent of either AM or PM. \"\"\" python执行命令123os.system('ls') # 返回的是命令返回码， 一般成功时是0os.popen(r\"ls\").read() # 返回命令执行结果， 以文件形式返回， 可以用read()读出 常用功能 功能 代码 参数 判断文件是否存在 os.path.isfile(path) 判断文件或文件夹是否存在 os.path.exists(path) 判断文件权限 os.access(path, mode) os.F_OK存在 os.R_OK可读 os.W_OK:可写os.X_OK可执行 判断文件夹存在 os.path.isdir(dir) 得到当前工作目录 os.getcwd() 删除文件 os.remove() 列出目录里的文件夹和文件 os.listdir(dir) 改变工作目录到dirname os.chdir(dirname) https://www.cnblogs.com/wq242424/p/5803721.html dis – 查看解释器得出的执行码123import dis# fn 是想查看执行码的函数名print(dis.dis(fn))","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"os","slug":"os","permalink":"http://yoursite.com/tags/os/"},{"name":"sys","slug":"sys","permalink":"http://yoursite.com/tags/sys/"},{"name":"dis","slug":"dis","permalink":"http://yoursite.com/tags/dis/"},{"name":"python基础库","slug":"python基础库","permalink":"http://yoursite.com/tags/python基础库/"}]},{"title":"笔记 Ruby 之父松本行弘：程序员的乐趣、存在感与平衡.md","slug":"读书笔记/笔记-Ruby 之父松本行弘：程序员的乐趣、存在感与平衡","date":"2018-12-08T03:21:20.000Z","updated":"2019-10-24T11:39:01.345Z","comments":true,"path":"wiki/读书笔记/笔记-Ruby 之父松本行弘：程序员的乐趣、存在感与平衡/","link":"","permalink":"http://yoursite.com/wiki/读书笔记/笔记-Ruby 之父松本行弘：程序员的乐趣、存在感与平衡/","excerpt":"","text":"原文: https://gitee.com/gitee-stars/12https://zhuanlan.zhihu.com/p/84470460) 您在书里提到过，阅读开源代码是您获取编程知识的重要方式。现在开源发展非常迅速，有非常多的可供阅读和学习的素材，另一方面，也会有难以选择的困扰，对于如何更高效地学习开源代码，您有什么看法？ 松本行弘：先说结论，我认为这是因人而异的。说实话，没有那种每个人阅读了都会有用的源代码，所以还是应该根据自己想要掌握的技术或者兴趣领域来选择。 我个人对编程语言的开发很感兴趣，从学生时代开始阅读了很多关于编程语言的源代码，了解了其他的编程语言的设计与开发细节。假设你对操作系统很感兴趣，可以通过阅读 Linux 的源代码来学习操作系统的相关知识；假设你对 Web 应用感兴趣，可以通过阅读其他人写的 Web 应用代码或者框架来学习。总之，根据自己的兴趣和需要掌握的技能来选择学习的源代码。 1wxy: 我应该学习sklearn的源代码 您觉得工作与生活应该怎样平衡？现在中国的软件工程师加班情况特别普遍，996、997 甚至还有号称 007 的公司，很多程序员都觉得疲于奔命，工作之余还要给自己充电防止被淘汰，这样说实话基本照顾不到家庭。我们知道您和妻子一共有四个孩子，还养了一只狗和一只猫，您平衡工作与生活的秘诀是什么？ 松本行弘：一天有 24 小时这对谁都是公平的，不管是男女老少一天都只有 24 小时。除去睡觉的时间，会剩下 16 小时左右。这 16 小时如何支配是由个人意志决定的：什么时间去公司，什么时间吃饭，什么时间与家人一起度过。 如何支配时间是由个人价值观中的优先顺序（Priority）来决定的，工作有时真的是很有吸引力，你经过努力获得了预想的结果，充满成就感。渐渐地，工作的优先级会越来越高，花在工作上的时间越来越长，相对地，与家人相处的时间就会变短，有时会引发家庭问题或者健康问题。 我在结婚之前就和家人约定好，把家庭放在优先级最高的位置。我有时沉迷于Ruby开发而不能自拔，这时候我妻子就会提醒我：“我们约好的不是？”“对对对，我忘了（笑）。”家庭和工作的优先级，和家人事先达成共识，是一件非常重要的事。生活和工作像一个天平，当我渐渐偏向工作的时候，我的妻子就会把我拉回来，在这样的反反复复之中，我们一家人走到了现在。 首先，确定家庭和工作的优先级，有条件的话把你的优先级告诉别人，这样在你违反优先级的时候，可能就会有人提醒你。如果你只把优先级藏在自己心里，常常会因为工作太有意思而忘记了优先级。 第二，在工作中，尊重是一件十分重要的事，比如我尊重我的工作，在截止日期之前把工作完成；反过来，公司也需要尊重我，为了保证我的工作效率，不制定不人性的规则，或者准备一些能够提高工作效率的工具，不过度干涉我的工作，这是一种双赢的机制。 不过管理者有时不是很懂技术，有些管理者甚至没有写过代码，这样就需要用各种方式和渠道，将你提高生产效率的方法传达给他：“我通过×××的方式，可以提高生产效率。”你如果直接找管理者谈话，他可能会不太能理解：“该不是在怠工吧？该不是不想工作才想出这样的点子吧？”假如有人在博客上论述自由度对真正提高程序员生产效率的重要性，或者长期加班的害处，通过各种方式让管理者理解，IT 业界长期加班不利于生产效率的提高，而且还会引发各种家庭问题，对公司和程序员都不是好事，上述双赢机制对整个 IT 生态的健康发展大有裨益。 日本的企业这几年越来越体会到加班的坏处，加班文化较之前有了很大的改善，虽然还有很大一部分人还在饱受加班之苦，希望中国的互联网企业也能做到这点。如果你在做出各种努力之后依然得到不到尊重，那么我建议跳槽。 日本把这样的企业叫做 “黑心企业”，顾名思义，这样的企业不尊重从业者，令其长期加班，严重损害从业者的身体健康。其实这种企业的存在，也部分归因于从业者的隐忍，如果从业者都能及时止损，这样的企业就渐渐消失了，这是我一直给别人的忠告。 程序员也是人，虽然三岁小孩都知道，但人们往往只把程序员看做一个编程的工具。我们要努力让大家知道，我们也是人，我们也需要休息来提高生产效率，我们的工作并不是大家所想象的那样机械化。例如盖房子，假设房子下周就要完工了，然后你提出：“不好意思，麻烦把房子向右移 5cm。”可能你觉得“才 5cm 很简单嘛。”，但是如果真的这么说肯定会被打，这个道理大家都能理解。但程序员就常常遇到在项目即将完工前要求改需求的情况，这时应该坚定地拒绝，让社会了解程序员的真实面目以及平时工作的艰辛，才能获得相应的尊重。 参考资料","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"}],"tags":[]},{"title":"分词和新词发现","slug":"人工智能/资料/分词和新词发现","date":"2018-12-08T03:21:20.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/资料/分词和新词发现/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/分词和新词发现/","excerpt":"","text":"相关博客无监督构建词库：更快更好的新词发现算法—苏剑林 重新写了之前的新词发现算法：更快更好的新词发现—苏剑林 【学术】结合无监督学习和迁移学习的新词发现实践—李辰刚 语句分割中新词发现/实体词挖掘，使用Deep Learning的方法的相关思路和论文？ python | 高效统计语言模型kenlm：新词发现、分词、智能纠错—悟乙己 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"分词和新词发现","slug":"人工智能/资料/聚类","date":"2018-01-28T10:21:20.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/资料/聚类/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/聚类/","excerpt":"","text":"相关博客基于深度神经网络的聚类综述–2017 这里介绍的方法和大多数的不太一样 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]},{"title":"分词和新词发现","slug":"人工智能/资料/词向量","date":"2017-12-18T10:21:20.000Z","updated":"2019-10-13T04:21:17.184Z","comments":true,"path":"wiki/人工智能/资料/词向量/","link":"","permalink":"http://yoursite.com/wiki/人工智能/资料/词向量/","excerpt":"","text":"相关博客CS224n笔记2 词的向量表示：word2vec—hankcs word2vec原理推导与代码分析—hankcs fastText原理及实践—机器之心 参考资料","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://yoursite.com/categories/人工智能/"},{"name":"资料","slug":"人工智能/资料","permalink":"http://yoursite.com/categories/人工智能/资料/"}],"tags":[]}]}