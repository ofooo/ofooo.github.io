---
title: 朴素贝叶斯
toc: true
categories:
  - 人工智能
  - 资料
date: 2020-05-21 14:05:57
tags:
---

## 贝叶斯公式

### 动机

贝叶斯为了解决“逆向概率”问题，提出了贝叶斯公式

### 公式

Posterior knowledge = Prior knowledge X Likelihood

P(A|B) = P(A) * [ P(B|A) / P(B) ]

[ P(B|A) / P(B) ]这部分是了解已知条件后对P(A)的信念更新，叫做“可能性函数”**Likelyhood**（也翻译成似然函数）

- 可能性函数>1,则先验概率被增强了
- 可能性函数=1, 则B事件无助与判断A事件的可能性
- 可能性函数<1, 则先验概率被削弱了



## 朴素贝叶斯分类器

### 对数处理

因为多个概率累乘的精确度太低，所以使用对数转化成加法形式。



### 拉普拉斯平滑（Laplace smoothing）

如果由于数据匮乏，很有可能出现某个特征和类别从未同时出现，这时候会造成预测概率为0

这是不合理的，**不能因为一个事件没有观察到就认为该事件不会发生。**

解决这个问题的办法是给每个特征和类别的组合加上给定个数的**虚假样本（“hallucinated” examples）**。

在分子上加1, 在分母上加上特征的可能分类的数量，叫做啦普拉斯平滑。也就是假设特征分类增加了一个等可能性的虚假样本。



### 为什么有效

人们在使用分类器之前，首先做的第一步（也是最重要的一步）往往是**特征选择（feature selection），**这个过程的目的就是为了**排除特征之间的共线性、选择相对较为独立的特征。**其次，当我们假设特征之间相互独立时，这事实上就暗含了正则化的过程；而**不考虑变量之间的相关性有效的降低了朴素贝叶斯的分类方差。**虽然这有可能提高分类的偏差，但是如果这样的偏差不改变样本的排列顺序，那么它对分类的结果影响不大。由于这些原因，朴素贝叶斯分类器在实际中往往能够取得非常优秀的结果。



## 高斯朴素贝叶斯分类器（Gaussian Naïve Bayes classifier）

当特征是连续变量时，需要假设条件概率分布的形式。一种常见假设是假设P(Y|Xi)满足正太分布，正太分布的均值和标准差从训练集学习得到。这样的模型叫高斯朴素贝叶斯分类器。



## 参考资料
> - [https://zhuanlan.zhihu.com/p/37768413](https://zhuanlan.zhihu.com/p/37768413)
> - []()
