---
title: cs224n-02词向量
toc: true
categories:
  - 人工智能
  - cs224n笔记
date: 2019-09-22 10:22:32
tags:
---

.

## 一些思想

机器学习的优化一般喜欢做最小化, 而不是最大化, 只需要一个负号即可进行这种转变



## wordnet 语义词典

```python
# 使用nltk打开wordnet
from nltk.corpus import wordnet 
panda = wordnet.synset('panda.n.01')
hyper = lambda s: s.hypernyms()
list(panda.closure(hyper))    

```

### 分布相似性

上下文相似的单词, 语义有相似性



## word2vec

主要思想: 预测每个单词和他们的上下文单词

2种算法: Skip-grams  /  Continuous Bag of Words(CBOW)

2种训练方法: hierarchical softmax    /   Negative sampling

### Skip-grams(SG)

每个单词有2个向量表示, 一个用于输入, 一个用于计算输出上下文的概率, 这样在计算上更简单



## softmax

为什么求指数, 这样能把任何浮点数转化成正数

除以指数的和, 是为了归一化, 时所有值的和为1, 这样时物理意义上变成了"概率"

之所以叫"softmax", 因为如果你取指数时,就接近于一个最大值函数, 这样大的数值会进一步放大, 结果他们占绝对主导, 就像一个 max 函数, 但仍然是一个软性操作(soft)



## 词汇表

distributional  

| 英文                        | 中文                                    |
| --------------------------- | --------------------------------------- |
| distributional              | 分布式                                  |
| distributed representations | 分布式表示(用密集型向量表示词汇的含义)  |
| co-occur                    | 共现                                    |
| distributional simliarity   | 分布相似性(上下文相同的2个单词的相似性) |
| hyper parameters            | 超参数                                  |
| first principal component   | 第一主成分                              |
|                             |                                         |
|                             |                                         |



## 参考资料
> - []()
> - []()
